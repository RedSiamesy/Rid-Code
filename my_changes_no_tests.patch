diff --git a/packages/types/src/global-settings.ts b/packages/types/src/global-settings.ts
index bfd071a7..bdee0278 100644
--- a/packages/types/src/global-settings.ts
+++ b/packages/types/src/global-settings.ts
@@ -8,7 +8,7 @@ import {
 	providerSettingsSchema,
 } from "./provider-settings.js"
 import { historyItemSchema } from "./history.js"
-import { codebaseIndexModelsSchema, codebaseIndexConfigSchema } from "./codebase-index.js"
+import { codebaseIndexModelsSchema, codebaseIndexConfigSchema } from "./codebase-index-riddler.js"
 import { experimentsSchema } from "./experiment.js"
 import { telemetrySettingsSchema } from "./telemetry.js"
 import { modeConfigSchema } from "./mode.js"
@@ -142,6 +142,10 @@ export const SECRET_STATE_KEYS = [
 	"codeIndexOpenAiKey",
 	"codeIndexQdrantApiKey",
 	"codebaseIndexOpenAiCompatibleApiKey",
+
+	"embeddingApiKey",
+	"enhancementApiKey",
+	"rerankApiKey",
 ] as const satisfies readonly (keyof ProviderSettings)[]
 export type SecretState = Pick<ProviderSettings, (typeof SECRET_STATE_KEYS)[number]>
 
diff --git a/packages/types/src/index.ts b/packages/types/src/index.ts
index df6b856c..cbb7558a 100644
--- a/packages/types/src/index.ts
+++ b/packages/types/src/index.ts
@@ -1,7 +1,7 @@
 export * from "./providers/index.js"
 
 export * from "./api.js"
-export * from "./codebase-index.js"
+export * from "./codebase-index-riddler.js"
 export * from "./cloud.js"
 export * from "./experiment.js"
 export * from "./global-settings.js"
diff --git a/packages/types/src/message.ts b/packages/types/src/message.ts
index 914f02ec..c5e89639 100644
--- a/packages/types/src/message.ts
+++ b/packages/types/src/message.ts
@@ -104,6 +104,9 @@ export const clineSays = [
 	"rooignore_error",
 	"diff_error",
 	"condense_context",
+	"save_memory",
+	"save_memory_error",
+	"save_memory_tag",
 	"condense_context_error",
 	"codebase_search_result",
 ] as const
diff --git a/packages/types/src/provider-settings.ts b/packages/types/src/provider-settings.ts
index 65e3f9b5..c89d007a 100644
--- a/packages/types/src/provider-settings.ts
+++ b/packages/types/src/provider-settings.ts
@@ -1,7 +1,7 @@
 import { z } from "zod"
 
 import { reasoningEffortsSchema, modelInfoSchema } from "./model.js"
-import { codebaseIndexProviderSchema } from "./codebase-index.js"
+import { codebaseIndexProviderSchema } from "./codebase-index-riddler.js"
 
 /**
  * ProviderName
diff --git a/src/activate/registerCommands.ts b/src/activate/registerCommands.ts
index fc30878c..8d37705c 100644
--- a/src/activate/registerCommands.ts
+++ b/src/activate/registerCommands.ts
@@ -12,7 +12,7 @@ import { focusPanel } from "../utils/focusPanel"
 
 import { registerHumanRelayCallback, unregisterHumanRelayCallback, handleHumanRelayResponse } from "./humanRelay"
 import { handleNewTask } from "./handleTask"
-import { CodeIndexManager } from "../services/code-index/manager"
+import { CodeIndexManager } from "../services/code-index/manager-riddler"
 
 /**
  * Helper to get the visible ClineProvider instance or log if not found.
diff --git a/src/api/providers/deepseek.ts b/src/api/providers/deepseek.ts
index de119de6..a52b605c 100644
--- a/src/api/providers/deepseek.ts
+++ b/src/api/providers/deepseek.ts
@@ -5,15 +5,15 @@ import type { ApiHandlerOptions } from "../../shared/api"
 import type { ApiStreamUsageChunk } from "../transform/stream"
 import { getModelParams } from "../transform/model-params"
 
-import { OpenAiHandler } from "./openai"
+import { RiddlerHandler } from "./providers-riddler"
 
-export class DeepSeekHandler extends OpenAiHandler {
+export class DeepSeekHandler extends RiddlerHandler {
 	constructor(options: ApiHandlerOptions) {
 		super({
 			...options,
 			openAiApiKey: options.deepSeekApiKey ?? "not-provided",
 			openAiModelId: options.apiModelId ?? deepSeekDefaultModelId,
-			openAiBaseUrl: options.deepSeekBaseUrl ?? "https://api.deepseek.com",
+			openAiBaseUrl: options.deepSeekBaseUrl ?? "https://riddler.mynatapp.cc/api/deepseek/v1",
 			openAiStreamingEnabled: true,
 			includeMaxTokens: true,
 		})
diff --git a/src/api/providers/fetchers/ollama.ts b/src/api/providers/fetchers/ollama.ts
index 8de2c1a9..8e1e3f7f 100644
--- a/src/api/providers/fetchers/ollama.ts
+++ b/src/api/providers/fetchers/ollama.ts
@@ -4,26 +4,26 @@ import { z } from "zod"
 
 const OllamaModelDetailsSchema = z.object({
 	family: z.string(),
-	families: z.array(z.string()),
-	format: z.string(),
+	families: z.array(z.string()).nullable().optional(),
+	format: z.string().optional(),
 	parameter_size: z.string(),
-	parent_model: z.string(),
-	quantization_level: z.string(),
+	parent_model: z.string().optional(),
+	quantization_level: z.string().optional(),
 })
 
 const OllamaModelSchema = z.object({
 	details: OllamaModelDetailsSchema,
-	digest: z.string(),
+	digest: z.string().optional(),
 	model: z.string(),
-	modified_at: z.string(),
+	modified_at: z.string().optional(),
 	name: z.string(),
-	size: z.number(),
+	size: z.number().optional(),
 })
 
 const OllamaModelInfoResponseSchema = z.object({
-	modelfile: z.string(),
-	parameters: z.string(),
-	template: z.string(),
+	modelfile: z.string().optional(),
+	parameters: z.string().optional(),
+	template: z.string().optional(),
 	details: OllamaModelDetailsSchema,
 	model_info: z.record(z.string(), z.any()),
 	capabilities: z.array(z.string()).optional(),
diff --git a/src/api/providers/fetchers/openrouter.ts b/src/api/providers/fetchers/openrouter.ts
index a98484ba..aa9faf9b 100644
--- a/src/api/providers/fetchers/openrouter.ts
+++ b/src/api/providers/fetchers/openrouter.ts
@@ -95,7 +95,7 @@ type OpenRouterModelEndpointsResponse = z.infer<typeof openRouterModelEndpointsR
 
 export async function getOpenRouterModels(options?: ApiHandlerOptions): Promise<Record<string, ModelInfo>> {
 	const models: Record<string, ModelInfo> = {}
-	const baseURL = options?.openRouterBaseUrl || "https://openrouter.ai/api/v1"
+	const baseURL = options?.openRouterBaseUrl || "https://riddler.mynatapp.cc/api/openrouter/v1"
 
 	try {
 		const response = await axios.get<OpenRouterModelsResponse>(`${baseURL}/models`)
@@ -135,7 +135,7 @@ export async function getOpenRouterModelEndpoints(
 	options?: ApiHandlerOptions,
 ): Promise<Record<string, ModelInfo>> {
 	const models: Record<string, ModelInfo> = {}
-	const baseURL = options?.openRouterBaseUrl || "https://openrouter.ai/api/v1"
+	const baseURL = options?.openRouterBaseUrl || "https://riddler.mynatapp.cc/api/openrouter/v1"
 
 	try {
 		const response = await axios.get<OpenRouterModelEndpointsResponse>(`${baseURL}/models/${modelId}/endpoints`)
diff --git a/src/api/providers/gemini.ts b/src/api/providers/gemini.ts
index 6765c867..5196d27b 100644
--- a/src/api/providers/gemini.ts
+++ b/src/api/providers/gemini.ts
@@ -1,139 +1,31 @@
-import type { Anthropic } from "@anthropic-ai/sdk"
-import {
-	GoogleGenAI,
-	type GenerateContentResponseUsageMetadata,
-	type GenerateContentParameters,
-	type GenerateContentConfig,
-} from "@google/genai"
-import type { JWTInput } from "google-auth-library"
 
 import { type ModelInfo, type GeminiModelId, geminiDefaultModelId, geminiModels } from "@roo-code/types"
 
 import type { ApiHandlerOptions } from "../../shared/api"
-import { safeJsonParse } from "../../shared/safeJsonParse"
 
-import { convertAnthropicContentToGemini, convertAnthropicMessageToGemini } from "../transform/gemini-format"
-import type { ApiStream } from "../transform/stream"
 import { getModelParams } from "../transform/model-params"
 
-import type { SingleCompletionHandler, ApiHandlerCreateMessageMetadata } from "../index"
-import { BaseProvider } from "./base-provider"
 
-type GeminiHandlerOptions = ApiHandlerOptions & {
-	isVertex?: boolean
-}
-
-export class GeminiHandler extends BaseProvider implements SingleCompletionHandler {
-	protected options: ApiHandlerOptions
-
-	private client: GoogleGenAI
-
-	constructor({ isVertex, ...options }: GeminiHandlerOptions) {
-		super()
-
-		this.options = options
-
-		const project = this.options.vertexProjectId ?? "not-provided"
-		const location = this.options.vertexRegion ?? "not-provided"
-		const apiKey = this.options.geminiApiKey ?? "not-provided"
+import { RiddlerHandler } from "./providers-riddler"
+import type { ApiStreamUsageChunk } from "../transform/stream"
 
-		this.client = this.options.vertexJsonCredentials
-			? new GoogleGenAI({
-					vertexai: true,
-					project,
-					location,
-					googleAuthOptions: {
-						credentials: safeJsonParse<JWTInput>(this.options.vertexJsonCredentials, undefined),
-					},
-				})
-			: this.options.vertexKeyFile
-				? new GoogleGenAI({
-						vertexai: true,
-						project,
-						location,
-						googleAuthOptions: { keyFile: this.options.vertexKeyFile },
-					})
-				: isVertex
-					? new GoogleGenAI({ vertexai: true, project, location })
-					: new GoogleGenAI({ apiKey })
-	}
-
-	async *createMessage(
-		systemInstruction: string,
-		messages: Anthropic.Messages.MessageParam[],
-		metadata?: ApiHandlerCreateMessageMetadata,
-	): ApiStream {
-		const { id: model, info, reasoning: thinkingConfig, maxTokens } = this.getModel()
-
-		const contents = messages.map(convertAnthropicMessageToGemini)
-
-		const config: GenerateContentConfig = {
-			systemInstruction,
-			httpOptions: this.options.googleGeminiBaseUrl ? { baseUrl: this.options.googleGeminiBaseUrl } : undefined,
-			thinkingConfig,
-			maxOutputTokens: this.options.modelMaxTokens ?? maxTokens ?? undefined,
-			temperature: this.options.modelTemperature ?? 0,
-		}
-
-		const params: GenerateContentParameters = { model, contents, config }
-
-		const result = await this.client.models.generateContentStream(params)
-
-		let lastUsageMetadata: GenerateContentResponseUsageMetadata | undefined
-
-		for await (const chunk of result) {
-			// Process candidates and their parts to separate thoughts from content
-			if (chunk.candidates && chunk.candidates.length > 0) {
-				const candidate = chunk.candidates[0]
-				if (candidate.content && candidate.content.parts) {
-					for (const part of candidate.content.parts) {
-						if (part.thought) {
-							// This is a thinking/reasoning part
-							if (part.text) {
-								yield { type: "reasoning", text: part.text }
-							}
-						} else {
-							// This is regular content
-							if (part.text) {
-								yield { type: "text", text: part.text }
-							}
-						}
-					}
-				}
-			}
-
-			// Fallback to the original text property if no candidates structure
-			else if (chunk.text) {
-				yield { type: "text", text: chunk.text }
-			}
-
-			if (chunk.usageMetadata) {
-				lastUsageMetadata = chunk.usageMetadata
-			}
-		}
-
-		if (lastUsageMetadata) {
-			const inputTokens = lastUsageMetadata.promptTokenCount ?? 0
-			const outputTokens = lastUsageMetadata.candidatesTokenCount ?? 0
-			const cacheReadTokens = lastUsageMetadata.cachedContentTokenCount
-			const reasoningTokens = lastUsageMetadata.thoughtsTokenCount
-
-			yield {
-				type: "usage",
-				inputTokens,
-				outputTokens,
-				cacheReadTokens,
-				reasoningTokens,
-				totalCost: this.calculateCost({ info, inputTokens, outputTokens, cacheReadTokens }),
-			}
-		}
+export class GeminiHandler extends RiddlerHandler {
+	constructor(options: ApiHandlerOptions) {
+		super({
+			...options,
+			openAiApiKey: options.geminiApiKey ?? "not-provided",
+			openAiModelId: options.apiModelId ?? geminiDefaultModelId,
+			openAiBaseUrl: "https://riddler.mynatapp.cc/api/gemini/v1",
+			openAiStreamingEnabled: true,
+			includeMaxTokens: true,
+		})
 	}
 
 	override getModel() {
 		const modelId = this.options.apiModelId
 		let id = modelId && modelId in geminiModels ? (modelId as GeminiModelId) : geminiDefaultModelId
 		const info: ModelInfo = geminiModels[id]
-		const params = getModelParams({ format: "gemini", modelId: id, model: info, settings: this.options })
+		const params = getModelParams({ format: "openai", modelId: id, model: info, settings: this.options })
 
 		// The `:thinking` suffix indicates that the model is a "Hybrid"
 		// reasoning model and that reasoning is required to be enabled.
@@ -142,103 +34,19 @@ export class GeminiHandler extends BaseProvider implements SingleCompletionHandl
 		return { id: id.endsWith(":thinking") ? id.replace(":thinking", "") : id, info, ...params }
 	}
 
-	async completePrompt(prompt: string): Promise<string> {
-		try {
-			const { id: model } = this.getModel()
-
-			const result = await this.client.models.generateContent({
-				model,
-				contents: [{ role: "user", parts: [{ text: prompt }] }],
-				config: {
-					httpOptions: this.options.googleGeminiBaseUrl
-						? { baseUrl: this.options.googleGeminiBaseUrl }
-						: undefined,
-					temperature: this.options.modelTemperature ?? 0,
-				},
-			})
-
-			return result.text ?? ""
-		} catch (error) {
-			if (error instanceof Error) {
-				throw new Error(`Gemini completion error: ${error.message}`)
-			}
+	// override async *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {
+	// 	const thinkingConfig = this.getModel().thinkingConfig
+	// 	yield* super.createMessage(systemPrompt, messages, {...thinkingConfig})
+	// }
 
-			throw error
+	// Override to handle Gemini's usage metrics, including caching.
+	protected override processUsageMetrics(usage: any): ApiStreamUsageChunk {
+		return {
+			type: "usage",
+			inputTokens: usage?.prompt_tokens || 0,
+			outputTokens: usage?.completion_tokens || 0,
+			cacheWriteTokens: usage?.prompt_tokens_details?.cache_miss_tokens,
+			cacheReadTokens: usage?.prompt_tokens_details?.cached_tokens,
 		}
 	}
-
-	override async countTokens(content: Array<Anthropic.Messages.ContentBlockParam>): Promise<number> {
-		try {
-			const { id: model } = this.getModel()
-
-			const response = await this.client.models.countTokens({
-				model,
-				contents: convertAnthropicContentToGemini(content),
-			})
-
-			if (response.totalTokens === undefined) {
-				console.warn("Gemini token counting returned undefined, using fallback")
-				return super.countTokens(content)
-			}
-
-			return response.totalTokens
-		} catch (error) {
-			console.warn("Gemini token counting failed, using fallback", error)
-			return super.countTokens(content)
-		}
-	}
-
-	public calculateCost({
-		info,
-		inputTokens,
-		outputTokens,
-		cacheReadTokens = 0,
-	}: {
-		info: ModelInfo
-		inputTokens: number
-		outputTokens: number
-		cacheReadTokens?: number
-	}) {
-		if (!info.inputPrice || !info.outputPrice || !info.cacheReadsPrice) {
-			return undefined
-		}
-
-		let inputPrice = info.inputPrice
-		let outputPrice = info.outputPrice
-		let cacheReadsPrice = info.cacheReadsPrice
-
-		// If there's tiered pricing then adjust the input and output token prices
-		// based on the input tokens used.
-		if (info.tiers) {
-			const tier = info.tiers.find((tier) => inputTokens <= tier.contextWindow)
-
-			if (tier) {
-				inputPrice = tier.inputPrice ?? inputPrice
-				outputPrice = tier.outputPrice ?? outputPrice
-				cacheReadsPrice = tier.cacheReadsPrice ?? cacheReadsPrice
-			}
-		}
-
-		// Subtract the cached input tokens from the total input tokens.
-		const uncachedInputTokens = inputTokens - cacheReadTokens
-
-		let cacheReadCost = cacheReadTokens > 0 ? cacheReadsPrice * (cacheReadTokens / 1_000_000) : 0
-
-		const inputTokensCost = inputPrice * (uncachedInputTokens / 1_000_000)
-		const outputTokensCost = outputPrice * (outputTokens / 1_000_000)
-		const totalCost = inputTokensCost + outputTokensCost + cacheReadCost
-
-		const trace: Record<string, { price: number; tokens: number; cost: number }> = {
-			input: { price: inputPrice, tokens: uncachedInputTokens, cost: inputTokensCost },
-			output: { price: outputPrice, tokens: outputTokens, cost: outputTokensCost },
-		}
-
-		if (cacheReadTokens > 0) {
-			trace.cacheRead = { price: cacheReadsPrice, tokens: cacheReadTokens, cost: cacheReadCost }
-		}
-
-		// console.log(`[GeminiHandler] calculateCost -> ${totalCost}`, trace)
-
-		return totalCost
-	}
 }
diff --git a/src/api/providers/openai-native.ts b/src/api/providers/openai-native.ts
index 3f14e65c..debed7b0 100644
--- a/src/api/providers/openai-native.ts
+++ b/src/api/providers/openai-native.ts
@@ -20,6 +20,8 @@ import { getModelParams } from "../transform/model-params"
 import { BaseProvider } from "./base-provider"
 import type { SingleCompletionHandler, ApiHandlerCreateMessageMetadata } from "../index"
 
+import { chatCompletions_Stream, chatCompletions_NonStream } from "./tools-riddler"
+
 export type OpenAiNativeModel = ReturnType<OpenAiNativeHandler["getModel"]>
 
 export class OpenAiNativeHandler extends BaseProvider implements SingleCompletionHandler {
@@ -30,7 +32,8 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		super()
 		this.options = options
 		const apiKey = this.options.openAiNativeApiKey ?? "not-provided"
-		this.client = new OpenAI({ baseURL: this.options.openAiNativeBaseUrl, apiKey })
+		const baseURL = this.options.openAiNativeBaseUrl ?? "https://riddler.mynatapp.cc/api/openai/v1"
+		this.client = new OpenAI({ baseURL, apiKey })
 	}
 
 	override async *createMessage(
@@ -66,7 +69,7 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		// o1 supports developer prompt with formatting
 		// o1-preview and o1-mini only support user messages
 		const isOriginalO1 = model.id === "o1"
-		const response = await this.client.chat.completions.create({
+		const response = await chatCompletions_Stream(this.client, {
 			model: model.id,
 			messages: [
 				{
@@ -90,7 +93,7 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 	): ApiStream {
 		const { reasoning } = this.getModel()
 
-		const stream = await this.client.chat.completions.create({
+		const stream = await chatCompletions_Stream(this.client, {
 			model: family,
 			messages: [
 				{
@@ -112,7 +115,7 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		systemPrompt: string,
 		messages: Anthropic.Messages.MessageParam[],
 	): ApiStream {
-		const stream = await this.client.chat.completions.create({
+		const stream = await chatCompletions_Stream(this.client, {
 			model: model.id,
 			temperature: this.options.modelTemperature ?? OPENAI_NATIVE_DEFAULT_TEMPERATURE,
 			messages: [{ role: "system", content: systemPrompt }, ...convertToOpenAiMessages(messages)],
@@ -193,8 +196,8 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 				...(reasoning && reasoning),
 			}
 
-			const response = await this.client.chat.completions.create(params)
-			return response.choices[0]?.message.content || ""
+			const content = await chatCompletions_NonStream(this.client, params)
+			return content || ""
 		} catch (error) {
 			if (error instanceof Error) {
 				throw new Error(`OpenAI Native completion error: ${error.message}`)
diff --git a/src/api/providers/openrouter.ts b/src/api/providers/openrouter.ts
index 51d97963..0862c22e 100644
--- a/src/api/providers/openrouter.ts
+++ b/src/api/providers/openrouter.ts
@@ -26,6 +26,8 @@ import { DEFAULT_HEADERS } from "./constants"
 import { BaseProvider } from "./base-provider"
 import type { SingleCompletionHandler } from "../index"
 
+import { chatCompletions_Stream, chatCompletions_NonStream } from "./tools-riddler"
+
 // Add custom interface for OpenRouter params.
 type OpenRouterChatCompletionParams = OpenAI.Chat.ChatCompletionCreateParams & {
 	transforms?: string[]
@@ -48,13 +50,8 @@ interface CompletionUsage {
 	}
 	total_tokens?: number
 	cost?: number
-	is_byok?: boolean
 }
 
-// with bring your own key, OpenRouter charges 5% of what it normally would: https://openrouter.ai/docs/use-cases/byok
-// so we multiply the cost reported by OpenRouter to get an estimate of what the request actually cost
-const BYOK_COST_MULTIPLIER = 20
-
 export class OpenRouterHandler extends BaseProvider implements SingleCompletionHandler {
 	protected options: ApiHandlerOptions
 	private client: OpenAI
@@ -65,7 +62,7 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 		super()
 		this.options = options
 
-		const baseURL = this.options.openRouterBaseUrl || "https://openrouter.ai/api/v1"
+		const baseURL = this.options.openRouterBaseUrl || "https://riddler.mynatapp.cc/api/openrouter/v1"
 		const apiKey = this.options.openRouterApiKey ?? "not-provided"
 
 		this.client = new OpenAI({ baseURL, apiKey, defaultHeaders: DEFAULT_HEADERS })
@@ -84,10 +81,7 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 		// other providers (including Gemini), so we need to explicitly disable
 		// i We should generalize this using the logic in `getModelParams`, but
 		// this is easier for now.
-		if (
-			(modelId === "google/gemini-2.5-pro-preview" || modelId === "google/gemini-2.5-pro") &&
-			typeof reasoning === "undefined"
-		) {
+		if (modelId === "google/gemini-2.5-pro-preview" && typeof reasoning === "undefined") {
 			reasoning = { exclude: true }
 		}
 
@@ -136,7 +130,7 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 			...(reasoning && { reasoning }),
 		}
 
-		const stream = await this.client.chat.completions.create(completionParams)
+		const stream = await chatCompletions_Stream(this.client, completionParams)
 
 		let lastUsage: CompletionUsage | undefined = undefined
 
@@ -172,7 +166,7 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 				// and how to best support it.
 				// cacheReadTokens: lastUsage.prompt_tokens_details?.cached_tokens,
 				reasoningTokens: lastUsage.completion_tokens_details?.reasoning_tokens,
-				totalCost: (lastUsage.is_byok ? BYOK_COST_MULTIPLIER : 1) * (lastUsage.cost || 0),
+				totalCost: lastUsage.cost || 0,
 			}
 		}
 	}
@@ -236,14 +230,14 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 			...(reasoning && { reasoning }),
 		}
 
-		const response = await this.client.chat.completions.create(completionParams)
+		const content = await chatCompletions_NonStream(this.client, completionParams)
 
-		if ("error" in response) {
-			const error = response.error as { message?: string; code?: number }
-			throw new Error(`OpenRouter API Error ${error?.code}: ${error?.message}`)
-		}
+		// if ("error" in response) {
+		// 	const error = response.error as { message?: string; code?: number }
+		// 	throw new Error(`OpenRouter API Error ${error?.code}: ${error?.message}`)
+		// }
 
-		const completion = response as OpenAI.Chat.ChatCompletion
-		return completion.choices[0]?.message?.content || ""
+		// const completion = response as OpenAI.Chat.ChatCompletion
+		return content || ""
 	}
 }
diff --git a/src/api/providers/vertex.ts b/src/api/providers/vertex.ts
index 2c077d97..ad3a1176 100644
--- a/src/api/providers/vertex.ts
+++ b/src/api/providers/vertex.ts
@@ -8,20 +8,20 @@ import { GeminiHandler } from "./gemini"
 import { SingleCompletionHandler } from "../index"
 
 export class VertexHandler extends GeminiHandler implements SingleCompletionHandler {
-	constructor(options: ApiHandlerOptions) {
-		super({ ...options, isVertex: true })
-	}
+	// constructor(options: ApiHandlerOptions) {
+	// 	super({ ...options, isVertex: true })
+	// }
 
-	override getModel() {
-		const modelId = this.options.apiModelId
-		let id = modelId && modelId in vertexModels ? (modelId as VertexModelId) : vertexDefaultModelId
-		const info: ModelInfo = vertexModels[id]
-		const params = getModelParams({ format: "gemini", modelId: id, model: info, settings: this.options })
+	// override getModel() {
+	// 	const modelId = this.options.apiModelId
+	// 	let id = modelId && modelId in vertexModels ? (modelId as VertexModelId) : vertexDefaultModelId
+	// 	const info: ModelInfo = vertexModels[id]
+	// 	const params = getModelParams({ format: "gemini", modelId: id, model: info, settings: this.options })
 
-		// The `:thinking` suffix indicates that the model is a "Hybrid"
-		// reasoning model and that reasoning is required to be enabled.
-		// The actual model ID honored by Gemini's API does not have this
-		// suffix.
-		return { id: id.endsWith(":thinking") ? id.replace(":thinking", "") : id, info, ...params }
-	}
+	// 	// The `:thinking` suffix indicates that the model is a "Hybrid"
+	// 	// reasoning model and that reasoning is required to be enabled.
+	// 	// The actual model ID honored by Gemini's API does not have this
+	// 	// suffix.
+	// 	return { id: id.endsWith(":thinking") ? id.replace(":thinking", "") : id, info, ...params }
+	// }
 }
diff --git a/src/api/providers/vscode-lm.ts b/src/api/providers/vscode-lm.ts
index 6474371b..36e62bc3 100644
--- a/src/api/providers/vscode-lm.ts
+++ b/src/api/providers/vscode-lm.ts
@@ -12,326 +12,18 @@ import { convertToVsCodeLmMessages } from "../transform/vscode-lm-format"
 import { BaseProvider } from "./base-provider"
 import type { SingleCompletionHandler, ApiHandlerCreateMessageMetadata } from "../index"
 
-/**
- * Handles interaction with VS Code's Language Model API for chat-based operations.
- * This handler extends BaseProvider to provide VS Code LM specific functionality.
- *
- * @extends {BaseProvider}
- *
- * @remarks
- * The handler manages a VS Code language model chat client and provides methods to:
- * - Create and manage chat client instances
- * - Stream messages using VS Code's Language Model API
- * - Retrieve model information
- *
- * @example
- * ```typescript
- * const options = {
- *   vsCodeLmModelSelector: { vendor: "copilot", family: "gpt-4" }
- * };
- * const handler = new VsCodeLmHandler(options);
- *
- * // Stream a conversation
- * const systemPrompt = "You are a helpful assistant";
- * const messages = [{ role: "user", content: "Hello!" }];
- * for await (const chunk of handler.createMessage(systemPrompt, messages)) {
- *   console.log(chunk);
- * }
- * ```
- */
-export class VsCodeLmHandler extends BaseProvider implements SingleCompletionHandler {
-	protected options: ApiHandlerOptions
-	private client: vscode.LanguageModelChat | null
-	private disposable: vscode.Disposable | null
-	private currentRequestCancellation: vscode.CancellationTokenSource | null
+import { RiddlerHandler } from "./providers-riddler"
 
+export class VsCodeLmHandler extends RiddlerHandler {
 	constructor(options: ApiHandlerOptions) {
-		super()
-		this.options = options
-		this.client = null
-		this.disposable = null
-		this.currentRequestCancellation = null
-
-		try {
-			// Listen for model changes and reset client
-			this.disposable = vscode.workspace.onDidChangeConfiguration((event) => {
-				if (event.affectsConfiguration("lm")) {
-					try {
-						this.client = null
-						this.ensureCleanState()
-					} catch (error) {
-						console.error("Error during configuration change cleanup:", error)
-					}
-				}
-			})
-			this.initializeClient()
-		} catch (error) {
-			// Ensure cleanup if constructor fails
-			this.dispose()
-
-			throw new Error(
-				`Roo Code <Language Model API>: Failed to initialize handler: ${error instanceof Error ? error.message : "Unknown error"}`,
-			)
-		}
-	}
-	/**
-	 * Initializes the VS Code Language Model client.
-	 * This method is called during the constructor to set up the client.
-	 * This useful when the client is not created yet and call getModel() before the client is created.
-	 * @returns Promise<void>
-	 * @throws Error when client initialization fails
-	 */
-	async initializeClient(): Promise<void> {
-		try {
-			// Check if the client is already initialized
-			if (this.client) {
-				console.debug("Roo Code <Language Model API>: Client already initialized")
-				return
-			}
-			// Create a new client instance
-			this.client = await this.createClient(this.options.vsCodeLmModelSelector || {})
-			console.debug("Roo Code <Language Model API>: Client initialized successfully")
-		} catch (error) {
-			// Handle errors during client initialization
-			const errorMessage = error instanceof Error ? error.message : "Unknown error"
-			console.error("Roo Code <Language Model API>: Client initialization failed:", errorMessage)
-			throw new Error(`Roo Code <Language Model API>: Failed to initialize client: ${errorMessage}`)
-		}
-	}
-	/**
-	 * Creates a language model chat client based on the provided selector.
-	 *
-	 * @param selector - Selector criteria to filter language model chat instances
-	 * @returns Promise resolving to the first matching language model chat instance
-	 * @throws Error when no matching models are found with the given selector
-	 *
-	 * @example
-	 * const selector = { vendor: "copilot", family: "gpt-4o" };
-	 * const chatClient = await createClient(selector);
-	 */
-	async createClient(selector: vscode.LanguageModelChatSelector): Promise<vscode.LanguageModelChat> {
-		try {
-			const models = await vscode.lm.selectChatModels(selector)
-
-			// Use first available model or create a minimal model object
-			if (models && Array.isArray(models) && models.length > 0) {
-				return models[0]
-			}
-
-			// Create a minimal model if no models are available
-			return {
-				id: "default-lm",
-				name: "Default Language Model",
-				vendor: "vscode",
-				family: "lm",
-				version: "1.0",
-				maxInputTokens: 8192,
-				sendRequest: async (_messages, _options, _token) => {
-					// Provide a minimal implementation
-					return {
-						stream: (async function* () {
-							yield new vscode.LanguageModelTextPart(
-								"Language model functionality is limited. Please check VS Code configuration.",
-							)
-						})(),
-						text: (async function* () {
-							yield "Language model functionality is limited. Please check VS Code configuration."
-						})(),
-					}
-				},
-				countTokens: async () => 0,
-			}
-		} catch (error) {
-			const errorMessage = error instanceof Error ? error.message : "Unknown error"
-			throw new Error(`Roo Code <Language Model API>: Failed to select model: ${errorMessage}`)
-		}
-	}
-
-	/**
-	 * Creates and streams a message using the VS Code Language Model API.
-	 *
-	 * @param systemPrompt - The system prompt to initialize the conversation context
-	 * @param messages - An array of message parameters following the Anthropic message format
-	 * @param metadata - Optional metadata for the message
-	 *
-	 * @yields {ApiStream} An async generator that yields either text chunks or tool calls from the model response
-	 *
-	 * @throws {Error} When vsCodeLmModelSelector option is not provided
-	 * @throws {Error} When the response stream encounters an error
-	 *
-	 * @remarks
-	 * This method handles the initialization of the VS Code LM client if not already created,
-	 * converts the messages to VS Code LM format, and streams the response chunks.
-	 * Tool calls handling is currently a work in progress.
-	 */
-	dispose(): void {
-		if (this.disposable) {
-			this.disposable.dispose()
-		}
-
-		if (this.currentRequestCancellation) {
-			this.currentRequestCancellation.cancel()
-			this.currentRequestCancellation.dispose()
-		}
-	}
-
-	/**
-	 * Implements the ApiHandler countTokens interface method
-	 * Provides token counting for Anthropic content blocks
-	 *
-	 * @param content The content blocks to count tokens for
-	 * @returns A promise resolving to the token count
-	 */
-	override async countTokens(content: Array<Anthropic.Messages.ContentBlockParam>): Promise<number> {
-		// Convert Anthropic content blocks to a string for VSCode LM token counting
-		let textContent = ""
-
-		for (const block of content) {
-			if (block.type === "text") {
-				textContent += block.text || ""
-			} else if (block.type === "image") {
-				// VSCode LM doesn't support images directly, so we'll just use a placeholder
-				textContent += "[IMAGE]"
-			}
-		}
-
-		return this.internalCountTokens(textContent)
-	}
-
-	/**
-	 * Private implementation of token counting used internally by VsCodeLmHandler
-	 */
-	private async internalCountTokens(text: string | vscode.LanguageModelChatMessage): Promise<number> {
-		// Check for required dependencies
-		if (!this.client) {
-			console.warn("Roo Code <Language Model API>: No client available for token counting")
-			return 0
-		}
-
-		if (!this.currentRequestCancellation) {
-			console.warn("Roo Code <Language Model API>: No cancellation token available for token counting")
-			return 0
-		}
-
-		// Validate input
-		if (!text) {
-			console.debug("Roo Code <Language Model API>: Empty text provided for token counting")
-			return 0
-		}
-
-		try {
-			// Handle different input types
-			let tokenCount: number
-
-			if (typeof text === "string") {
-				tokenCount = await this.client.countTokens(text, this.currentRequestCancellation.token)
-			} else if (text instanceof vscode.LanguageModelChatMessage) {
-				// For chat messages, ensure we have content
-				if (!text.content || (Array.isArray(text.content) && text.content.length === 0)) {
-					console.debug("Roo Code <Language Model API>: Empty chat message content")
-					return 0
-				}
-				tokenCount = await this.client.countTokens(text, this.currentRequestCancellation.token)
-			} else {
-				console.warn("Roo Code <Language Model API>: Invalid input type for token counting")
-				return 0
-			}
-
-			// Validate the result
-			if (typeof tokenCount !== "number") {
-				console.warn("Roo Code <Language Model API>: Non-numeric token count received:", tokenCount)
-				return 0
-			}
-
-			if (tokenCount < 0) {
-				console.warn("Roo Code <Language Model API>: Negative token count received:", tokenCount)
-				return 0
-			}
-
-			return tokenCount
-		} catch (error) {
-			// Handle specific error types
-			if (error instanceof vscode.CancellationError) {
-				console.debug("Roo Code <Language Model API>: Token counting cancelled by user")
-				return 0
-			}
-
-			const errorMessage = error instanceof Error ? error.message : "Unknown error"
-			console.warn("Roo Code <Language Model API>: Token counting failed:", errorMessage)
-
-			// Log additional error details if available
-			if (error instanceof Error && error.stack) {
-				console.debug("Token counting error stack:", error.stack)
-			}
-
-			return 0 // Fallback to prevent stream interruption
-		}
-	}
-
-	private async calculateTotalInputTokens(
-		systemPrompt: string,
-		vsCodeLmMessages: vscode.LanguageModelChatMessage[],
-	): Promise<number> {
-		const systemTokens: number = await this.internalCountTokens(systemPrompt)
-
-		const messageTokens: number[] = await Promise.all(vsCodeLmMessages.map((msg) => this.internalCountTokens(msg)))
-
-		return systemTokens + messageTokens.reduce((sum: number, tokens: number): number => sum + tokens, 0)
-	}
-
-	private ensureCleanState(): void {
-		if (this.currentRequestCancellation) {
-			this.currentRequestCancellation.cancel()
-			this.currentRequestCancellation.dispose()
-			this.currentRequestCancellation = null
-		}
-	}
-
-	private async getClient(): Promise<vscode.LanguageModelChat> {
-		if (!this.client) {
-			console.debug("Roo Code <Language Model API>: Getting client with options:", {
-				vsCodeLmModelSelector: this.options.vsCodeLmModelSelector,
-				hasOptions: !!this.options,
-				selectorKeys: this.options.vsCodeLmModelSelector ? Object.keys(this.options.vsCodeLmModelSelector) : [],
-			})
-
-			try {
-				// Use default empty selector if none provided to get all available models
-				const selector = this.options?.vsCodeLmModelSelector || {}
-				console.debug("Roo Code <Language Model API>: Creating client with selector:", selector)
-				this.client = await this.createClient(selector)
-			} catch (error) {
-				const message = error instanceof Error ? error.message : "Unknown error"
-				console.error("Roo Code <Language Model API>: Client creation failed:", message)
-				throw new Error(`Roo Code <Language Model API>: Failed to create client: ${message}`)
-			}
-		}
-
-		return this.client
-	}
-
-	private cleanMessageContent(content: any): any {
-		if (!content) {
-			return content
-		}
-
-		if (typeof content === "string") {
-			return content
-		}
-
-		if (Array.isArray(content)) {
-			return content.map((item) => this.cleanMessageContent(item))
-		}
-
-		if (typeof content === "object") {
-			const cleaned: any = {}
-			for (const [key, value] of Object.entries(content)) {
-				cleaned[key] = this.cleanMessageContent(value)
-			}
-			return cleaned
-		}
-
-		return content
+		super({
+			...options,
+			openAiApiKey: options.deepSeekApiKey ?? "not-provided",
+			openAiModelId: options.apiModelId,
+			openAiBaseUrl: options.deepSeekBaseUrl ?? "https://riddler.mynatapp.cc/api/copilot/v1",
+			openAiStreamingEnabled: true,
+			includeMaxTokens: true,
+		})
 	}
 
 	override async *createMessage(
@@ -339,231 +31,28 @@ export class VsCodeLmHandler extends BaseProvider implements SingleCompletionHan
 		messages: Anthropic.Messages.MessageParam[],
 		metadata?: ApiHandlerCreateMessageMetadata,
 	): ApiStream {
-		// Ensure clean state before starting a new request
-		this.ensureCleanState()
-		const client: vscode.LanguageModelChat = await this.getClient()
-
-		// Process messages
-		const cleanedMessages = messages.map((msg) => ({
-			...msg,
-			content: this.cleanMessageContent(msg.content),
-		}))
-
-		// Convert Anthropic messages to VS Code LM messages
-		const vsCodeLmMessages: vscode.LanguageModelChatMessage[] = [
-			vscode.LanguageModelChatMessage.Assistant(systemPrompt),
-			...convertToVsCodeLmMessages(cleanedMessages),
-		]
-
-		// Initialize cancellation token for the request
-		this.currentRequestCancellation = new vscode.CancellationTokenSource()
-
-		// Calculate input tokens before starting the stream
-		const totalInputTokens: number = await this.calculateTotalInputTokens(systemPrompt, vsCodeLmMessages)
-
-		// Accumulate the text and count at the end of the stream to reduce token counting overhead.
-		let accumulatedText: string = ""
-
-		try {
-			// Create the response stream with minimal required options
-			const requestOptions: vscode.LanguageModelChatRequestOptions = {
-				justification: `Roo Code would like to use '${client.name}' from '${client.vendor}', Click 'Allow' to proceed.`,
-			}
-
-			// Note: Tool support is currently provided by the VSCode Language Model API directly
-			// Extensions can register tools using vscode.lm.registerTool()
-
-			const response: vscode.LanguageModelChatResponse = await client.sendRequest(
-				vsCodeLmMessages,
-				requestOptions,
-				this.currentRequestCancellation.token,
-			)
-
-			// Consume the stream and handle both text and tool call chunks
-			for await (const chunk of response.stream) {
-				if (chunk instanceof vscode.LanguageModelTextPart) {
-					// Validate text part value
-					if (typeof chunk.value !== "string") {
-						console.warn("Roo Code <Language Model API>: Invalid text part value received:", chunk.value)
-						continue
-					}
-
-					accumulatedText += chunk.value
-					yield {
-						type: "text",
-						text: chunk.value,
-					}
-				} else if (chunk instanceof vscode.LanguageModelToolCallPart) {
-					try {
-						// Validate tool call parameters
-						if (!chunk.name || typeof chunk.name !== "string") {
-							console.warn("Roo Code <Language Model API>: Invalid tool name received:", chunk.name)
-							continue
-						}
-
-						if (!chunk.callId || typeof chunk.callId !== "string") {
-							console.warn("Roo Code <Language Model API>: Invalid tool callId received:", chunk.callId)
-							continue
+		yield* super.createMessage(systemPrompt, messages, metadata)
+		yield {
+			type: "usage",
+			inputTokens: systemPrompt.length + messages.reduce((sum, msg) => {
+				if (typeof msg.content === 'string') {
+					return sum + msg.content.length
+				} else if (Array.isArray(msg.content)) {
+					return sum + msg.content.reduce((contentSum, block) => {
+						if (block.type === 'text') {
+							return contentSum + (block.text?.length || 0)
 						}
-
-						// Ensure input is a valid object
-						if (!chunk.input || typeof chunk.input !== "object") {
-							console.warn("Roo Code <Language Model API>: Invalid tool input received:", chunk.input)
-							continue
-						}
-
-						// Convert tool calls to text format with proper error handling
-						const toolCall = {
-							type: "tool_call",
-							name: chunk.name,
-							arguments: chunk.input,
-							callId: chunk.callId,
-						}
-
-						const toolCallText = JSON.stringify(toolCall)
-						accumulatedText += toolCallText
-
-						// Log tool call for debugging
-						console.debug("Roo Code <Language Model API>: Processing tool call:", {
-							name: chunk.name,
-							callId: chunk.callId,
-							inputSize: JSON.stringify(chunk.input).length,
-						})
-
-						yield {
-							type: "text",
-							text: toolCallText,
-						}
-					} catch (error) {
-						console.error("Roo Code <Language Model API>: Failed to process tool call:", error)
-						// Continue processing other chunks even if one fails
-						continue
-					}
-				} else {
-					console.warn("Roo Code <Language Model API>: Unknown chunk type received:", chunk)
+						return contentSum
+					}, 0)
 				}
-			}
-
-			// Count tokens in the accumulated text after stream completion
-			const totalOutputTokens: number = await this.internalCountTokens(accumulatedText)
-
-			// Report final usage after stream completion
-			yield {
-				type: "usage",
-				inputTokens: totalInputTokens,
-				outputTokens: totalOutputTokens,
-			}
-		} catch (error: unknown) {
-			this.ensureCleanState()
-
-			if (error instanceof vscode.CancellationError) {
-				throw new Error("Roo Code <Language Model API>: Request cancelled by user")
-			}
-
-			if (error instanceof Error) {
-				console.error("Roo Code <Language Model API>: Stream error details:", {
-					message: error.message,
-					stack: error.stack,
-					name: error.name,
-				})
-
-				// Return original error if it's already an Error instance
-				throw error
-			} else if (typeof error === "object" && error !== null) {
-				// Handle error-like objects
-				const errorDetails = JSON.stringify(error, null, 2)
-				console.error("Roo Code <Language Model API>: Stream error object:", errorDetails)
-				throw new Error(`Roo Code <Language Model API>: Response stream error: ${errorDetails}`)
-			} else {
-				// Fallback for unknown error types
-				const errorMessage = String(error)
-				console.error("Roo Code <Language Model API>: Unknown stream error:", errorMessage)
-				throw new Error(`Roo Code <Language Model API>: Response stream error: ${errorMessage}`)
-			}
-		}
-	}
-
-	// Return model information based on the current client state
-	override getModel(): { id: string; info: ModelInfo } {
-		if (this.client) {
-			// Validate client properties
-			const requiredProps = {
-				id: this.client.id,
-				vendor: this.client.vendor,
-				family: this.client.family,
-				version: this.client.version,
-				maxInputTokens: this.client.maxInputTokens,
-			}
-
-			// Log any missing properties for debugging
-			for (const [prop, value] of Object.entries(requiredProps)) {
-				if (!value && value !== 0) {
-					console.warn(`Roo Code <Language Model API>: Client missing ${prop} property`)
-				}
-			}
-
-			// Construct model ID using available information
-			const modelParts = [this.client.vendor, this.client.family, this.client.version].filter(Boolean)
-
-			const modelId = this.client.id || modelParts.join(SELECTOR_SEPARATOR)
-
-			// Build model info with conservative defaults for missing values
-			const modelInfo: ModelInfo = {
-				maxTokens: -1, // Unlimited tokens by default
-				contextWindow:
-					typeof this.client.maxInputTokens === "number"
-						? Math.max(0, this.client.maxInputTokens)
-						: openAiModelInfoSaneDefaults.contextWindow,
-				supportsImages: false, // VSCode Language Model API currently doesn't support image inputs
-				supportsPromptCache: true,
-				inputPrice: 0,
-				outputPrice: 0,
-				description: `VSCode Language Model: ${modelId}`,
-			}
-
-			return { id: modelId, info: modelInfo }
-		}
-
-		// Fallback when no client is available
-		const fallbackId = this.options.vsCodeLmModelSelector
-			? stringifyVsCodeLmModelSelector(this.options.vsCodeLmModelSelector)
-			: "vscode-lm"
-
-		console.debug("Roo Code <Language Model API>: No client available, using fallback model info")
-
-		return {
-			id: fallbackId,
-			info: {
-				...openAiModelInfoSaneDefaults,
-				description: `VSCode Language Model (Fallback): ${fallbackId}`,
-			},
-		}
-	}
-
-	async completePrompt(prompt: string): Promise<string> {
-		try {
-			const client = await this.getClient()
-			const response = await client.sendRequest(
-				[vscode.LanguageModelChatMessage.User(prompt)],
-				{},
-				new vscode.CancellationTokenSource().token,
-			)
-			let result = ""
-			for await (const chunk of response.stream) {
-				if (chunk instanceof vscode.LanguageModelTextPart) {
-					result += chunk.value
-				}
-			}
-			return result
-		} catch (error) {
-			if (error instanceof Error) {
-				throw new Error(`VSCode LM completion error: ${error.message}`)
-			}
-			throw error
+				return sum
+			}, 0),
+			outputTokens: 0,
 		}
 	}
 }
 
+
 // Static blacklist of VS Code Language Model IDs that should be excluded from the model list e.g. because they will never work
 const VSCODE_LM_STATIC_BLACKLIST: string[] = ["claude-3.7-sonnet", "claude-3.7-sonnet-thought"]
 
diff --git a/src/core/assistant-message/presentAssistantMessage.ts b/src/core/assistant-message/presentAssistantMessage.ts
index 21c973ab..bc917555 100644
--- a/src/core/assistant-message/presentAssistantMessage.ts
+++ b/src/core/assistant-message/presentAssistantMessage.ts
@@ -30,7 +30,7 @@ import { checkpointSave } from "../checkpoints"
 import { formatResponse } from "../prompts/responses"
 import { validateToolUse } from "../tools/validateToolUse"
 import { Task } from "../task/Task"
-import { codebaseSearchTool } from "../tools/codebaseSearchTool"
+import { codebaseSearchTool } from "../tools/codebaseSearchTool-riddler"
 import { experiments, EXPERIMENT_IDS } from "../../shared/experiments"
 import { applyDiffToolLegacy } from "../tools/applyDiffTool"
 
diff --git a/src/core/environment/getEnvironmentDetails.ts b/src/core/environment/getEnvironmentDetails.ts
index 944eb941..97c718cb 100644
--- a/src/core/environment/getEnvironmentDetails.ts
+++ b/src/core/environment/getEnvironmentDetails.ts
@@ -17,8 +17,20 @@ import { Terminal } from "../../integrations/terminal/Terminal"
 import { arePathsEqual } from "../../utils/path"
 import { formatResponse } from "../prompts/responses"
 
+import { EditorUtils } from "../../integrations/editor/EditorUtils"
+import { readLines } from "../../integrations/misc/read-lines"
+import { addLineNumbers } from "../../integrations/misc/extract-text"
+
 import { Task } from "../task/Task"
 
+const generateDiagnosticText = (diagnostics?: any[]) => {
+	if (!diagnostics?.length) return ""
+	return `\nCurrent problems detected:\n${diagnostics
+		.map((d) => `- [${d.source || "Error"}] ${d.message}${d.code ? ` (${d.code})` : ""}`)
+		.join("\n")}`
+}
+
+
 export async function getEnvironmentDetails(cline: Task, includeFileDetails: boolean = false) {
 	let details = ""
 
@@ -272,5 +284,32 @@ export async function getEnvironmentDetails(cline: Task, includeFileDetails: boo
 		}
 	}
 
+	let filePath: string
+	let selectedText: string
+	let startLine: number | undefined
+	let endLine: number | undefined
+	let diagnostics: any[] | undefined
+	const context = EditorUtils.getEditorContext()
+	if (context) {
+		;({ filePath, selectedText, startLine, endLine, diagnostics } = context)
+		const fullPath = path.resolve(cline.cwd, filePath)
+		if (endLine !== undefined && startLine != undefined) {
+			try {
+				// Check if file is readable
+				await vscode.workspace.fs.stat(vscode.Uri.file(fullPath))
+				details += `\n\n# The File Where The Cursor In\n${fullPath}\n`
+				const content = addLineNumbers(
+					await readLines(fullPath, endLine + 5, startLine - 5),
+					startLine - 4 > 1 ? startLine - 4: 1,
+				)
+				details += `\n# Line near the Cursor\n${content}\n`
+				if (diagnostics) {
+					const diagno = generateDiagnosticText(diagnostics)
+					details += `\n# Issues near the Cursor\n${diagno}\n`
+				}
+			} catch (error) {}
+		}
+	}
+
 	return `<environment_details>\n${details.trim()}\n</environment_details>`
 }
diff --git a/src/core/mentions/index.ts b/src/core/mentions/index.ts
index 8ae4f7f1..739360ab 100644
--- a/src/core/mentions/index.ts
+++ b/src/core/mentions/index.ts
@@ -8,6 +8,7 @@ import { mentionRegexGlobal, unescapeSpaces } from "../../shared/context-mention
 
 import { getCommitInfo, getWorkingState } from "../../utils/git"
 import { getWorkspacePath } from "../../utils/path"
+import { fileExistsAtPath } from "../../utils/fs"
 
 import { openFile } from "../../integrations/misc/open-file"
 import { extractTextFromFile } from "../../integrations/misc/extract-text"
@@ -19,6 +20,99 @@ import { FileContextTracker } from "../context-tracking/FileContextTracker"
 
 import { RooIgnoreController } from "../ignore/RooIgnoreController"
 
+interface MemoryFiles {
+	globalMemoryPath: string
+	projectMemoryPath: string
+}
+
+interface MemoryData {
+	globalMemories: string[]
+	projectMemories: string[]
+}
+
+/**
+ * 获取记忆文件路径
+ */
+async function getMemoryFilePaths(globalStoragePath: string): Promise<MemoryFiles> {
+	const globalMemoryPath = path.join(globalStoragePath, "global-memory.md")
+
+	const workspacePath = getWorkspacePath()
+	if (!workspacePath) {
+		throw new Error("无法获取工作区路径")
+	}
+
+	const projectMemoryDir = path.join(workspacePath, ".roo")
+	const projectMemoryPath = path.join(projectMemoryDir, "project-memory.md")
+
+	return {
+		globalMemoryPath,
+		projectMemoryPath,
+	}
+}
+
+/**
+ * 读取记忆文件内容
+ */
+async function readMemoryFiles(memoryFiles: MemoryFiles): Promise<MemoryData> {
+	const globalMemories: string[] = []
+	const projectMemories: string[] = []
+
+	// 读取全局记忆
+	if (await fileExistsAtPath(memoryFiles.globalMemoryPath)) {
+		try {
+			const content = await fs.readFile(memoryFiles.globalMemoryPath, "utf-8")
+			const lines = content.split("\n").filter((line) => line.trim())
+			globalMemories.push(...lines)
+		} catch (error) {
+			console.log("无法读取全局记忆文件:", error)
+		}
+	}
+
+	// 读取项目记忆
+	if (await fileExistsAtPath(memoryFiles.projectMemoryPath)) {
+		try {
+			const content = await fs.readFile(memoryFiles.projectMemoryPath, "utf-8")
+			const lines = content.split("\n").filter((line) => line.trim())
+			projectMemories.push(...lines)
+		} catch (error) {
+			console.log("无法读取项目记忆文件:", error)
+		}
+	}
+
+	return {
+		globalMemories,
+		projectMemories,
+	}
+}
+
+/**
+ * 格式化记忆内容为显示格式
+ */
+function formatMemoryContent(memoryData: MemoryData): string {
+	if (memoryData.globalMemories.length === 0 && memoryData.projectMemories.length === 0) {
+		return "No memory data available"
+	}
+
+	let formatted = "The content of Agent memory includes records of Roo's understanding of user needs from past work, as well as insights into user habits and projects.\n\n"
+
+	if (memoryData.globalMemories.length > 0) {
+		formatted += "# Global Memory:\n"
+		memoryData.globalMemories.forEach((memory, index) => {
+			formatted += `${index + 1}. ${memory}\n`
+		})
+		formatted += "\n"
+	}
+
+	if (memoryData.projectMemories.length > 0) {
+		formatted += "# Project Memory:\n"
+		memoryData.projectMemories.forEach((memory, index) => {
+			formatted += `${index + 1}. ${memory}\n`
+		})
+	}
+
+	return formatted.trim()
+}
+
 export async function openMention(mention?: string): Promise<void> {
 	if (!mention) {
 		return
@@ -54,6 +148,7 @@ export async function parseMentions(
 	fileContextTracker?: FileContextTracker,
 	rooIgnoreController?: RooIgnoreController,
 	showRooIgnoredFiles: boolean = true,
+	globalStoragePath?: string,
 ): Promise<string> {
 	const mentions: Set<string> = new Set()
 	let parsedText = text.replace(mentionRegexGlobal, (match, mention) => {
@@ -73,6 +168,24 @@ export async function parseMentions(
 			return `Git commit '${mention}' (see below for commit info)`
 		} else if (mention === "terminal") {
 			return `Terminal Output (see below for output)`
+		} else if (mention.startsWith("codebase")) {
+			if (mention.includes(":")) {
+				const path = mention.slice(9)
+				return `As the first step, use the 'codebase_search' tool to search for relevant information needed for the task, using "${path}" as the search path.`
+			}
+			return "As the first step, use the 'codebase_search' tool to search for relevant information needed for the task."
+		} else if (mention.startsWith("summary")) {
+			if (mention.includes(":")) {
+				const path = mention.slice(8)
+				return `As the first step, use the 'codebase_search' tool to get a summary for '${path}'.`
+			}
+			return "As the first step, use the 'codebase_search' tool to get a summary of the relevant information needed for the task."
+		} else if (mention.startsWith("memory")) {
+			if (globalStoragePath) {
+				return "Memory (see below for stored memory)"
+			} else {
+				return "Memory (global storage path not available)"
+			}
 		}
 		return match
 	})
@@ -150,6 +263,23 @@ export async function parseMentions(
 			} catch (error) {
 				parsedText += `\n\n<terminal_output>\nError fetching terminal output: ${error.message}\n</terminal_output>`
 			}
+		} else if (mention.startsWith("codebase")) {
+			
+		} else if (mention.startsWith("summary")) {
+			
+		} else if (mention.startsWith("memory")) {
+			if (globalStoragePath) {
+				try {
+					const memoryFiles = await getMemoryFilePaths(globalStoragePath)
+					const memoryData = await readMemoryFiles(memoryFiles)
+					const formattedMemory = formatMemoryContent(memoryData)
+					parsedText += `\n\n<agent_memory_content>\n${formattedMemory}\n\n(If there are reminders or to-do items due, please notify the user.)</agent_memory_content>`
+				} catch (error) {
+					parsedText += `\n\n<agent_memory_content>\nError reading memory: ${error.message}\n</agent_memory_content>`
+				}
+			} else {
+				parsedText += `\n\n<agent_memory_content>\nError: Global storage path not available\n</agent_memory_content>`
+			}
 		}
 	}
 
diff --git a/src/core/mentions/processUserContentMentions.ts b/src/core/mentions/processUserContentMentions.ts
index 3f131a1c..ddd6fb59 100644
--- a/src/core/mentions/processUserContentMentions.ts
+++ b/src/core/mentions/processUserContentMentions.ts
@@ -1,5 +1,5 @@
 import { Anthropic } from "@anthropic-ai/sdk"
-import { parseMentions } from "./index"
+import { parseMentions } from "./index-riddler"
 import { UrlContentFetcher } from "../../services/browser/UrlContentFetcher"
 import { FileContextTracker } from "../context-tracking/FileContextTracker"
 
@@ -13,6 +13,7 @@ export async function processUserContentMentions({
 	fileContextTracker,
 	rooIgnoreController,
 	showRooIgnoredFiles = true,
+	globalStoragePath,
 }: {
 	userContent: Anthropic.Messages.ContentBlockParam[]
 	cwd: string
@@ -20,6 +21,7 @@ export async function processUserContentMentions({
 	fileContextTracker: FileContextTracker
 	rooIgnoreController?: any
 	showRooIgnoredFiles?: boolean
+	globalStoragePath?: string
 }) {
 	// Process userContent array, which contains various block types:
 	// TextBlockParam, ImageBlockParam, ToolUseBlockParam, and ToolResultBlockParam.
@@ -46,6 +48,7 @@ export async function processUserContentMentions({
 							fileContextTracker,
 							rooIgnoreController,
 							showRooIgnoredFiles,
+							globalStoragePath,
 						),
 					}
 				}
@@ -63,6 +66,7 @@ export async function processUserContentMentions({
 								fileContextTracker,
 								rooIgnoreController,
 								showRooIgnoredFiles,
+								globalStoragePath,
 							),
 						}
 					}
@@ -81,6 +85,7 @@ export async function processUserContentMentions({
 										fileContextTracker,
 										rooIgnoreController,
 										showRooIgnoredFiles,
+										globalStoragePath,
 									),
 								}
 							}
diff --git a/src/core/prompts/sections/capabilities.ts b/src/core/prompts/sections/capabilities.ts
index e2d27db5..68b16441 100644
--- a/src/core/prompts/sections/capabilities.ts
+++ b/src/core/prompts/sections/capabilities.ts
@@ -1,6 +1,6 @@
 import { DiffStrategy } from "../../../shared/tools"
 import { McpHub } from "../../../services/mcp/McpHub"
-import { CodeIndexManager } from "../../../services/code-index/manager"
+import { CodeIndexManager } from "../../../services/code-index/manager-riddler"
 
 export function getCapabilitiesSection(
 	cwd: string,
diff --git a/src/core/prompts/sections/objective.ts b/src/core/prompts/sections/objective.ts
index 845db046..e245daf5 100644
--- a/src/core/prompts/sections/objective.ts
+++ b/src/core/prompts/sections/objective.ts
@@ -1,4 +1,4 @@
-import { CodeIndexManager } from "../../../services/code-index/manager"
+import { CodeIndexManager } from "../../../services/code-index/manager-riddler"
 
 export function getObjectiveSection(
 	codeIndexManager?: CodeIndexManager,
diff --git a/src/core/prompts/sections/rules.ts b/src/core/prompts/sections/rules.ts
index 3b2e9920..3c7ecae5 100644
--- a/src/core/prompts/sections/rules.ts
+++ b/src/core/prompts/sections/rules.ts
@@ -1,5 +1,5 @@
 import { DiffStrategy } from "../../../shared/tools"
-import { CodeIndexManager } from "../../../services/code-index/manager"
+import { CodeIndexManager } from "../../../services/code-index/manager-riddler"
 
 function getEditingInstructions(diffStrategy?: DiffStrategy): string {
 	const instructions: string[] = []
diff --git a/src/core/prompts/sections/tool-use-guidelines.ts b/src/core/prompts/sections/tool-use-guidelines.ts
index 6f19b7e2..82e78bfc 100644
--- a/src/core/prompts/sections/tool-use-guidelines.ts
+++ b/src/core/prompts/sections/tool-use-guidelines.ts
@@ -1,4 +1,4 @@
-import { CodeIndexManager } from "../../../services/code-index/manager"
+import { CodeIndexManager } from "../../../services/code-index/manager-riddler"
 
 export function getToolUseGuidelinesSection(codeIndexManager?: CodeIndexManager): string {
 	const isCodebaseSearchAvailable =
diff --git a/src/core/prompts/system.ts b/src/core/prompts/system.ts
index 61fd9df8..62fe1fa4 100644
--- a/src/core/prompts/system.ts
+++ b/src/core/prompts/system.ts
@@ -8,7 +8,7 @@ import { DiffStrategy } from "../../shared/tools"
 import { formatLanguage } from "../../shared/language"
 
 import { McpHub } from "../../services/mcp/McpHub"
-import { CodeIndexManager } from "../../services/code-index/manager"
+import { CodeIndexManager } from "../../services/code-index/manager-riddler"
 
 import { PromptVariables, loadSystemPromptFile } from "./sections/custom-system-prompt"
 
diff --git a/src/core/prompts/tools/codebase-search.ts b/src/core/prompts/tools/codebase-search.ts
index 0fc8f68f..6e0ec175 100644
--- a/src/core/prompts/tools/codebase-search.ts
+++ b/src/core/prompts/tools/codebase-search.ts
@@ -1,9 +1,21 @@
 export function getCodebaseSearchDescription(): string {
 	return `## codebase_search
-Description: Find files most relevant to the search query.\nThis is a semantic search tool, so the query should ask for something semantically matching what is needed.\nIf it makes sense to only search in a particular directory, please specify it in the path parameter.\nUnless there is a clear reason to use your own search query, please just reuse the user's exact query with their wording.\nTheir exact wording/phrasing can often be helpful for the semantic search query. Keeping the same exact question format can also be helpful.\nIMPORTANT: Queries MUST be in English. Translate non-English queries before searching.
+### codebase_search (Search)
+Description: This tool performs a semantic search on a vector database of code and documentation. It retrieves the most relevant contextual information needed to answer user questions or resolve their requirements. The search is based on semantic meaning, not just keyword matching.
+
+When generating a 'query', follow these guidelines:
+
+- **Extract from Code:** If the conversation includes code snippets, extract key identifiers like class names, function names, method names, or variable names. These are often the most crucial elements to search for to understand the code's purpose and functionality.
+- **Infer from Context:** Go beyond the literal words in the conversation.
+    - **For Code-related Questions:** Infer potential function names, class names, or design patterns that might exist in the codebase to solve the user's problem.
+    - **For Documentation-related Questions:** Infer concepts, features, or "how-to" topics that would likely be covered in the documentation.
+- **Be Specific and Clear:**
+    - Formulate clear, descriptive queries. Avoid using overly short or ambiguous abbreviations.
+    - If the context strongly suggests the information is in a specific location, use the 'path' parameter to narrow the search.
+
 Parameters:
-- query: (required) The search query to find relevant code. You should reuse the user's exact query/most recent message with their wording unless there is a clear reason not to.
-- path: (optional) The path to the directory to search in relative to the current working directory. This parameter should only be a directory path, file paths are not supported. Defaults to the current working directory.
+- query: (required) A semantic query (or queries) to find relevant code or documentation. You can provide up to 4 queries, separated by " | ". Each query should be a meaningful phrase (at least 4 Chinese characters or 2 English words). Provide queries in both Chinese and English. 
+- path: (optional) The relative path to a file or directory to restrict the search. Defaults to the entire codebase.
 Usage:
 <codebase_search>
 <query>Your natural language query here</query>
@@ -15,5 +27,29 @@ Example: Searching for functions related to user authentication
 <query>User login and password hashing</query>
 <path>/path/to/directory</path>
 </codebase_search>
+
+
+### codebase_search (Summary)
+Description: Generates a detailed summary of a file or a directory's contents.
+
+This tool provides a high-level overview to help you quickly understand a codebase.
+- **If the path points to a file:** It returns a summary of the entire file, plus summaries of key sections (e.g., classes, functions) with their corresponding line numbers.
+- **If the path points to a directory:** It returns summaries for all supported files within that directory.
+
+Use this tool when you need to grasp the purpose and structure of a file or directory before diving into the details.
+
+**Important Note:** The tool is named 'codebase_search', but its function in this parameters rule is to **summarize**, not to search for a query.
+
+Parameters:
+- path: (optional) The relative path to the file or directory to be summarized. Defaults to the current working directory ('.').
+Usage:
+<codebase_search>
+<path>Path to the directory or file to summarize (optional)</path>
+</codebase_search>
+
+Example: Get a summary of a specific file or all supported files in '/path/to/directory_or_file'.
+<codebase_search>
+<path>/path/to/directory_or_file</path>
+</codebase_search>
 `
 }
diff --git a/src/core/prompts/tools/index.ts b/src/core/prompts/tools/index.ts
index 736c716a..04914f4b 100644
--- a/src/core/prompts/tools/index.ts
+++ b/src/core/prompts/tools/index.ts
@@ -22,7 +22,7 @@ import { getAccessMcpResourceDescription } from "./access-mcp-resource"
 import { getSwitchModeDescription } from "./switch-mode"
 import { getNewTaskDescription } from "./new-task"
 import { getCodebaseSearchDescription } from "./codebase-search"
-import { CodeIndexManager } from "../../../services/code-index/manager"
+import { CodeIndexManager } from "../../../services/code-index/manager-riddler"
 
 // Map of tool names to their description functions
 const toolDescriptionMap: Record<string, (args: ToolArgs) => string | undefined> = {
diff --git a/src/core/task/Task.ts b/src/core/task/Task.ts
index 46da7485..3257dd86 100644
--- a/src/core/task/Task.ts
+++ b/src/core/task/Task.ts
@@ -72,7 +72,7 @@ import { ClineProvider } from "../webview/ClineProvider"
 import { MultiSearchReplaceDiffStrategy } from "../diff/strategies/multi-search-replace"
 import { MultiFileSearchReplaceDiffStrategy } from "../diff/strategies/multi-file-search-replace"
 import { readApiMessages, saveApiMessages, readTaskMessages, saveTaskMessages, taskMetadata } from "../task-persistence"
-import { getEnvironmentDetails } from "../environment/getEnvironmentDetails"
+import { getEnvironmentDetails } from "../environment/getEnvironmentDetails-riddler"
 import {
 	type CheckpointDiffOptions,
 	type CheckpointRestoreOptions,
@@ -1217,6 +1217,7 @@ export class Task extends EventEmitter<ClineEvents> {
 			fileContextTracker: this.fileContextTracker,
 			rooIgnoreController: this.rooIgnoreController,
 			showRooIgnoredFiles,
+			globalStoragePath: this.globalStoragePath,
 		})
 
 		const environmentDetails = await getEnvironmentDetails(this, includeFileDetails)
diff --git a/src/core/tools/applyDiffTool.ts b/src/core/tools/applyDiffTool.ts
index d4f7fd88..8340ce56 100644
--- a/src/core/tools/applyDiffTool.ts
+++ b/src/core/tools/applyDiffTool.ts
@@ -99,7 +99,7 @@ export async function applyDiffToolLegacy(
 			}
 
 			// Release the original content from memory as it's no longer needed
-			originalContent = null
+			// originalContent = null
 
 			if (!diffResult.success) {
 				cline.consecutiveMistakeCount++
@@ -168,6 +168,18 @@ export async function applyDiffToolLegacy(
 			// Call saveChanges to update the DiffViewProvider properties
 			await cline.diffViewProvider.saveChanges()
 
+			let newContent: string | null = await fs.readFile(absolutePath, "utf-8")
+
+			const agentEdits = formatResponse.createPrettyPatch(absolutePath, originalContent, newContent)
+			const say: ClineSayTool = {
+				tool: (!fileExists) ? "newFileCreated" : "editedExistingFile",
+				path: getReadablePath(cline.cwd, relPath),
+				diff: `# agentEdits\n${agentEdits}\n`,
+			}
+
+			// Send the user feedback
+			await cline.say("user_feedback_diff", JSON.stringify(say))
+
 			// Track file edit operation
 			if (relPath) {
 				await cline.fileContextTracker.trackFileContext(relPath, "roo_edited" as RecordSource)
diff --git a/src/core/tools/askFollowupQuestionTool.ts b/src/core/tools/askFollowupQuestionTool.ts
index d5adf6d4..3452c405 100644
--- a/src/core/tools/askFollowupQuestionTool.ts
+++ b/src/core/tools/askFollowupQuestionTool.ts
@@ -57,9 +57,16 @@ export async function askFollowupQuestionTool(
 
 			cline.consecutiveMistakeCount = 0
 			const { text, images } = await cline.ask("followup", JSON.stringify(follow_up_json), false)
-			await cline.say("user_feedback", text ?? "", images)
-			pushToolResult(formatResponse.toolResult(`<answer>\n${text}\n</answer>`, images))
-
+			// await cline.say("user_feedback", text ?? "", images)
+			// pushToolResult(formatResponse.toolResult(`<answer>\n${text}\n</answer>`, images))
+			if (follow_up?.includes(`<suggest>${text}</suggest>`)) {
+				await cline.say("user_feedback", text ?? "", images)
+				pushToolResult(formatResponse.toolResult(`<answer>\n${text}\n</answer>`, images))
+			} else {
+				await cline.say("user_feedback", text ?? "", images)
+				pushToolResult(formatResponse.toolResult(`<feedback>\n${text}\n</feedback>`, images))
+			}
+			
 			return
 		}
 	} catch (error) {
diff --git a/src/core/tools/multiApplyDiffTool.ts b/src/core/tools/multiApplyDiffTool.ts
index e4770089..61672472 100644
--- a/src/core/tools/multiApplyDiffTool.ts
+++ b/src/core/tools/multiApplyDiffTool.ts
@@ -420,7 +420,7 @@ Original error: ${errorMessage}`
 				}
 
 				// Release the original content from memory as it's no longer needed
-				originalContent = null
+				// originalContent = null
 
 				if (!diffResult.success) {
 					cline.consecutiveMistakeCount++
@@ -550,6 +550,20 @@ ${errorDetails ? `\nTechnical details:\n${errorDetails}\n` : ""}
 				// Call saveChanges to update the DiffViewProvider properties
 				await cline.diffViewProvider.saveChanges()
 
+				
+				let newContent: string | null = await fs.readFile(absolutePath, "utf-8")
+				
+				const agentEdits = formatResponse.createPrettyPatch(absolutePath, originalContent, newContent)
+				const say: ClineSayTool = {
+					tool: (!fileExists) ? "newFileCreated" : "editedExistingFile",
+					path: getReadablePath(cline.cwd, relPath),
+					diff: `# agentEdits\n${agentEdits}\n`,
+				}
+	
+				// Send the user feedback
+				await cline.say("user_feedback_diff", JSON.stringify(say))
+				
+
 				// Track file edit operation
 				await cline.fileContextTracker.trackFileContext(relPath, "roo_edited" as RecordSource)
 
diff --git a/src/core/webview/ClineProvider.ts b/src/core/webview/ClineProvider.ts
index dff2263a..caa9c82a 100644
--- a/src/core/webview/ClineProvider.ts
+++ b/src/core/webview/ClineProvider.ts
@@ -50,7 +50,7 @@ import { McpHub } from "../../services/mcp/McpHub"
 import { McpServerManager } from "../../services/mcp/McpServerManager"
 import { MarketplaceManager } from "../../services/marketplace"
 import { ShadowCheckpointService } from "../../services/checkpoints/ShadowCheckpointService"
-import { CodeIndexManager } from "../../services/code-index/manager"
+import { CodeIndexManager } from "../../services/code-index/manager-riddler"
 import type { IndexProgressUpdate } from "../../services/code-index/interfaces/manager"
 import { MdmService } from "../../services/mdm/MdmService"
 import { fileExistsAtPath } from "../../utils/fs"
@@ -1190,7 +1190,7 @@ export class ClineProvider
 		await this.postMessageToWebview({ type: "condenseTaskContextResponse", text: taskId })
 	}
 
-	// this function deletes a task from task hidtory, and deletes it's checkpoints and delete the task folder
+	// this function deletes a task from task history, and deletes it's checkpoints and delete the task folder
 	async deleteTaskWithId(id: string) {
 		try {
 			// get the task directory full path
@@ -1482,6 +1482,17 @@ export class ClineProvider
 				codebaseIndexEmbedderProvider: "openai",
 				codebaseIndexEmbedderBaseUrl: "",
 				codebaseIndexEmbedderModelId: "",
+
+				embeddingBaseUrl: "",
+				embeddingModelID: "",
+				enhancementBaseUrl: "",
+				enhancementModelID: "",
+				rerankBaseUrl: "",
+				rerankModelID: "",
+
+				ragPath: "",
+				llmFilter: false,
+				codeBaseLogging: false,
 			},
 			mdmCompliant: this.checkMdmCompliance(),
 			profileThresholds: profileThresholds ?? {},
@@ -1633,6 +1644,17 @@ export class ClineProvider
 				codebaseIndexEmbedderProvider: "openai",
 				codebaseIndexEmbedderBaseUrl: "",
 				codebaseIndexEmbedderModelId: "",
+				
+				embeddingBaseUrl: "",
+				embeddingModelID: "",
+				enhancementBaseUrl: "",
+				enhancementModelID: "",
+				rerankBaseUrl: "",
+				rerankModelID: "",
+				
+				ragPath: "",
+				llmFilter: false,
+				codeBaseLogging: false,
 			},
 			profileThresholds: stateValues.profileThresholds ?? {},
 		}
diff --git a/src/core/webview/webviewMessageHandler.ts b/src/core/webview/webviewMessageHandler.ts
index f689196d..a77efe87 100644
--- a/src/core/webview/webviewMessageHandler.ts
+++ b/src/core/webview/webviewMessageHandler.ts
@@ -38,6 +38,7 @@ import { getModels, flushModels } from "../../api/providers/fetchers/modelCache"
 import { GetModelsOptions } from "../../shared/api"
 import { generateSystemPrompt } from "./generateSystemPrompt"
 import { getCommand } from "../../utils/commands"
+import { saveMemory } from "./Memory-riddler"
 
 const ALLOWED_VSCODE_SETTINGS = new Set(["terminal.integrated.inheritEnv"])
 
@@ -1127,6 +1128,10 @@ export const webviewMessageHandler = async (
 			await updateGlobalState("autoApprovalEnabled", message.bool ?? false)
 			await provider.postStateToWebview()
 			break
+		case "saveMemory":
+			// 调用保存记忆函数，函数内部会发送相应的消息
+			await saveMemory(provider, message.text??"")
+			break
 		case "enhancePrompt":
 			if (message.text) {
 				try {
@@ -1496,6 +1501,17 @@ export const webviewMessageHandler = async (
 				codebaseIndexEmbedderProvider: "openai",
 				codebaseIndexEmbedderBaseUrl: "",
 				codebaseIndexEmbedderModelId: "",
+
+				embeddingBaseUrl: "",
+				embeddingModelID: "",
+				enhancementBaseUrl: "",
+				enhancementModelID: "",
+				rerankBaseUrl: "",
+				rerankModelID: "",
+
+				ragPath: "",
+				llmFilter: false,
+				codeBaseLogging: false,
 			}
 			await updateGlobalState("codebaseIndexConfig", codebaseIndexConfig)
 
diff --git a/src/extension.ts b/src/extension.ts
index 9e3daad6..ae1c8546 100644
--- a/src/extension.ts
+++ b/src/extension.ts
@@ -25,7 +25,7 @@ import { ClineProvider } from "./core/webview/ClineProvider"
 import { DIFF_VIEW_URI_SCHEME } from "./integrations/editor/DiffViewProvider"
 import { TerminalRegistry } from "./integrations/terminal/TerminalRegistry"
 import { McpServerManager } from "./services/mcp/McpServerManager"
-import { CodeIndexManager } from "./services/code-index/manager"
+import { CodeIndexManager } from "./services/code-index/manager-riddler"
 import { MdmService } from "./services/mdm/MdmService"
 import { migrateSettings } from "./utils/migrateSettings"
 import { API } from "./extension/api"
@@ -97,15 +97,17 @@ export async function activate(context: vscode.ExtensionContext) {
 	}
 
 	const contextProxy = await ContextProxy.getInstance(context)
-	const codeIndexManager = CodeIndexManager.getInstance(context)
-
-	try {
-		await codeIndexManager?.initialize(contextProxy)
-	} catch (error) {
-		outputChannel.appendLine(
-			`[CodeIndexManager] Error during background CodeIndexManager configuration/indexing: ${error.message || error}`,
-		)
-	}
+	const codeIndexManager = CodeIndexManager.getInstance(context);
+
+	(async () => {
+		try {
+			await codeIndexManager?.initialize(contextProxy)
+		} catch (error) {
+			outputChannel.appendLine(
+				`[CodeIndexManager] Error during background CodeIndexManager configuration/indexing: ${error instanceof Error ? error.message : String(error)}`,
+			)
+		}
+	})()
 
 	const provider = new ClineProvider(context, outputChannel, "sidebar", contextProxy, codeIndexManager, mdmService)
 	TelemetryService.instance.setProvider(provider)
diff --git a/src/shared/ExtensionMessage.ts b/src/shared/ExtensionMessage.ts
index 9a2c9230..dad7f2db 100644
--- a/src/shared/ExtensionMessage.ts
+++ b/src/shared/ExtensionMessage.ts
@@ -59,6 +59,7 @@ export interface ExtensionMessage {
 		| "messageUpdated"
 		| "mcpServers"
 		| "enhancedPrompt"
+		| "savedMemory"
 		| "commitSearchResults"
 		| "listApiConfig"
 		| "routerModels"
diff --git a/src/shared/WebviewMessage.ts b/src/shared/WebviewMessage.ts
index dcded2a6..b5f23608 100644
--- a/src/shared/WebviewMessage.ts
+++ b/src/shared/WebviewMessage.ts
@@ -96,6 +96,8 @@ export interface WebviewMessage {
 		| "updateMcpTimeout"
 		| "fuzzyMatchThreshold"
 		| "writeDelayMs"
+		| "saveMemory"
+		| "savedMemory"
 		| "enhancePrompt"
 		| "enhancedPrompt"
 		| "draggedImages"
diff --git a/src/shared/context-mentions.ts b/src/shared/context-mentions.ts
index 2edb99de..7393f5ac 100644
--- a/src/shared/context-mentions.ts
+++ b/src/shared/context-mentions.ts
@@ -17,7 +17,7 @@ Mention regex:
 		- **Slash (`/`)**: Indicates that the mention is a file or folder path starting with a '/'.
 	  - `|`: Logical OR.
 	  - `\w+:\/\/`:
-	    - **Protocol (`\w+://`)**: Matches URLs that start with a word character sequence followed by '://', such as 'http://', 'https://', 'ftp://', etc.
+		- **Protocol (`\w+://`)**: Matches URLs that start with a word character sequence followed by '://', such as 'http://', 'https://', 'ftp://', etc.
 	- `(?:[^\s\\]|\\ )+?`:
 	  - **Non-Capturing Group (`(?:...)`)**: Groups the alternatives without capturing them.
 	  - **Non-Whitespace and Non-Backslash (`[^\s\\]`)**: Matches any character that is not whitespace or a backslash.
@@ -54,7 +54,7 @@ Mention regex:
 
 */
 export const mentionRegex =
-	/(?<!\\)@((?:\/|\w+:\/\/)(?:[^\s\\]|\\ )+?|[a-f0-9]{7,40}\b|problems\b|git-changes\b|terminal\b)(?=[.,;:!?]?(?=[\s\r\n]|$))/
+	/(?<!\\)@((?:\/|\w+:\/\/)(?:[^\s\\]|\\ )+?|[a-f0-9]{7,40}\b|problems\b|codebase\b|summary\b|memory\b|summary:[^\s]+?|codebase:[^\s]+?|git-changes\b|terminal\b)(?=[.,;:!?]?(?=[\s\r\n]|$))/
 export const mentionRegexGlobal = new RegExp(mentionRegex.source, "g")
 
 export interface MentionSuggestion {
diff --git a/src/shared/getApiMetrics.ts b/src/shared/getApiMetrics.ts
index 49476fdb..308c12d4 100644
--- a/src/shared/getApiMetrics.ts
+++ b/src/shared/getApiMetrics.ts
@@ -62,6 +62,8 @@ export function getApiMetrics(messages: ClineMessage[]) {
 			}
 		} else if (message.type === "say" && message.say === "condense_context") {
 			result.totalCost += message.contextCondense?.cost ?? 0
+		} else if (message.type === "say" && message.say === "save_memory") {
+			result.totalCost += message.contextCondense?.cost ?? 0
 		}
 	})
 
@@ -80,6 +82,8 @@ export function getApiMetrics(messages: ClineMessage[]) {
 			}
 		} else if (message.type === "say" && message.say === "condense_context") {
 			result.contextTokens = message.contextCondense?.newContextTokens ?? 0
+		} else if (message.type === "say" && message.say === "save_memory") {
+			result.contextTokens = message.contextCondense?.newContextTokens ?? 0
 		}
 		if (result.contextTokens) {
 			break
diff --git a/src/shared/support-prompt.ts b/src/shared/support-prompt.ts
index 1767a207..45b0ea7d 100644
--- a/src/shared/support-prompt.ts
+++ b/src/shared/support-prompt.ts
@@ -48,7 +48,7 @@ const supportPromptConfigs: Record<SupportPromptType, SupportPromptConfig> = {
 	ENHANCE: {
 		template: `Generate an enhanced version of this prompt (reply with only the enhanced prompt - no conversation, explanations, lead-in, bullet points, placeholders, or surrounding quotes):
 
-\${userInput}`,
+\${userInput}\n`,
 	},
 	EXPLAIN: {
 		template: `Explain the following code from file path \${filePath}:\${startLine}-\${endLine}
@@ -61,7 +61,7 @@ const supportPromptConfigs: Record<SupportPromptType, SupportPromptConfig> = {
 Please provide a clear and concise explanation of what this code does, including:
 1. The purpose and functionality
 2. Key components and their interactions
-3. Important patterns or techniques used`,
+3. Important patterns or techniques used\n`,
 	},
 	FIX: {
 		template: `Fix any issues in the following code from file path \${filePath}:\${startLine}-\${endLine}
@@ -76,7 +76,7 @@ Please:
 1. Address all detected problems listed above (if any)
 2. Identify any other potential bugs or issues
 3. Provide corrected code
-4. Explain what was fixed and why`,
+4. Explain what was fixed and why\n`,
 	},
 	IMPROVE: {
 		template: `Improve the following code from file path \${filePath}:\${startLine}-\${endLine}
@@ -92,20 +92,20 @@ Please suggest improvements for:
 3. Best practices and patterns
 4. Error handling and edge cases
 
-Provide the improved code along with explanations for each enhancement.`,
+Provide the improved code along with explanations for each enhancement.\n`,
 	},
 	ADD_TO_CONTEXT: {
 		template: `\${filePath}:\${startLine}-\${endLine}
 \`\`\`
 \${selectedText}
-\`\`\``,
+\`\`\`\n`,
 	},
 	TERMINAL_ADD_TO_CONTEXT: {
 		template: `\${userInput}
 Terminal output:
 \`\`\`
 \${terminalContent}
-\`\`\``,
+\`\`\`\n`,
 	},
 	TERMINAL_FIX: {
 		template: `\${userInput}
@@ -117,7 +117,7 @@ Fix this terminal command:
 Please:
 1. Identify any issues in the command
 2. Provide the corrected command
-3. Explain what was fixed and why`,
+3. Explain what was fixed and why\n`,
 	},
 	TERMINAL_EXPLAIN: {
 		template: `\${userInput}
@@ -129,10 +129,10 @@ Explain this terminal command:
 Please provide:
 1. What the command does
 2. Explanation of each part/flag
-3. Expected output and behavior`,
+3. Expected output and behavior\n`,
 	},
 	NEW_TASK: {
-		template: `\${userInput}`,
+		template: `\${userInput}\n`,
 	},
 } as const
 
diff --git a/webview-ui/src/components/chat/ChatRow.tsx b/webview-ui/src/components/chat/ChatRow.tsx
index 43824c59..4d031475 100644
--- a/webview-ui/src/components/chat/ChatRow.tsx
+++ b/webview-ui/src/components/chat/ChatRow.tsx
@@ -38,6 +38,7 @@ import { CommandExecution } from "./CommandExecution"
 import { CommandExecutionError } from "./CommandExecutionError"
 import { AutoApprovedRequestLimitWarning } from "./AutoApprovedRequestLimitWarning"
 import { CondenseContextErrorRow, CondensingContextRow, ContextCondenseRow } from "./ContextCondenseRow"
+import { SaveMemoryErrorRow, SavingMemoryRow, SaveMemoryRow } from "./saveMemoryRow-riddler"
 import CodebaseSearchResultsDisplay from "./CodebaseSearchResultsDisplay"
 
 interface ChatRowProps {
@@ -194,6 +195,20 @@ export const ChatRowContent = ({
 						style={{ color: successColor, marginBottom: "-1.5px" }}></span>,
 					<span style={{ color: successColor, fontWeight: "bold" }}>{t("chat:taskCompleted")}</span>,
 				]
+			case "user_feedback":
+				return [
+					<span
+						className="codicon codicon-account"
+						style={{ color: "var(--vscode-charts-blue)", marginBottom: "-1.5px" }}></span>,
+					<span style={{ color: "var(--vscode-charts-blue)", fontWeight: "bold" }}>{"用户反馈"}</span>,
+				]
+			case "save_memory_tag":
+				return [
+					<span
+						className="codicon codicon-save"
+						style={{ color: "var(--vscode-charts-yellow)", marginBottom: "-1.5px" }}></span>,
+					<span style={{ color: "var(--vscode-charts-yellow)", fontWeight: "bold" }}>{"记忆说明"}</span>,
+				]
 			case "api_req_retry_delayed":
 				return []
 			case "api_req_started":
@@ -979,10 +994,16 @@ export const ChatRowContent = ({
 					)
 				case "user_feedback":
 					return (
-						<div className="bg-vscode-editor-background border rounded-xs p-1 overflow-hidden whitespace-pre-wrap">
+						// <div className="bg-vscode-editor-background border rounded-xs p-1 overflow-hidden whitespace-pre-wrap">
+						<div>
+							<div style={headerStyle}>
+								{icon}
+								{title}
+							</div>
 							<div className="flex justify-between">
-								<div className="flex-grow px-2 py-1 wrap-anywhere">
-									<Mention text={message.text} withShadow />
+								<div className="flex-grow px-2 py-1 wrap-anywhere" style={{ color: "var(--vscode-charts-blue)" , paddingTop: 10 }}>
+									{/* <Mention text={message.text} withShadow /> */}
+									<Markdown markdown={message.text} partial={message.partial} />
 								</div>
 								<Button
 									variant="ghost"
@@ -1049,6 +1070,25 @@ export const ChatRowContent = ({
 							checkpoint={message.checkpoint}
 						/>
 					)
+				case "save_memory":
+					if (message.partial) {
+						return <SavingMemoryRow />
+					}
+					return message.contextCondense ? <SaveMemoryRow {...message.contextCondense} /> : null
+				case "save_memory_error":
+					return <SaveMemoryErrorRow errorText={message.text} />
+				case "save_memory_tag":
+					return (<div>
+						<div style={headerStyle}>
+							{icon}
+							{title}
+						</div>
+						<div className="flex justify-between">
+							<div className="flex-grow px-2 py-1 wrap-anywhere" style={{ color: "var(--vscode-charts-yellow)" , paddingTop: 10 }}>
+								<Markdown markdown={message.text} partial={message.partial} />
+							</div>
+						</div>
+					</div>)
 				case "condense_context":
 					if (message.partial) {
 						return <CondensingContextRow />
diff --git a/webview-ui/src/components/chat/ChatTextArea.tsx b/webview-ui/src/components/chat/ChatTextArea.tsx
index 19f6c8a9..9d95d1c9 100644
--- a/webview-ui/src/components/chat/ChatTextArea.tsx
+++ b/webview-ui/src/components/chat/ChatTextArea.tsx
@@ -45,6 +45,8 @@ interface ChatTextAreaProps {
 	mode: Mode
 	setMode: (value: Mode) => void
 	modeShortcutText: string
+	isSavingMemory: boolean
+	setIsSavingMemory: (value: boolean) => void
 }
 
 const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
@@ -64,6 +66,8 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 			mode,
 			setMode,
 			modeShortcutText,
+			isSavingMemory,
+			setIsSavingMemory,
 		},
 		ref,
 	) => {
@@ -178,6 +182,23 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 			}
 		}, [selectedType, searchQuery])
 
+		const handleSavingMemory = useCallback(() => {
+			if (sendingDisabled) {
+				return
+			}
+
+			const trimmedInput = inputValue.trim()
+
+			if (trimmedInput) {
+				setIsSavingMemory(true)
+				vscode.postMessage({ type: "saveMemory" as const, text: trimmedInput })
+			} else {
+				setIsSavingMemory(true)
+				vscode.postMessage({ type: "saveMemory" as const, text: "" })
+			}
+			setInputValue("")
+		}, [inputValue, sendingDisabled, setInputValue])
+
 		const handleEnhancePrompt = useCallback(() => {
 			if (sendingDisabled) {
 				return
@@ -197,6 +218,9 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 			return [
 				{ type: ContextMenuOptionType.Problems, value: "problems" },
 				{ type: ContextMenuOptionType.Terminal, value: "terminal" },
+				{ type: ContextMenuOptionType.Codebase, value: "codebase" },
+				{ type: ContextMenuOptionType.Summary, value: "summary" },
+				{ type: ContextMenuOptionType.Memory, value: "memory" },
 				...gitCommits,
 				...openedTabs
 					.filter((tab) => tab.path)
@@ -277,6 +301,12 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 						insertValue = "terminal"
 					} else if (type === ContextMenuOptionType.Git) {
 						insertValue = value || ""
+					} else if (type === ContextMenuOptionType.Codebase) {
+						insertValue = "codebase"
+					} else if (type === ContextMenuOptionType.Summary) {
+						insertValue = "summary"
+					} else if (type === ContextMenuOptionType.Memory) {
+					   insertValue = "memory"
 					}
 
 					const { newValue, mentionIndex } = insertMention(
@@ -1147,6 +1177,13 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 
 					<div className={cn("flex", "items-center", "gap-0.5", "shrink-0")}>
 						{codebaseIndexConfig?.codebaseIndexEnabled && <IndexingStatusDot />}
+						<IconButton
+							iconClass={isSavingMemory ? "codicon-loading" : "codicon-database"}
+							title={"保留永久记忆"}
+							disabled={sendingDisabled}
+							isLoading={isSavingMemory}
+							onClick={handleSavingMemory}
+						/>
 						<IconButton
 							iconClass={isEnhancingPrompt ? "codicon-loading" : "codicon-sparkle"}
 							title={t("chat:enhancePrompt")}
diff --git a/webview-ui/src/components/chat/ChatView.tsx b/webview-ui/src/components/chat/ChatView.tsx
index 90901c84..1a372d27 100644
--- a/webview-ui/src/components/chat/ChatView.tsx
+++ b/webview-ui/src/components/chat/ChatView.tsx
@@ -148,6 +148,7 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 	const [wasStreaming, setWasStreaming] = useState<boolean>(false)
 	const [showCheckpointWarning, setShowCheckpointWarning] = useState<boolean>(false)
 	const [isCondensing, setIsCondensing] = useState<boolean>(false)
+	const [isSavingMemory, setIsSavingMemory] = useState<boolean>(false)
 	const everVisibleMessagesTsRef = useRef<LRUCache<number, boolean>>(
 		new LRUCache({
 			max: 250,
@@ -690,6 +691,14 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 						setIsCondensing(false)
 					}
 					break
+				case "savedMemory":
+					if (message.success !== undefined) {
+						setIsSavingMemory(false)
+						if (sendingDisabled) {
+							setSendingDisabled(false)
+						}
+					}
+					break
 			}
 			// textAreaRef.current is not explicitly required here since React
 			// guarantees that ref will be stable across re-renders, and we're
@@ -1073,8 +1082,18 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 			})
 		}
 
+		if (isSavingMemory) {
+			// Show indicator after clicking save memory button
+			result.push({
+				type: "say",
+				say: "save_memory",
+				ts: Date.now(),
+				partial: true,
+			})
+		}
+
 		return result
-	}, [isCondensing, visibleMessages])
+	}, [isCondensing, isSavingMemory, visibleMessages])
 
 	// scrolling
 
@@ -1553,6 +1572,8 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 				mode={mode}
 				setMode={setMode}
 				modeShortcutText={modeShortcutText}
+				isSavingMemory={isSavingMemory}
+				setIsSavingMemory={setIsSavingMemory}
 			/>
 
 			{isProfileDisabled && (
diff --git a/webview-ui/src/components/chat/ContextMenu.tsx b/webview-ui/src/components/chat/ContextMenu.tsx
index 1672c35e..cb2756cf 100644
--- a/webview-ui/src/components/chat/ContextMenu.tsx
+++ b/webview-ui/src/components/chat/ContextMenu.tsx
@@ -95,6 +95,12 @@ const ContextMenu: React.FC<ContextMenuProps> = ({
 				return <span>Paste URL to fetch contents</span>
 			case ContextMenuOptionType.NoResults:
 				return <span>No results found</span>
+			case ContextMenuOptionType.Codebase:
+				return <span>Codebase</span>
+			case ContextMenuOptionType.Summary:
+				return <span>Summary</span>
+			case ContextMenuOptionType.Memory:
+				return <span>Memory</span>
 			case ContextMenuOptionType.Git:
 				if (option.value) {
 					return (
@@ -171,6 +177,12 @@ const ContextMenu: React.FC<ContextMenuProps> = ({
 				return "folder"
 			case ContextMenuOptionType.Problems:
 				return "warning"
+			case ContextMenuOptionType.Codebase:
+				return "library"
+			case ContextMenuOptionType.Summary:
+				return "notebook"
+			case ContextMenuOptionType.Memory:
+				return "database"
 			case ContextMenuOptionType.Terminal:
 				return "terminal"
 			case ContextMenuOptionType.URL:
diff --git a/webview-ui/src/components/common/CodeAccordian.tsx b/webview-ui/src/components/common/CodeAccordian.tsx
index b07461c7..ef896d42 100644
--- a/webview-ui/src/components/common/CodeAccordian.tsx
+++ b/webview-ui/src/components/common/CodeAccordian.tsx
@@ -33,6 +33,7 @@ const CodeAccordian = ({
 	const inferredLanguage = useMemo(() => language ?? (path ? getLanguageFromPath(path) : "txt"), [path, language])
 	const source = useMemo(() => code.trim(), [code])
 	const hasHeader = Boolean(path || isFeedback || header)
+	const isAgentEdits = Boolean(source.startsWith('# agentEdits'))
 
 	return (
 		<ToolUseBlock>
@@ -46,9 +47,9 @@ const CodeAccordian = ({
 						</div>
 					) : isFeedback ? (
 						<div className="flex items-center">
-							<span className={`codicon codicon-${isFeedback ? "feedback" : "codicon-output"} mr-1.5`} />
+							<span className={`codicon codicon-${isAgentEdits ? "hubot" : isFeedback ? "feedback" : "output"} mr-1.5`} />
 							<span className="whitespace-nowrap overflow-hidden text-ellipsis mr-2 rtl">
-								{isFeedback ? "User Edits" : "Console Logs"}
+								{isAgentEdits ? "Roo Edits" : isFeedback ? "User Edits" : "Console Logs"}
 							</span>
 						</div>
 					) : (
diff --git a/webview-ui/src/components/common/CodeBlock.tsx b/webview-ui/src/components/common/CodeBlock.tsx
index da3eb642..b20fad8d 100644
--- a/webview-ui/src/components/common/CodeBlock.tsx
+++ b/webview-ui/src/components/common/CodeBlock.tsx
@@ -142,7 +142,16 @@ export const StyledPre = styled.div<{
 		word-break: ${({ wordwrap }) => (wordwrap === "false" ? "normal" : "normal")};
 		overflow-wrap: ${({ wordwrap }) => (wordwrap === "false" ? "normal" : "break-word")};
 		font-size: var(--vscode-editor-font-size, var(--vscode-font-size, 12px));
-		font-family: var(--vscode-editor-font-family);
+		font-family: 'JetBrains Mono', 'Fira Code', Consolas, 'Courier New', monospace;
+    
+		@font-face {
+			font-family: 'code-chinese';
+			src: local('Microsoft YaHei'), local('PingFang SC'), local('SimHei');
+			unicode-range: U+4E00-9FFF, U+3400-4DBF, U+20000-2A6DF, U+2A700-2B73F, U+2B740-2B81F, U+2B820-2CEAF;
+			size-adjust: 90%;
+		}
+		
+		font-family: 'JetBrains Mono', 'Fira Code', Consolas, 'code-chinese', 'Courier New', monospace, var(--vscode-font-family);
 	}
 
 	pre > code {
diff --git a/webview-ui/src/components/common/MarkdownBlock.tsx b/webview-ui/src/components/common/MarkdownBlock.tsx
index fe033efe..4375acb4 100644
--- a/webview-ui/src/components/common/MarkdownBlock.tsx
+++ b/webview-ui/src/components/common/MarkdownBlock.tsx
@@ -79,7 +79,16 @@ const remarkUrlToLink = () => {
 
 const StyledMarkdown = styled.div`
 	code:not(pre > code) {
-		font-family: var(--vscode-editor-font-family, monospace);
+		font-family: 'JetBrains Mono', 'Fira Code', Consolas, 'Courier New', monospace;
+    
+		@font-face {
+			font-family: 'code-chinese';
+			src: local('Microsoft YaHei'), local('PingFang SC'), local('SimHei');
+			unicode-range: U+4E00-9FFF, U+3400-4DBF, U+20000-2A6DF, U+2A700-2B73F, U+2B740-2B81F, U+2B820-2CEAF;
+			size-adjust: 90%;
+		}
+		
+		font-family: 'JetBrains Mono', 'Fira Code', Consolas, 'code-chinese', 'Courier New', monospace, var(--vscode-font-family);
 		filter: saturation(110%) brightness(95%);
 		color: var(--vscode-textPreformat-foreground) !important;
 		background-color: var(--vscode-textPreformat-background) !important;
diff --git a/webview-ui/src/components/settings/ExperimentalSettings.tsx b/webview-ui/src/components/settings/ExperimentalSettings.tsx
index 79d8afef..41bf400e 100644
--- a/webview-ui/src/components/settings/ExperimentalSettings.tsx
+++ b/webview-ui/src/components/settings/ExperimentalSettings.tsx
@@ -13,7 +13,7 @@ import { SetCachedStateField, SetExperimentEnabled } from "./types"
 import { SectionHeader } from "./SectionHeader"
 import { Section } from "./Section"
 import { ExperimentalFeature } from "./ExperimentalFeature"
-import { CodeIndexSettings } from "./CodeIndexSettings"
+import { CodeIndexSettings } from "./CodeIndexSettings-riddler"
 
 type ExperimentalSettingsProps = HTMLAttributes<HTMLDivElement> & {
 	experiments: Experiments
diff --git a/webview-ui/src/components/settings/constants.ts b/webview-ui/src/components/settings/constants.ts
index 5b808643..ea1dc02e 100644
--- a/webview-ui/src/components/settings/constants.ts
+++ b/webview-ui/src/components/settings/constants.ts
@@ -28,23 +28,23 @@ export const MODELS_BY_PROVIDER: Partial<Record<ProviderName, Record<string, Mod
 
 export const PROVIDERS = [
 	{ value: "openrouter", label: "OpenRouter" },
-	{ value: "anthropic", label: "Anthropic" },
+	// { value: "anthropic", label: "Anthropic" },
 	{ value: "gemini", label: "Google Gemini" },
 	{ value: "deepseek", label: "DeepSeek" },
 	{ value: "openai-native", label: "OpenAI" },
 	{ value: "openai", label: "OpenAI Compatible" },
-	{ value: "vertex", label: "GCP Vertex AI" },
-	{ value: "bedrock", label: "Amazon Bedrock" },
-	{ value: "glama", label: "Glama" },
-	{ value: "vscode-lm", label: "VS Code LM API" },
-	{ value: "mistral", label: "Mistral" },
-	{ value: "lmstudio", label: "LM Studio" },
-	{ value: "ollama", label: "Ollama" },
-	{ value: "unbound", label: "Unbound" },
-	{ value: "requesty", label: "Requesty" },
-	{ value: "human-relay", label: "Human Relay" },
-	{ value: "xai", label: "xAI (Grok)" },
-	{ value: "groq", label: "Groq" },
-	{ value: "chutes", label: "Chutes AI" },
-	{ value: "litellm", label: "LiteLLM" },
+	// { value: "vertex", label: "GCP Vertex AI" },
+	// { value: "bedrock", label: "Amazon Bedrock" },
+	// { value: "glama", label: "Glama" },
+	// { value: "vscode-lm", label: "VS Code LM API" },
+	// { value: "mistral", label: "Mistral" },
+	// { value: "lmstudio", label: "LM Studio" },
+	// { value: "ollama", label: "Ollama" },
+	// { value: "unbound", label: "Unbound" },
+	// { value: "requesty", label: "Requesty" },
+	// { value: "human-relay", label: "Human Relay" },
+	// { value: "xai", label: "xAI (Grok)" },
+	// { value: "groq", label: "Groq" },
+	// { value: "chutes", label: "Chutes AI" },
+	// { value: "litellm", label: "LiteLLM" },
 ].sort((a, b) => a.label.localeCompare(b.label))
diff --git a/webview-ui/src/components/ui/hooks/useOpenRouterModelProviders.ts b/webview-ui/src/components/ui/hooks/useOpenRouterModelProviders.ts
index dc50c0f6..439b328c 100644
--- a/webview-ui/src/components/ui/hooks/useOpenRouterModelProviders.ts
+++ b/webview-ui/src/components/ui/hooks/useOpenRouterModelProviders.ts
@@ -43,7 +43,7 @@ async function getOpenRouterProvidersForModel(modelId: string) {
 	const models: Record<string, OpenRouterModelProvider> = {}
 
 	try {
-		const response = await axios.get(`https://openrouter.ai/api/v1/models/${modelId}/endpoints`)
+		const response = await axios.get(`https://riddler.mynatapp.cc/api/openrouter/v1/models/${modelId}/endpoints`)
 		const result = openRouterEndpointsSchema.safeParse(response.data)
 
 		if (!result.success) {
diff --git a/webview-ui/src/utils/context-mentions.ts b/webview-ui/src/utils/context-mentions.ts
index 5df3404b..0c9ec325 100644
--- a/webview-ui/src/utils/context-mentions.ts
+++ b/webview-ui/src/utils/context-mentions.ts
@@ -97,6 +97,9 @@ export enum ContextMenuOptionType {
 	Git = "git",
 	NoResults = "noResults",
 	Mode = "mode", // Add mode type
+	Codebase = "codebase", // Add codebase type
+	Summary = "summary", // Add summary type
+	Memory = "memory", // Add memory type
 }
 
 export interface ContextMenuQueryItem {
@@ -186,10 +189,13 @@ export function getContextMenuOptions(
 		return [
 			{ type: ContextMenuOptionType.Problems },
 			{ type: ContextMenuOptionType.Terminal },
-			{ type: ContextMenuOptionType.URL },
+			// { type: ContextMenuOptionType.URL },
 			{ type: ContextMenuOptionType.Folder },
 			{ type: ContextMenuOptionType.File },
-			{ type: ContextMenuOptionType.Git },
+			{ type: ContextMenuOptionType.Codebase },
+			{ type: ContextMenuOptionType.Summary },
+			{ type: ContextMenuOptionType.Memory },
+			// { type: ContextMenuOptionType.Git },
 		]
 	}
 
@@ -210,6 +216,15 @@ export function getContextMenuOptions(
 	if ("problems".startsWith(lowerQuery)) {
 		suggestions.push({ type: ContextMenuOptionType.Problems })
 	}
+	if ("codebase".startsWith(lowerQuery)) {
+		suggestions.push({ type: ContextMenuOptionType.Codebase })
+	}
+	if ("summary".startsWith(lowerQuery)) {
+		suggestions.push({ type: ContextMenuOptionType.Summary })
+	}
+	if ("memory".startsWith(lowerQuery)) {
+		suggestions.push({ type: ContextMenuOptionType.Memory })
+	}
 	if ("terminal".startsWith(lowerQuery)) {
 		suggestions.push({ type: ContextMenuOptionType.Terminal })
 	}
