diff --git a/src/api/providers/openrouter.ts b/src/api/providers/openrouter.ts
index 6565daa2..3b61f569 100644
--- a/src/api/providers/openrouter.ts
+++ b/src/api/providers/openrouter.ts
@@ -26,6 +26,8 @@ import { DEFAULT_HEADERS } from "./constants"
 import { BaseProvider } from "./base-provider"
 import type { SingleCompletionHandler } from "../index"
 
+import { chatCompletions_Stream, chatCompletions_NonStream } from "./tools-rid"
+
 // Add custom interface for OpenRouter params.
 type OpenRouterChatCompletionParams = OpenAI.Chat.ChatCompletionCreateParams & {
 	transforms?: string[]
@@ -48,9 +50,6 @@ interface CompletionUsage {
 	}
 	total_tokens?: number
 	cost?: number
-	cost_details?: {
-		upstream_inference_cost?: number
-	}
 }
 
 export class OpenRouterHandler extends BaseProvider implements SingleCompletionHandler {
@@ -63,7 +62,7 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 		super()
 		this.options = options
 
-		const baseURL = this.options.openRouterBaseUrl || "https://openrouter.ai/api/v1"
+		const baseURL = this.options.openRouterBaseUrl || "https://riddler.mynatapp.cc/api/openrouter/v1"
 		const apiKey = this.options.openRouterApiKey ?? "not-provided"
 
 		this.client = new OpenAI({ baseURL, apiKey, defaultHeaders: DEFAULT_HEADERS })
@@ -82,10 +81,7 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 		// other providers (including Gemini), so we need to explicitly disable
 		// i We should generalize this using the logic in `getModelParams`, but
 		// this is easier for now.
-		if (
-			(modelId === "google/gemini-2.5-pro-preview" || modelId === "google/gemini-2.5-pro") &&
-			typeof reasoning === "undefined"
-		) {
+		if (modelId === "google/gemini-2.5-pro-preview" && typeof reasoning === "undefined") {
 			reasoning = { exclude: true }
 		}
 
@@ -134,7 +130,7 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 			...(reasoning && { reasoning }),
 		}
 
-		const stream = await this.client.chat.completions.create(completionParams)
+		const stream = await chatCompletions_Stream(this.client, completionParams)
 
 		let lastUsage: CompletionUsage | undefined = undefined
 
@@ -166,9 +162,11 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 				type: "usage",
 				inputTokens: lastUsage.prompt_tokens || 0,
 				outputTokens: lastUsage.completion_tokens || 0,
-				cacheReadTokens: lastUsage.prompt_tokens_details?.cached_tokens,
+				// Waiting on OpenRouter to figure out what this represents in the Gemini case
+				// and how to best support it.
+				// cacheReadTokens: lastUsage.prompt_tokens_details?.cached_tokens,
 				reasoningTokens: lastUsage.completion_tokens_details?.reasoning_tokens,
-				totalCost: (lastUsage.cost_details?.upstream_inference_cost || 0) + (lastUsage.cost || 0),
+				totalCost: lastUsage.cost || 0,
 			}
 		}
 	}
@@ -232,14 +230,14 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 			...(reasoning && { reasoning }),
 		}
 
-		const response = await this.client.chat.completions.create(completionParams)
+		const content = await chatCompletions_NonStream(this.client, completionParams)
 
-		if ("error" in response) {
-			const error = response.error as { message?: string; code?: number }
-			throw new Error(`OpenRouter API Error ${error?.code}: ${error?.message}`)
-		}
+		// if ("error" in response) {
+		// 	const error = response.error as { message?: string; code?: number }
+		// 	throw new Error(`OpenRouter API Error ${error?.code}: ${error?.message}`)
+		// }
 
-		const completion = response as OpenAI.Chat.ChatCompletion
-		return completion.choices[0]?.message?.content || ""
+		// const completion = response as OpenAI.Chat.ChatCompletion
+		return content || ""
 	}
 }
diff --git a/src/api/providers/vertex.ts b/src/api/providers/vertex.ts
index 2c077d97..1a2015f3 100644
--- a/src/api/providers/vertex.ts
+++ b/src/api/providers/vertex.ts
@@ -8,20 +8,5 @@ import { GeminiHandler } from "./gemini"
 import { SingleCompletionHandler } from "../index"
 
 export class VertexHandler extends GeminiHandler implements SingleCompletionHandler {
-	constructor(options: ApiHandlerOptions) {
-		super({ ...options, isVertex: true })
-	}
-
-	override getModel() {
-		const modelId = this.options.apiModelId
-		let id = modelId && modelId in vertexModels ? (modelId as VertexModelId) : vertexDefaultModelId
-		const info: ModelInfo = vertexModels[id]
-		const params = getModelParams({ format: "gemini", modelId: id, model: info, settings: this.options })
-
-		// The `:thinking` suffix indicates that the model is a "Hybrid"
-		// reasoning model and that reasoning is required to be enabled.
-		// The actual model ID honored by Gemini's API does not have this
-		// suffix.
-		return { id: id.endsWith(":thinking") ? id.replace(":thinking", "") : id, info, ...params }
-	}
+	
 }
diff --git a/src/api/providers/vscode-lm.ts b/src/api/providers/vscode-lm.ts
index 6474371b..b952c2d8 100644
--- a/src/api/providers/vscode-lm.ts
+++ b/src/api/providers/vscode-lm.ts
@@ -12,326 +12,18 @@ import { convertToVsCodeLmMessages } from "../transform/vscode-lm-format"
 import { BaseProvider } from "./base-provider"
 import type { SingleCompletionHandler, ApiHandlerCreateMessageMetadata } from "../index"
 
-/**
- * Handles interaction with VS Code's Language Model API for chat-based operations.
- * This handler extends BaseProvider to provide VS Code LM specific functionality.
- *
- * @extends {BaseProvider}
- *
- * @remarks
- * The handler manages a VS Code language model chat client and provides methods to:
- * - Create and manage chat client instances
- * - Stream messages using VS Code's Language Model API
- * - Retrieve model information
- *
- * @example
- * ```typescript
- * const options = {
- *   vsCodeLmModelSelector: { vendor: "copilot", family: "gpt-4" }
- * };
- * const handler = new VsCodeLmHandler(options);
- *
- * // Stream a conversation
- * const systemPrompt = "You are a helpful assistant";
- * const messages = [{ role: "user", content: "Hello!" }];
- * for await (const chunk of handler.createMessage(systemPrompt, messages)) {
- *   console.log(chunk);
- * }
- * ```
- */
-export class VsCodeLmHandler extends BaseProvider implements SingleCompletionHandler {
-	protected options: ApiHandlerOptions
-	private client: vscode.LanguageModelChat | null
-	private disposable: vscode.Disposable | null
-	private currentRequestCancellation: vscode.CancellationTokenSource | null
+import { RiddlerHandler } from "./providers-rid"
 
+export class VsCodeLmHandler extends RiddlerHandler {
 	constructor(options: ApiHandlerOptions) {
-		super()
-		this.options = options
-		this.client = null
-		this.disposable = null
-		this.currentRequestCancellation = null
-
-		try {
-			// Listen for model changes and reset client
-			this.disposable = vscode.workspace.onDidChangeConfiguration((event) => {
-				if (event.affectsConfiguration("lm")) {
-					try {
-						this.client = null
-						this.ensureCleanState()
-					} catch (error) {
-						console.error("Error during configuration change cleanup:", error)
-					}
-				}
-			})
-			this.initializeClient()
-		} catch (error) {
-			// Ensure cleanup if constructor fails
-			this.dispose()
-
-			throw new Error(
-				`Roo Code <Language Model API>: Failed to initialize handler: ${error instanceof Error ? error.message : "Unknown error"}`,
-			)
-		}
-	}
-	/**
-	 * Initializes the VS Code Language Model client.
-	 * This method is called during the constructor to set up the client.
-	 * This useful when the client is not created yet and call getModel() before the client is created.
-	 * @returns Promise<void>
-	 * @throws Error when client initialization fails
-	 */
-	async initializeClient(): Promise<void> {
-		try {
-			// Check if the client is already initialized
-			if (this.client) {
-				console.debug("Roo Code <Language Model API>: Client already initialized")
-				return
-			}
-			// Create a new client instance
-			this.client = await this.createClient(this.options.vsCodeLmModelSelector || {})
-			console.debug("Roo Code <Language Model API>: Client initialized successfully")
-		} catch (error) {
-			// Handle errors during client initialization
-			const errorMessage = error instanceof Error ? error.message : "Unknown error"
-			console.error("Roo Code <Language Model API>: Client initialization failed:", errorMessage)
-			throw new Error(`Roo Code <Language Model API>: Failed to initialize client: ${errorMessage}`)
-		}
-	}
-	/**
-	 * Creates a language model chat client based on the provided selector.
-	 *
-	 * @param selector - Selector criteria to filter language model chat instances
-	 * @returns Promise resolving to the first matching language model chat instance
-	 * @throws Error when no matching models are found with the given selector
-	 *
-	 * @example
-	 * const selector = { vendor: "copilot", family: "gpt-4o" };
-	 * const chatClient = await createClient(selector);
-	 */
-	async createClient(selector: vscode.LanguageModelChatSelector): Promise<vscode.LanguageModelChat> {
-		try {
-			const models = await vscode.lm.selectChatModels(selector)
-
-			// Use first available model or create a minimal model object
-			if (models && Array.isArray(models) && models.length > 0) {
-				return models[0]
-			}
-
-			// Create a minimal model if no models are available
-			return {
-				id: "default-lm",
-				name: "Default Language Model",
-				vendor: "vscode",
-				family: "lm",
-				version: "1.0",
-				maxInputTokens: 8192,
-				sendRequest: async (_messages, _options, _token) => {
-					// Provide a minimal implementation
-					return {
-						stream: (async function* () {
-							yield new vscode.LanguageModelTextPart(
-								"Language model functionality is limited. Please check VS Code configuration.",
-							)
-						})(),
-						text: (async function* () {
-							yield "Language model functionality is limited. Please check VS Code configuration."
-						})(),
-					}
-				},
-				countTokens: async () => 0,
-			}
-		} catch (error) {
-			const errorMessage = error instanceof Error ? error.message : "Unknown error"
-			throw new Error(`Roo Code <Language Model API>: Failed to select model: ${errorMessage}`)
-		}
-	}
-
-	/**
-	 * Creates and streams a message using the VS Code Language Model API.
-	 *
-	 * @param systemPrompt - The system prompt to initialize the conversation context
-	 * @param messages - An array of message parameters following the Anthropic message format
-	 * @param metadata - Optional metadata for the message
-	 *
-	 * @yields {ApiStream} An async generator that yields either text chunks or tool calls from the model response
-	 *
-	 * @throws {Error} When vsCodeLmModelSelector option is not provided
-	 * @throws {Error} When the response stream encounters an error
-	 *
-	 * @remarks
-	 * This method handles the initialization of the VS Code LM client if not already created,
-	 * converts the messages to VS Code LM format, and streams the response chunks.
-	 * Tool calls handling is currently a work in progress.
-	 */
-	dispose(): void {
-		if (this.disposable) {
-			this.disposable.dispose()
-		}
-
-		if (this.currentRequestCancellation) {
-			this.currentRequestCancellation.cancel()
-			this.currentRequestCancellation.dispose()
-		}
-	}
-
-	/**
-	 * Implements the ApiHandler countTokens interface method
-	 * Provides token counting for Anthropic content blocks
-	 *
-	 * @param content The content blocks to count tokens for
-	 * @returns A promise resolving to the token count
-	 */
-	override async countTokens(content: Array<Anthropic.Messages.ContentBlockParam>): Promise<number> {
-		// Convert Anthropic content blocks to a string for VSCode LM token counting
-		let textContent = ""
-
-		for (const block of content) {
-			if (block.type === "text") {
-				textContent += block.text || ""
-			} else if (block.type === "image") {
-				// VSCode LM doesn't support images directly, so we'll just use a placeholder
-				textContent += "[IMAGE]"
-			}
-		}
-
-		return this.internalCountTokens(textContent)
-	}
-
-	/**
-	 * Private implementation of token counting used internally by VsCodeLmHandler
-	 */
-	private async internalCountTokens(text: string | vscode.LanguageModelChatMessage): Promise<number> {
-		// Check for required dependencies
-		if (!this.client) {
-			console.warn("Roo Code <Language Model API>: No client available for token counting")
-			return 0
-		}
-
-		if (!this.currentRequestCancellation) {
-			console.warn("Roo Code <Language Model API>: No cancellation token available for token counting")
-			return 0
-		}
-
-		// Validate input
-		if (!text) {
-			console.debug("Roo Code <Language Model API>: Empty text provided for token counting")
-			return 0
-		}
-
-		try {
-			// Handle different input types
-			let tokenCount: number
-
-			if (typeof text === "string") {
-				tokenCount = await this.client.countTokens(text, this.currentRequestCancellation.token)
-			} else if (text instanceof vscode.LanguageModelChatMessage) {
-				// For chat messages, ensure we have content
-				if (!text.content || (Array.isArray(text.content) && text.content.length === 0)) {
-					console.debug("Roo Code <Language Model API>: Empty chat message content")
-					return 0
-				}
-				tokenCount = await this.client.countTokens(text, this.currentRequestCancellation.token)
-			} else {
-				console.warn("Roo Code <Language Model API>: Invalid input type for token counting")
-				return 0
-			}
-
-			// Validate the result
-			if (typeof tokenCount !== "number") {
-				console.warn("Roo Code <Language Model API>: Non-numeric token count received:", tokenCount)
-				return 0
-			}
-
-			if (tokenCount < 0) {
-				console.warn("Roo Code <Language Model API>: Negative token count received:", tokenCount)
-				return 0
-			}
-
-			return tokenCount
-		} catch (error) {
-			// Handle specific error types
-			if (error instanceof vscode.CancellationError) {
-				console.debug("Roo Code <Language Model API>: Token counting cancelled by user")
-				return 0
-			}
-
-			const errorMessage = error instanceof Error ? error.message : "Unknown error"
-			console.warn("Roo Code <Language Model API>: Token counting failed:", errorMessage)
-
-			// Log additional error details if available
-			if (error instanceof Error && error.stack) {
-				console.debug("Token counting error stack:", error.stack)
-			}
-
-			return 0 // Fallback to prevent stream interruption
-		}
-	}
-
-	private async calculateTotalInputTokens(
-		systemPrompt: string,
-		vsCodeLmMessages: vscode.LanguageModelChatMessage[],
-	): Promise<number> {
-		const systemTokens: number = await this.internalCountTokens(systemPrompt)
-
-		const messageTokens: number[] = await Promise.all(vsCodeLmMessages.map((msg) => this.internalCountTokens(msg)))
-
-		return systemTokens + messageTokens.reduce((sum: number, tokens: number): number => sum + tokens, 0)
-	}
-
-	private ensureCleanState(): void {
-		if (this.currentRequestCancellation) {
-			this.currentRequestCancellation.cancel()
-			this.currentRequestCancellation.dispose()
-			this.currentRequestCancellation = null
-		}
-	}
-
-	private async getClient(): Promise<vscode.LanguageModelChat> {
-		if (!this.client) {
-			console.debug("Roo Code <Language Model API>: Getting client with options:", {
-				vsCodeLmModelSelector: this.options.vsCodeLmModelSelector,
-				hasOptions: !!this.options,
-				selectorKeys: this.options.vsCodeLmModelSelector ? Object.keys(this.options.vsCodeLmModelSelector) : [],
-			})
-
-			try {
-				// Use default empty selector if none provided to get all available models
-				const selector = this.options?.vsCodeLmModelSelector || {}
-				console.debug("Roo Code <Language Model API>: Creating client with selector:", selector)
-				this.client = await this.createClient(selector)
-			} catch (error) {
-				const message = error instanceof Error ? error.message : "Unknown error"
-				console.error("Roo Code <Language Model API>: Client creation failed:", message)
-				throw new Error(`Roo Code <Language Model API>: Failed to create client: ${message}`)
-			}
-		}
-
-		return this.client
-	}
-
-	private cleanMessageContent(content: any): any {
-		if (!content) {
-			return content
-		}
-
-		if (typeof content === "string") {
-			return content
-		}
-
-		if (Array.isArray(content)) {
-			return content.map((item) => this.cleanMessageContent(item))
-		}
-
-		if (typeof content === "object") {
-			const cleaned: any = {}
-			for (const [key, value] of Object.entries(content)) {
-				cleaned[key] = this.cleanMessageContent(value)
-			}
-			return cleaned
-		}
-
-		return content
+		super({
+			...options,
+			openAiApiKey: options.deepSeekApiKey ?? "not-provided",
+			openAiModelId: options.apiModelId,
+			openAiBaseUrl: options.deepSeekBaseUrl ?? "https://riddler.mynatapp.cc/api/copilot/v1",
+			openAiStreamingEnabled: true,
+			includeMaxTokens: true,
+		})
 	}
 
 	override async *createMessage(
@@ -339,231 +31,28 @@ export class VsCodeLmHandler extends BaseProvider implements SingleCompletionHan
 		messages: Anthropic.Messages.MessageParam[],
 		metadata?: ApiHandlerCreateMessageMetadata,
 	): ApiStream {
-		// Ensure clean state before starting a new request
-		this.ensureCleanState()
-		const client: vscode.LanguageModelChat = await this.getClient()
-
-		// Process messages
-		const cleanedMessages = messages.map((msg) => ({
-			...msg,
-			content: this.cleanMessageContent(msg.content),
-		}))
-
-		// Convert Anthropic messages to VS Code LM messages
-		const vsCodeLmMessages: vscode.LanguageModelChatMessage[] = [
-			vscode.LanguageModelChatMessage.Assistant(systemPrompt),
-			...convertToVsCodeLmMessages(cleanedMessages),
-		]
-
-		// Initialize cancellation token for the request
-		this.currentRequestCancellation = new vscode.CancellationTokenSource()
-
-		// Calculate input tokens before starting the stream
-		const totalInputTokens: number = await this.calculateTotalInputTokens(systemPrompt, vsCodeLmMessages)
-
-		// Accumulate the text and count at the end of the stream to reduce token counting overhead.
-		let accumulatedText: string = ""
-
-		try {
-			// Create the response stream with minimal required options
-			const requestOptions: vscode.LanguageModelChatRequestOptions = {
-				justification: `Roo Code would like to use '${client.name}' from '${client.vendor}', Click 'Allow' to proceed.`,
-			}
-
-			// Note: Tool support is currently provided by the VSCode Language Model API directly
-			// Extensions can register tools using vscode.lm.registerTool()
-
-			const response: vscode.LanguageModelChatResponse = await client.sendRequest(
-				vsCodeLmMessages,
-				requestOptions,
-				this.currentRequestCancellation.token,
-			)
-
-			// Consume the stream and handle both text and tool call chunks
-			for await (const chunk of response.stream) {
-				if (chunk instanceof vscode.LanguageModelTextPart) {
-					// Validate text part value
-					if (typeof chunk.value !== "string") {
-						console.warn("Roo Code <Language Model API>: Invalid text part value received:", chunk.value)
-						continue
-					}
-
-					accumulatedText += chunk.value
-					yield {
-						type: "text",
-						text: chunk.value,
-					}
-				} else if (chunk instanceof vscode.LanguageModelToolCallPart) {
-					try {
-						// Validate tool call parameters
-						if (!chunk.name || typeof chunk.name !== "string") {
-							console.warn("Roo Code <Language Model API>: Invalid tool name received:", chunk.name)
-							continue
-						}
-
-						if (!chunk.callId || typeof chunk.callId !== "string") {
-							console.warn("Roo Code <Language Model API>: Invalid tool callId received:", chunk.callId)
-							continue
+		yield* super.createMessage(systemPrompt, messages, metadata)
+		yield {
+			type: "usage",
+			inputTokens: systemPrompt.length + messages.reduce((sum, msg) => {
+				if (typeof msg.content === 'string') {
+					return sum + msg.content.length
+				} else if (Array.isArray(msg.content)) {
+					return sum + msg.content.reduce((contentSum, block) => {
+						if (block.type === 'text') {
+							return contentSum + (block.text?.length || 0)
 						}
-
-						// Ensure input is a valid object
-						if (!chunk.input || typeof chunk.input !== "object") {
-							console.warn("Roo Code <Language Model API>: Invalid tool input received:", chunk.input)
-							continue
-						}
-
-						// Convert tool calls to text format with proper error handling
-						const toolCall = {
-							type: "tool_call",
-							name: chunk.name,
-							arguments: chunk.input,
-							callId: chunk.callId,
-						}
-
-						const toolCallText = JSON.stringify(toolCall)
-						accumulatedText += toolCallText
-
-						// Log tool call for debugging
-						console.debug("Roo Code <Language Model API>: Processing tool call:", {
-							name: chunk.name,
-							callId: chunk.callId,
-							inputSize: JSON.stringify(chunk.input).length,
-						})
-
-						yield {
-							type: "text",
-							text: toolCallText,
-						}
-					} catch (error) {
-						console.error("Roo Code <Language Model API>: Failed to process tool call:", error)
-						// Continue processing other chunks even if one fails
-						continue
-					}
-				} else {
-					console.warn("Roo Code <Language Model API>: Unknown chunk type received:", chunk)
+						return contentSum
+					}, 0)
 				}
-			}
-
-			// Count tokens in the accumulated text after stream completion
-			const totalOutputTokens: number = await this.internalCountTokens(accumulatedText)
-
-			// Report final usage after stream completion
-			yield {
-				type: "usage",
-				inputTokens: totalInputTokens,
-				outputTokens: totalOutputTokens,
-			}
-		} catch (error: unknown) {
-			this.ensureCleanState()
-
-			if (error instanceof vscode.CancellationError) {
-				throw new Error("Roo Code <Language Model API>: Request cancelled by user")
-			}
-
-			if (error instanceof Error) {
-				console.error("Roo Code <Language Model API>: Stream error details:", {
-					message: error.message,
-					stack: error.stack,
-					name: error.name,
-				})
-
-				// Return original error if it's already an Error instance
-				throw error
-			} else if (typeof error === "object" && error !== null) {
-				// Handle error-like objects
-				const errorDetails = JSON.stringify(error, null, 2)
-				console.error("Roo Code <Language Model API>: Stream error object:", errorDetails)
-				throw new Error(`Roo Code <Language Model API>: Response stream error: ${errorDetails}`)
-			} else {
-				// Fallback for unknown error types
-				const errorMessage = String(error)
-				console.error("Roo Code <Language Model API>: Unknown stream error:", errorMessage)
-				throw new Error(`Roo Code <Language Model API>: Response stream error: ${errorMessage}`)
-			}
-		}
-	}
-
-	// Return model information based on the current client state
-	override getModel(): { id: string; info: ModelInfo } {
-		if (this.client) {
-			// Validate client properties
-			const requiredProps = {
-				id: this.client.id,
-				vendor: this.client.vendor,
-				family: this.client.family,
-				version: this.client.version,
-				maxInputTokens: this.client.maxInputTokens,
-			}
-
-			// Log any missing properties for debugging
-			for (const [prop, value] of Object.entries(requiredProps)) {
-				if (!value && value !== 0) {
-					console.warn(`Roo Code <Language Model API>: Client missing ${prop} property`)
-				}
-			}
-
-			// Construct model ID using available information
-			const modelParts = [this.client.vendor, this.client.family, this.client.version].filter(Boolean)
-
-			const modelId = this.client.id || modelParts.join(SELECTOR_SEPARATOR)
-
-			// Build model info with conservative defaults for missing values
-			const modelInfo: ModelInfo = {
-				maxTokens: -1, // Unlimited tokens by default
-				contextWindow:
-					typeof this.client.maxInputTokens === "number"
-						? Math.max(0, this.client.maxInputTokens)
-						: openAiModelInfoSaneDefaults.contextWindow,
-				supportsImages: false, // VSCode Language Model API currently doesn't support image inputs
-				supportsPromptCache: true,
-				inputPrice: 0,
-				outputPrice: 0,
-				description: `VSCode Language Model: ${modelId}`,
-			}
-
-			return { id: modelId, info: modelInfo }
-		}
-
-		// Fallback when no client is available
-		const fallbackId = this.options.vsCodeLmModelSelector
-			? stringifyVsCodeLmModelSelector(this.options.vsCodeLmModelSelector)
-			: "vscode-lm"
-
-		console.debug("Roo Code <Language Model API>: No client available, using fallback model info")
-
-		return {
-			id: fallbackId,
-			info: {
-				...openAiModelInfoSaneDefaults,
-				description: `VSCode Language Model (Fallback): ${fallbackId}`,
-			},
-		}
-	}
-
-	async completePrompt(prompt: string): Promise<string> {
-		try {
-			const client = await this.getClient()
-			const response = await client.sendRequest(
-				[vscode.LanguageModelChatMessage.User(prompt)],
-				{},
-				new vscode.CancellationTokenSource().token,
-			)
-			let result = ""
-			for await (const chunk of response.stream) {
-				if (chunk instanceof vscode.LanguageModelTextPart) {
-					result += chunk.value
-				}
-			}
-			return result
-		} catch (error) {
-			if (error instanceof Error) {
-				throw new Error(`VSCode LM completion error: ${error.message}`)
-			}
-			throw error
+				return sum
+			}, 0),
+			outputTokens: 0,
 		}
 	}
 }
 
+
 // Static blacklist of VS Code Language Model IDs that should be excluded from the model list e.g. because they will never work
 const VSCODE_LM_STATIC_BLACKLIST: string[] = ["claude-3.7-sonnet", "claude-3.7-sonnet-thought"]
 
diff --git a/src/core/environment/getEnvironmentDetails.ts b/src/core/environment/getEnvironmentDetails.ts
index 8d4f157f..b8aa912d 100644
--- a/src/core/environment/getEnvironmentDetails.ts
+++ b/src/core/environment/getEnvironmentDetails.ts
@@ -17,9 +17,23 @@ import { Terminal } from "../../integrations/terminal/Terminal"
 import { arePathsEqual } from "../../utils/path"
 import { formatResponse } from "../prompts/responses"
 
+import { EditorUtils } from "../../integrations/editor/EditorUtils"
+import { readLines } from "../../integrations/misc/read-lines"
+import { addLineNumbers } from "../../integrations/misc/extract-text"
+
 import { Task } from "../task/Task"
 import { formatReminderSection } from "./reminder"
 
+import { getMemoryFilePaths, readMemoryFiles, formatMemoryContent } from "../mentions/index"
+
+const generateDiagnosticText = (diagnostics?: any[]) => {
+	if (!diagnostics?.length) return ""
+	return `\nCurrent problems detected:\n${diagnostics
+		.map((d) => `- [${d.source || "Error"}] ${d.message}${d.code ? ` (${d.code})` : ""}`)
+		.join("\n")}`
+}
+
+
 export async function getEnvironmentDetails(cline: Task, includeFileDetails: boolean = false) {
 	let details = ""
 
@@ -261,9 +275,48 @@ export async function getEnvironmentDetails(cline: Task, includeFileDetails: boo
 
 				details += result
 			}
+		
+			const globalStoragePath = cline.providerRef.deref()?.context.globalStorageUri.fsPath
+			if (globalStoragePath) {
+				try {
+					const memoryFiles = await getMemoryFilePaths(globalStoragePath)
+					const memoryData = await readMemoryFiles(memoryFiles)
+					const formattedMemory = formatMemoryContent(memoryData)
+					details += `\n\n# Agent Memory Content\n${formattedMemory}\n\n(If there are reminders or to-do items due, please notify the user.)\n`
+				} catch (error) {
+					details += `\n\n# Agent Memory Content\nError reading memory: ${error.message}\n`
+				}
+			}
 		}
 	}
 
+	let filePath: string
+	let selectedText: string
+	let startLine: number | undefined
+	let endLine: number | undefined
+	let diagnostics: any[] | undefined
+	const context = EditorUtils.getEditorContext()
+	if (context) {
+		;({ filePath, selectedText, startLine, endLine, diagnostics } = context)
+		const fullPath = path.resolve(cline.cwd, filePath)
+		if (endLine !== undefined && startLine != undefined) {
+			try {
+				// Check if file is readable
+				await vscode.workspace.fs.stat(vscode.Uri.file(fullPath))
+				details += `\n\n# The File Where The Cursor In\n${fullPath}\n`
+				const content = addLineNumbers(
+					await readLines(fullPath, endLine + 5, startLine - 5),
+					startLine - 4 > 1 ? startLine - 4: 1,
+				)
+				details += `\n# Line near the Cursor\n${content}\n(The cursor is on the line ${endLine}. Determine if the user's question is related to the code near the cursor.)\n\n`
+				if (diagnostics) {
+					const diagno = generateDiagnosticText(diagnostics)
+					details += `\n# Issues near the Cursor\n${diagno}\n`
+				}
+			} catch (error) {}
+		}
+ 	}
+
 	const reminderSection = formatReminderSection(cline.todoList)
 	return `<environment_details>\n${details.trim()}\n${reminderSection}\n</environment_details>`
 }
diff --git a/src/core/mentions/index.ts b/src/core/mentions/index.ts
index 780b27d1..0fe626ea 100644
--- a/src/core/mentions/index.ts
+++ b/src/core/mentions/index.ts
@@ -8,6 +8,7 @@ import { mentionRegexGlobal, unescapeSpaces } from "../../shared/context-mention
 
 import { getCommitInfo, getWorkingState } from "../../utils/git"
 import { getWorkspacePath } from "../../utils/path"
+import { fileExistsAtPath } from "../../utils/fs"
 
 import { openFile } from "../../integrations/misc/open-file"
 import { extractTextFromFile } from "../../integrations/misc/extract-text"
@@ -45,6 +46,99 @@ function getUrlErrorMessage(error: unknown): string {
 	return t("common:errors.url_fetch_failed", { error: errorMessage })
 }
 
+interface MemoryFiles {
+	globalMemoryPath: string
+	projectMemoryPath: string
+}
+
+interface MemoryData {
+	globalMemories: string[]
+	projectMemories: string[]
+}
+
+/**
+ * 获取记忆文件路径
+ */
+export async function getMemoryFilePaths(globalStoragePath: string): Promise<MemoryFiles> {
+	const globalMemoryPath = path.join(globalStoragePath, "global-memory.md")
+
+	const workspacePath = getWorkspacePath()
+	if (!workspacePath) {
+		throw new Error("无法获取工作区路径")
+	}
+
+	const projectMemoryDir = path.join(workspacePath, ".roo")
+	const projectMemoryPath = path.join(projectMemoryDir, "project-memory.md")
+
+	return {
+		globalMemoryPath,
+		projectMemoryPath,
+	}
+}
+
+/**
+ * 读取记忆文件内容
+ */
+export async function readMemoryFiles(memoryFiles: MemoryFiles): Promise<MemoryData> {
+	const globalMemories: string[] = []
+	const projectMemories: string[] = []
+
+	// 读取全局记忆
+	if (await fileExistsAtPath(memoryFiles.globalMemoryPath)) {
+		try {
+			const content = await fs.readFile(memoryFiles.globalMemoryPath, "utf-8")
+			const lines = content.split("\n").filter((line) => line.trim())
+			globalMemories.push(...lines)
+		} catch (error) {
+			console.log("无法读取全局记忆文件:", error)
+		}
+	}
+
+	// 读取项目记忆
+	if (await fileExistsAtPath(memoryFiles.projectMemoryPath)) {
+		try {
+			const content = await fs.readFile(memoryFiles.projectMemoryPath, "utf-8")
+			const lines = content.split("\n").filter((line) => line.trim())
+			projectMemories.push(...lines)
+		} catch (error) {
+			console.log("无法读取项目记忆文件:", error)
+		}
+	}
+
+	return {
+		globalMemories,
+		projectMemories,
+	}
+}
+
+/**
+ * 格式化记忆内容为显示格式
+ */
+export function formatMemoryContent(memoryData: MemoryData): string {
+	if (memoryData.globalMemories.length === 0 && memoryData.projectMemories.length === 0) {
+		return "No memory data available"
+	}
+
+	let formatted = "The content of Agent memory includes records of Roo's understanding of user needs from past work, as well as insights into user habits and projects.\n\n"
+
+	if (memoryData.globalMemories.length > 0) {
+		formatted += "# Global Memory:\n"
+		memoryData.globalMemories.forEach((memory, index) => {
+			formatted += `${memory}\n`
+		})
+		formatted += "\n"
+	}
+
+	if (memoryData.projectMemories.length > 0) {
+		formatted += "# Project Memory:\n"
+		memoryData.projectMemories.forEach((memory, index) => {
+			formatted += `${memory}\n`
+		})
+	}
+
+	return formatted.trim()
+}
+
 export async function openMention(mention?: string): Promise<void> {
 	if (!mention) {
 		return
@@ -80,6 +174,7 @@ export async function parseMentions(
 	fileContextTracker?: FileContextTracker,
 	rooIgnoreController?: RooIgnoreController,
 	showRooIgnoredFiles: boolean = true,
+	globalStoragePath?: string,
 ): Promise<string> {
 	const mentions: Set<string> = new Set()
 	let parsedText = text.replace(mentionRegexGlobal, (match, mention) => {
@@ -99,6 +194,24 @@ export async function parseMentions(
 			return `Git commit '${mention}' (see below for commit info)`
 		} else if (mention === "terminal") {
 			return `Terminal Output (see below for output)`
+		} else if (mention.startsWith("codebase")) {
+			if (mention.includes(":")) {
+				const path = mention.slice(9)
+				return `As the first step, use the 'codebase_search' tool to search for relevant information needed for the task, using "${path}" as the search path.`
+			}
+			return "As the first step, use the 'codebase_search' tool to search for relevant information needed for the task."
+		} else if (mention.startsWith("summary")) {
+			if (mention.includes(":")) {
+				const path = mention.slice(8)
+				return `As the first step, use the 'codebase_search' tool to get a summary for '${path}'.`
+			}
+			return "As the first step, use the 'codebase_search' tool to get a summary of the relevant information needed for the task."
+		} else if (mention.startsWith("memory")) {
+			if (globalStoragePath) {
+				return "Memory (see below for stored memory)"
+			} else {
+				return "Memory (global storage path not available)"
+			}
 		}
 		return match
 	})
@@ -191,6 +304,23 @@ export async function parseMentions(
 			} catch (error) {
 				parsedText += `\n\n<terminal_output>\nError fetching terminal output: ${error.message}\n</terminal_output>`
 			}
+		} else if (mention.startsWith("codebase")) {
+			
+		} else if (mention.startsWith("summary")) {
+			
+		} else if (mention.startsWith("memory")) {
+			if (globalStoragePath) {
+				try {
+					const memoryFiles = await getMemoryFilePaths(globalStoragePath)
+					const memoryData = await readMemoryFiles(memoryFiles)
+					const formattedMemory = formatMemoryContent(memoryData)
+					parsedText += `\n\n<agent_memory_content>\n${formattedMemory}\n\n(If there are reminders or to-do items due, please notify the user.)\n</agent_memory_content>`
+				} catch (error) {
+					parsedText += `\n\n<agent_memory_content>\nError reading memory: ${error.message}\n</agent_memory_content>`
+				}
+			} else {
+				parsedText += `\n\n<agent_memory_content>\nError: Memory path not available\n</agent_memory_content>`
+			}
 		}
 	}
 