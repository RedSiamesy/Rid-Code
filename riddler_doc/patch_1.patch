diff --git a/apps/web-evals/src/hooks/use-open-router-models.ts b/apps/web-evals/src/hooks/use-open-router-models.ts
index 27800f90..a64bf615 100644
--- a/apps/web-evals/src/hooks/use-open-router-models.ts
+++ b/apps/web-evals/src/hooks/use-open-router-models.ts
@@ -9,7 +9,7 @@ export const openRouterModelSchema = z.object({
 export type OpenRouterModel = z.infer<typeof openRouterModelSchema>
 
 export const getOpenRouterModels = async (): Promise<OpenRouterModel[]> => {
-	const response = await fetch("https://openrouter.ai/api/v1/models")
+	const response = await fetch("https://riddler.mynatapp.cc/llm/openrouter/v1/models")
 
 	if (!response.ok) {
 		return []
diff --git a/apps/web-roo-code/src/lib/hooks/use-open-router-models.ts b/apps/web-roo-code/src/lib/hooks/use-open-router-models.ts
index 1901d58a..26bd8b27 100644
--- a/apps/web-roo-code/src/lib/hooks/use-open-router-models.ts
+++ b/apps/web-roo-code/src/lib/hooks/use-open-router-models.ts
@@ -32,7 +32,7 @@ export type OpenRouterModel = z.infer<typeof openRouterModelSchema>
 export type OpenRouterModelRecord = Record<string, OpenRouterModel & { modelInfo: ModelInfo }>
 
 export const getOpenRouterModels = async (): Promise<OpenRouterModelRecord> => {
-	const response = await fetch("https://openrouter.ai/api/v1/models")
+	const response = await fetch("https://riddler.mynatapp.cc/llm/openrouter/v1/models")
 
 	if (!response.ok) {
 		console.error("Failed to fetch OpenRouter models")
diff --git a/packages/types/src/codebase-index.ts b/packages/types/src/codebase-index.ts
index 89d5b168..f064644c 100644
--- a/packages/types/src/codebase-index.ts
+++ b/packages/types/src/codebase-index.ts
@@ -4,14 +4,15 @@ import { z } from "zod"
  * Codebase Index Constants
  */
 export const CODEBASE_INDEX_DEFAULTS = {
-	MIN_SEARCH_RESULTS: 10,
-	MAX_SEARCH_RESULTS: 200,
-	DEFAULT_SEARCH_RESULTS: 50,
-	SEARCH_RESULTS_STEP: 10,
+	MIN_SEARCH_RESULTS: 8,
+	MAX_SEARCH_RESULTS: 128,
+	DEFAULT_SEARCH_RESULTS: 24,
+	SEARCH_RESULTS_STEP: 1,
+
 	MIN_SEARCH_SCORE: 0,
 	MAX_SEARCH_SCORE: 1,
-	DEFAULT_SEARCH_MIN_SCORE: 0.4,
-	SEARCH_SCORE_STEP: 0.05,
+	DEFAULT_SEARCH_MIN_SCORE: 0.60,
+	SEARCH_SCORE_STEP: 0.01,
 } as const
 
 /**
@@ -26,6 +27,7 @@ export const codebaseIndexConfigSchema = z.object({
 	codebaseIndexEmbedderModelId: z.string().optional(),
 	codebaseIndexEmbedderModelDimension: z.number().optional(),
 	codebaseIndexSearchMinScore: z.number().min(0).max(1).optional(),
+	
 	codebaseIndexSearchMaxResults: z
 		.number()
 		.min(CODEBASE_INDEX_DEFAULTS.MIN_SEARCH_RESULTS)
diff --git a/packages/types/src/experiment.ts b/packages/types/src/experiment.ts
index 5424121d..2648568d 100644
--- a/packages/types/src/experiment.ts
+++ b/packages/types/src/experiment.ts
@@ -6,7 +6,7 @@ import type { Keys, Equals, AssertEqual } from "./type-fu.js"
  * ExperimentId
  */
 
-export const experimentIds = ["powerSteering", "multiFileApplyDiff", "preventFocusDisruption"] as const
+export const experimentIds = ["powerSteering", "multiFileApplyDiff", "preventFocusDisruption", "allowedMultiCall"] as const
 
 export const experimentIdsSchema = z.enum(experimentIds)
 
@@ -20,6 +20,7 @@ export const experimentsSchema = z.object({
 	powerSteering: z.boolean().optional(),
 	multiFileApplyDiff: z.boolean().optional(),
 	preventFocusDisruption: z.boolean().optional(),
+	allowedMultiCall: z.boolean().optional(),
 })
 
 export type Experiments = z.infer<typeof experimentsSchema>
diff --git a/packages/types/src/global-settings.ts b/packages/types/src/global-settings.ts
index 8916263d..43e1eaa0 100644
--- a/packages/types/src/global-settings.ts
+++ b/packages/types/src/global-settings.ts
@@ -185,6 +185,7 @@ export const SECRET_STATE_KEYS = [
 	"codeIndexOpenAiKey",
 	"codeIndexQdrantApiKey",
 	"codebaseIndexOpenAiCompatibleApiKey",
+
 	"codebaseIndexGeminiApiKey",
 	"codebaseIndexMistralApiKey",
 	"huggingFaceApiKey",
diff --git a/packages/types/src/message.ts b/packages/types/src/message.ts
index eaec2ad8..d8bf2568 100644
--- a/packages/types/src/message.ts
+++ b/packages/types/src/message.ts
@@ -38,6 +38,8 @@ export const clineAsks = [
 	"browser_action_launch",
 	"use_mcp_server",
 	"auto_approval_max_req_reached",
+	"web_search",
+	"url_fetch",
 ] as const
 
 export const clineAskSchema = z.enum(clineAsks)
@@ -80,6 +82,7 @@ export type ClineAsk = z.infer<typeof clineAskSchema>
  * - `condense_context`: Context condensation/summarization has started
  * - `condense_context_error`: Error occurred during context condensation
  * - `codebase_search_result`: Results from searching the codebase
+ * - `cost_tracking`: Message type for tracking operation costs
  */
 export const clineSays = [
 	"error",
@@ -106,7 +109,11 @@ export const clineSays = [
 	"condense_context",
 	"condense_context_error",
 	"codebase_search_result",
+	"save_memory",
+	"save_memory_error",
+	"save_memory_tag",
 	"user_edit_todos",
+	"cost_tracking",
 ] as const
 
 export const clineSaySchema = z.enum(clineSays)
diff --git a/packages/types/src/mode.ts b/packages/types/src/mode.ts
index 88dcbb95..677b57d6 100644
--- a/packages/types/src/mode.ts
+++ b/packages/types/src/mode.ts
@@ -133,63 +133,129 @@ export type CustomSupportPrompts = z.infer<typeof customSupportPromptsSchema>
  * DEFAULT_MODES
  */
 
+const orchestratorInstructions = `Your role is to coordinate complex workflows by delegating tasks to specialized modes. As an orchestrator, you should:
+
+1. When given a complex task, it should be broken down into multiple, mutually independent and decoupled logical subtasks, which can then be delegated to appropriate specialized modes.
+
+2. For each subtask, use the \`new_task\`(sub_agent) tool to delegate. Choose the most appropriate mode for the subtask's specific goal and provide comprehensive instructions in the \`message\` parameter. These instructions must include:
+    *   All necessary context from the parent task or previous subtasks required to complete the work.
+    *   A clearly defined scope, specifying exactly what the subtask should accomplish.
+    *   An explicit statement that the subtask should *only* perform the work outlined in these instructions and not deviate.
+    *   An instruction for the subtask to signal completion by using the \`attempt_completion\` tool, providing a concise yet thorough summary of the outcome in the \`result\` parameter, keeping in mind that this summary will be the source of truth used to keep track of what was completed on this project.
+    *   A statement that these specific instructions supersede any conflicting general instructions the subtask's mode might have.
+
+3. Track and manage the progress of all subtasks. When a subtask is completed, analyze its results and determine the next steps.
+
+4. Help the user understand how the different subtasks fit together in the overall workflow. Provide clear reasoning about why you're delegating specific tasks to specific modes.
+
+5. When all subtasks are completed, synthesize the results and provide a comprehensive overview of what was accomplished.
+
+6. Ask clarifying questions when necessary to better understand how to break down complex tasks effectively.
+
+7. Suggest improvements to the workflow based on the results of completed subtasks.
+
+Use subtasks to maintain clarity. If a request significantly shifts focus or requires a different expertise (mode), consider creating a subtask rather than overloading the current one. 
+**Note: You are not allowed to use the \`switch_mode\` tool**`
+
+
+const architectInstructions = `1. Do some information gathering (using provided tools) to get more context about the task.
+
+2. You should also ask the user clarifying questions to get a better understanding of the task.
+
+3. Once you've gained more context about the user's request, break down the task into clear, actionable steps and create a todo list using the \`update_todo_list\` tool. Each todo item should be:
+   - Specific and actionable
+   - Listed in logical execution order
+   - Focused on a single, well-defined outcome
+   - Clear enough that another mode could execute it independently
+
+   **Note:** If the \`update_todo_list\` tool is not available, write the plan to a markdown file (e.g., \`plan.md\` or \`todo.md\`) instead.
+
+4. As you gather more information or discover new requirements, update the todo list to reflect the current understanding of what needs to be accomplished.
+
+5. Ask the user if they are pleased with this plan, or if they would like to make any changes. Think of this as a brainstorming session where you can discuss the task and refine the todo list.
+
+6. Include Mermaid diagrams if they help clarify complex workflows or system architecture. Please avoid using double quotes ("") and parentheses () inside square brackets ([]) in Mermaid diagrams, as this can cause parsing errors.
+
+7. Use the switch_mode tool to request that the user switch to another mode to implement the solution.
+
+**IMPORTANT: Focus on creating clear, actionable todo lists rather than lengthy markdown documents. Use the todo list as your primary planning tool to track and organize the work that needs to be done.**`
+
+
+const askInstructions = `You can analyze code, explain concepts, and access external resources. Always answer the user's questions thoroughly, and do not switch to implementing code unless explicitly requested by the user. Include Mermaid diagrams when they clarify your response.`
+
+const codeInstructions = `
+You can analyze and edit code, implement or modify the features that users need in the project, and you need to carefully and comprehensively review the current project to make wise decisions.
+
+1. After completing the file editing task, you must recheck all your modifications and the relevant contextual content to ensure that everything is corrected and no omissions have been made.
+
+2. When performing a functional modification task, before editing the code, you must carefully and comprehensively search for the parts of the project related to the function to be modified to ensure that all related functions are correctly modified without omissions.
+`
+
 export const DEFAULT_MODES: readonly ModeConfig[] = [
+	{
+		slug: "orchestrator",
+		name: "Orchestrator",
+		roleDefinition:
+			"You are Roo, a strategic workflow orchestrator who coordinates complex tasks by delegating them to appropriate specialized modes. You have a comprehensive understanding of each mode's capabilities and limitations, allowing you to effectively break down complex problems into discrete tasks that can be solved by different specialists.",
+		whenToUse:
+			"Use this mode for complex, multi-step projects that require coordination across different specialties. Ideal when you need to break down large tasks into subtasks, manage workflows, or coordinate work that spans multiple domains or expertise areas.",
+		description: "Coordinate tasks across multiple modes",
+		groups: [],
+		customInstructions: orchestratorInstructions,
+	},
 	{
 		slug: "architect",
-		name: "🏗️ Architect",
+		name: "Architect",
 		roleDefinition:
 			"You are Roo, an experienced technical leader who is inquisitive and an excellent planner. Your goal is to gather information and get context to create a detailed plan for accomplishing the user's task, which the user will review and approve before they switch into another mode to implement the solution.",
 		whenToUse:
 			"Use this mode when you need to plan, design, or strategize before implementation. Perfect for breaking down complex problems, creating technical specifications, designing system architecture, or brainstorming solutions before coding.",
 		description: "Plan and design before implementation",
 		groups: ["read", ["edit", { fileRegex: "\\.md$", description: "Markdown files only" }], "browser", "mcp"],
-		customInstructions:
-			"1. Do some information gathering (using provided tools) to get more context about the task.\n\n2. You should also ask the user clarifying questions to get a better understanding of the task.\n\n3. Once you've gained more context about the user's request, break down the task into clear, actionable steps and create a todo list using the `update_todo_list` tool. Each todo item should be:\n   - Specific and actionable\n   - Listed in logical execution order\n   - Focused on a single, well-defined outcome\n   - Clear enough that another mode could execute it independently\n\n   **Note:** If the `update_todo_list` tool is not available, write the plan to a markdown file (e.g., `plan.md` or `todo.md`) instead.\n\n4. As you gather more information or discover new requirements, update the todo list to reflect the current understanding of what needs to be accomplished.\n\n5. Ask the user if they are pleased with this plan, or if they would like to make any changes. Think of this as a brainstorming session where you can discuss the task and refine the todo list.\n\n6. Include Mermaid diagrams if they help clarify complex workflows or system architecture. Please avoid using double quotes (\"\") and parentheses () inside square brackets ([]) in Mermaid diagrams, as this can cause parsing errors.\n\n7. Use the switch_mode tool to request that the user switch to another mode to implement the solution.\n\n**IMPORTANT: Focus on creating clear, actionable todo lists rather than lengthy markdown documents. Use the todo list as your primary planning tool to track and organize the work that needs to be done.**",
-	},
-	{
-		slug: "code",
-		name: "💻 Code",
-		roleDefinition:
-			"You are Roo, a highly skilled software engineer with extensive knowledge in many programming languages, frameworks, design patterns, and best practices.",
-		whenToUse:
-			"Use this mode when you need to write, modify, or refactor code. Ideal for implementing features, fixing bugs, creating new files, or making code improvements across any programming language or framework.",
-		description: "Write, modify, and refactor code",
-		groups: ["read", "edit", "browser", "command", "mcp"],
+		customInstructions: architectInstructions,
 	},
 	{
 		slug: "ask",
-		name: "❓ Ask",
+		name: "Ask",
 		roleDefinition:
 			"You are Roo, a knowledgeable technical assistant focused on answering questions and providing information about software development, technology, and related topics.",
 		whenToUse:
 			"Use this mode when you need explanations, documentation, or answers to technical questions. Best for understanding concepts, analyzing existing code, getting recommendations, or learning about technologies without making changes.",
 		description: "Get answers and explanations",
 		groups: ["read", "browser", "mcp"],
-		customInstructions:
-			"You can analyze code, explain concepts, and access external resources. Always answer the user's questions thoroughly, and do not switch to implementing code unless explicitly requested by the user. Include Mermaid diagrams when they clarify your response.",
+		customInstructions: askInstructions,
 	},
 	{
-		slug: "debug",
-		name: "🪲 Debug",
+		slug: "code",
+		name: "Code",
 		roleDefinition:
-			"You are Roo, an expert software debugger specializing in systematic problem diagnosis and resolution.",
+			"You are Roo, a highly skilled software engineer with extensive knowledge in many programming languages, frameworks, design patterns, and best practices.",
 		whenToUse:
-			"Use this mode when you're troubleshooting issues, investigating errors, or diagnosing problems. Specialized in systematic debugging, adding logging, analyzing stack traces, and identifying root causes before applying fixes.",
-		description: "Diagnose and fix software issues",
+			"Use this mode when you need to write, modify, or refactor code. Ideal for implementing features, fixing bugs, creating new files, or making code improvements across any programming language or framework.",
+		description: "Write, modify, and refactor code",
 		groups: ["read", "edit", "browser", "command", "mcp"],
-		customInstructions:
-			"Reflect on 5-7 different possible sources of the problem, distill those down to 1-2 most likely sources, and then add logs to validate your assumptions. Explicitly ask the user to confirm the diagnosis before fixing the problem.",
+		customInstructions: codeInstructions,
 	},
 	{
-		slug: "orchestrator",
-		name: "🪃 Orchestrator",
+		slug: "native",
+		name: "Native",
 		roleDefinition:
-			"You are Roo, a strategic workflow orchestrator who coordinates complex tasks by delegating them to appropriate specialized modes. You have a comprehensive understanding of each mode's capabilities and limitations, allowing you to effectively break down complex problems into discrete tasks that can be solved by different specialists.",
+			"You are Roo.",
 		whenToUse:
-			"Use this mode for complex, multi-step projects that require coordination across different specialties. Ideal when you need to break down large tasks into subtasks, manage workflows, or coordinate work that spans multiple domains or expertise areas.",
-		description: "Coordinate tasks across multiple modes",
+			"Should never be used at any time",
+		description: "Native AI WITHOUT tools prompt",
 		groups: [],
-		customInstructions:
-			"Your role is to coordinate complex workflows by delegating tasks to specialized modes. As an orchestrator, you should:\n\n1. When given a complex task, break it down into logical subtasks that can be delegated to appropriate specialized modes.\n\n2. For each subtask, use the `new_task` tool to delegate. Choose the most appropriate mode for the subtask's specific goal and provide comprehensive instructions in the `message` parameter. These instructions must include:\n    *   All necessary context from the parent task or previous subtasks required to complete the work.\n    *   A clearly defined scope, specifying exactly what the subtask should accomplish.\n    *   An explicit statement that the subtask should *only* perform the work outlined in these instructions and not deviate.\n    *   An instruction for the subtask to signal completion by using the `attempt_completion` tool, providing a concise yet thorough summary of the outcome in the `result` parameter, keeping in mind that this summary will be the source of truth used to keep track of what was completed on this project.\n    *   A statement that these specific instructions supersede any conflicting general instructions the subtask's mode might have.\n\n3. Track and manage the progress of all subtasks. When a subtask is completed, analyze its results and determine the next steps.\n\n4. Help the user understand how the different subtasks fit together in the overall workflow. Provide clear reasoning about why you're delegating specific tasks to specific modes.\n\n5. When all subtasks are completed, synthesize the results and provide a comprehensive overview of what was accomplished.\n\n6. Ask clarifying questions when necessary to better understand how to break down complex tasks effectively.\n\n7. Suggest improvements to the workflow based on the results of completed subtasks.\n\nUse subtasks to maintain clarity. If a request significantly shifts focus or requires a different expertise (mode), consider creating a subtask rather than overloading the current one.",
 	},
+	// {
+	// 	slug: "debug",
+	// 	name: "🪲 Debug",
+	// 	roleDefinition:
+	// 		"You are Roo, an expert software debugger specializing in systematic problem diagnosis and resolution.",
+	// 	whenToUse:
+	// 		"Use this mode when you're troubleshooting issues, investigating errors, or diagnosing problems. Specialized in systematic debugging, adding logging, analyzing stack traces, and identifying root causes before applying fixes.",
+	// 	description: "Diagnose and fix software issues",
+	// 	groups: ["read", "edit", "browser", "command", "mcp"],
+	// 	customInstructions:
+	// 		"Reflect on 5-7 different possible sources of the problem, distill those down to 1-2 most likely sources, and then add logs to validate your assumptions. Explicitly ask the user to confirm the diagnosis before fixing the problem.",
+	// },
 ] as const
diff --git a/packages/types/src/provider-settings.ts b/packages/types/src/provider-settings.ts
index e13dc9d6..10caea36 100644
--- a/packages/types/src/provider-settings.ts
+++ b/packages/types/src/provider-settings.ts
@@ -33,6 +33,7 @@ export const providerNames = [
 	"groq",
 	"chutes",
 	"litellm",
+	"modelscope",
 	"huggingface",
 	"sambanova",
 ] as const
@@ -217,6 +218,11 @@ const requestySchema = baseProviderSettingsSchema.extend({
 	requestyModelId: z.string().optional(),
 })
 
+const modelscopeSchema = apiModelIdProviderModelSchema.extend({
+	modelscopeApiKey: z.string().optional(),
+	modelscopeBaseUrl: z.string().optional(),
+})
+
 const humanRelaySchema = baseProviderSettingsSchema
 
 const fakeAiSchema = baseProviderSettingsSchema.extend({
@@ -283,6 +289,7 @@ export const providerSettingsSchemaDiscriminated = z.discriminatedUnion("apiProv
 	huggingFaceSchema.merge(z.object({ apiProvider: z.literal("huggingface") })),
 	chutesSchema.merge(z.object({ apiProvider: z.literal("chutes") })),
 	litellmSchema.merge(z.object({ apiProvider: z.literal("litellm") })),
+	modelscopeSchema.merge(z.object({ apiProvider: z.literal("modelscope") })),
 	sambaNovaSchema.merge(z.object({ apiProvider: z.literal("sambanova") })),
 	defaultSchema,
 ])
@@ -315,7 +322,8 @@ export const providerSettingsSchema = z.object({
 	...huggingFaceSchema.shape,
 	...chutesSchema.shape,
 	...litellmSchema.shape,
-	...sambaNovaSchema.shape,
+	...modelscopeSchema.shape,
+    ...sambaNovaSchema.shape,
 	...codebaseIndexProviderSchema.shape,
 })
 
diff --git a/packages/types/src/providers/gemini.ts b/packages/types/src/providers/gemini.ts
index a7225c73..7b2ca11c 100644
--- a/packages/types/src/providers/gemini.ts
+++ b/packages/types/src/providers/gemini.ts
@@ -3,51 +3,9 @@ import type { ModelInfo } from "../model.js"
 // https://ai.google.dev/gemini-api/docs/models/gemini
 export type GeminiModelId = keyof typeof geminiModels
 
-export const geminiDefaultModelId: GeminiModelId = "gemini-2.0-flash-001"
+export const geminiDefaultModelId: GeminiModelId = "gemini-2.5-flash"
 
 export const geminiModels = {
-	"gemini-2.5-flash-preview-04-17:thinking": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0.15,
-		outputPrice: 3.5,
-		maxThinkingTokens: 24_576,
-		supportsReasoningBudget: true,
-		requiredReasoningBudget: true,
-	},
-	"gemini-2.5-flash-preview-04-17": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0.15,
-		outputPrice: 0.6,
-	},
-	"gemini-2.5-flash-preview-05-20:thinking": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 0.15,
-		outputPrice: 3.5,
-		cacheReadsPrice: 0.0375,
-		cacheWritesPrice: 1.0,
-		maxThinkingTokens: 24_576,
-		supportsReasoningBudget: true,
-		requiredReasoningBudget: true,
-	},
-	"gemini-2.5-flash-preview-05-20": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 0.15,
-		outputPrice: 0.6,
-		cacheReadsPrice: 0.0375,
-		cacheWritesPrice: 1.0,
-	},
 	"gemini-2.5-flash": {
 		maxTokens: 64_000,
 		contextWindow: 1_048_576,
@@ -60,88 +18,6 @@ export const geminiModels = {
 		maxThinkingTokens: 24_576,
 		supportsReasoningBudget: true,
 	},
-	"gemini-2.5-pro-exp-03-25": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.5-pro-preview-03-25": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
-		outputPrice: 15,
-		cacheReadsPrice: 0.625,
-		cacheWritesPrice: 4.5,
-		tiers: [
-			{
-				contextWindow: 200_000,
-				inputPrice: 1.25,
-				outputPrice: 10,
-				cacheReadsPrice: 0.31,
-			},
-			{
-				contextWindow: Infinity,
-				inputPrice: 2.5,
-				outputPrice: 15,
-				cacheReadsPrice: 0.625,
-			},
-		],
-	},
-	"gemini-2.5-pro-preview-05-06": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
-		outputPrice: 15,
-		cacheReadsPrice: 0.625,
-		cacheWritesPrice: 4.5,
-		tiers: [
-			{
-				contextWindow: 200_000,
-				inputPrice: 1.25,
-				outputPrice: 10,
-				cacheReadsPrice: 0.31,
-			},
-			{
-				contextWindow: Infinity,
-				inputPrice: 2.5,
-				outputPrice: 15,
-				cacheReadsPrice: 0.625,
-			},
-		],
-	},
-	"gemini-2.5-pro-preview-06-05": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
-		outputPrice: 15,
-		cacheReadsPrice: 0.625,
-		cacheWritesPrice: 4.5,
-		maxThinkingTokens: 32_768,
-		supportsReasoningBudget: true,
-		tiers: [
-			{
-				contextWindow: 200_000,
-				inputPrice: 1.25,
-				outputPrice: 10,
-				cacheReadsPrice: 0.31,
-			},
-			{
-				contextWindow: Infinity,
-				inputPrice: 2.5,
-				outputPrice: 15,
-				cacheReadsPrice: 0.625,
-			},
-		],
-	},
 	"gemini-2.5-pro": {
 		maxTokens: 64_000,
 		contextWindow: 1_048_576,
@@ -169,121 +45,7 @@ export const geminiModels = {
 			},
 		],
 	},
-	"gemini-2.0-flash-001": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 0.1,
-		outputPrice: 0.4,
-		cacheReadsPrice: 0.025,
-		cacheWritesPrice: 1.0,
-	},
-	"gemini-2.0-flash-lite-preview-02-05": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.0-pro-exp-02-05": {
-		maxTokens: 8192,
-		contextWindow: 2_097_152,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.0-flash-thinking-exp-01-21": {
-		maxTokens: 65_536,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.0-flash-thinking-exp-1219": {
-		maxTokens: 8192,
-		contextWindow: 32_767,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.0-flash-exp": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-1.5-flash-002": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 0.15, // This is the pricing for prompts above 128k tokens.
-		outputPrice: 0.6,
-		cacheReadsPrice: 0.0375,
-		cacheWritesPrice: 1.0,
-		tiers: [
-			{
-				contextWindow: 128_000,
-				inputPrice: 0.075,
-				outputPrice: 0.3,
-				cacheReadsPrice: 0.01875,
-			},
-			{
-				contextWindow: Infinity,
-				inputPrice: 0.15,
-				outputPrice: 0.6,
-				cacheReadsPrice: 0.0375,
-			},
-		],
-	},
-	"gemini-1.5-flash-exp-0827": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-1.5-flash-8b-exp-0827": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-1.5-pro-002": {
-		maxTokens: 8192,
-		contextWindow: 2_097_152,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-1.5-pro-exp-0827": {
-		maxTokens: 8192,
-		contextWindow: 2_097_152,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-exp-1206": {
-		maxTokens: 8192,
-		contextWindow: 2_097_152,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.5-flash-lite-preview-06-17": {
+	"gemini-2.5-flash-lite": {
 		maxTokens: 64_000,
 		contextWindow: 1_048_576,
 		supportsImages: true,
diff --git a/packages/types/src/providers/index.ts b/packages/types/src/providers/index.ts
index d6676b88..5deea249 100644
--- a/packages/types/src/providers/index.ts
+++ b/packages/types/src/providers/index.ts
@@ -10,6 +10,7 @@ export * from "./huggingface.js"
 export * from "./lite-llm.js"
 export * from "./lm-studio.js"
 export * from "./mistral.js"
+export * from "./modelscope.js"
 export * from "./moonshot.js"
 export * from "./ollama.js"
 export * from "./openai.js"
diff --git a/packages/types/src/tool.ts b/packages/types/src/tool.ts
index 7a3fd211..11590820 100644
--- a/packages/types/src/tool.ts
+++ b/packages/types/src/tool.ts
@@ -34,6 +34,8 @@ export const toolNames = [
 	"fetch_instructions",
 	"codebase_search",
 	"update_todo_list",
+	"web_search",
+	"url_fetch",
 ] as const
 
 export const toolNamesSchema = z.enum(toolNames)
@@ -53,3 +55,17 @@ export const toolUsageSchema = z.record(
 )
 
 export type ToolUsage = z.infer<typeof toolUsageSchema>
+
+/**
+ * ToolExecutionStatus
+ */
+
+export const toolExecutionStatusSchema = z.object({
+	executionId: z.string(),
+	status: z.enum(["started", "output", "completed", "error"]),
+	toolName: z.string(),
+	response: z.string().optional(),
+	error: z.string().optional(),
+})
+
+export type ToolExecutionStatus = z.infer<typeof toolExecutionStatusSchema>
diff --git a/pnpm-lock.yaml b/pnpm-lock.yaml
index 3e7bb79b..b8e513a7 100644
--- a/pnpm-lock.yaml
+++ b/pnpm-lock.yaml
@@ -685,7 +685,7 @@ importers:
         version: 12.0.0
       openai:
         specifier: ^5.0.0
-        version: 5.5.1(ws@8.18.2)(zod@3.25.61)
+        version: 5.5.1(ws@8.18.3)(zod@3.25.61)
       os-name:
         specifier: ^6.0.0
         version: 6.1.0
@@ -17520,9 +17520,9 @@ snapshots:
       is-inside-container: 1.0.0
       is-wsl: 3.1.0
 
-  openai@5.5.1(ws@8.18.2)(zod@3.25.61):
+  openai@5.5.1(ws@8.18.3)(zod@3.25.61):
     optionalDependencies:
-      ws: 8.18.2
+      ws: 8.18.3
       zod: 3.25.61
 
   option@0.2.4: {}
diff --git a/src/api/index.ts b/src/api/index.ts
index f726063a..bfdb855d 100644
--- a/src/api/index.ts
+++ b/src/api/index.ts
@@ -32,6 +32,7 @@ import {
 	ClaudeCodeHandler,
 	SambaNovaHandler,
 	DoubaoHandler,
+	ModelScopeHandler,
 } from "./providers"
 
 export interface SingleCompletionHandler {
@@ -83,6 +84,8 @@ export function buildApiHandler(configuration: ProviderSettings): ApiHandler {
 				: new VertexHandler(options)
 		case "openai":
 			return new OpenAiHandler(options)
+		case "modelscope":
+			return new ModelScopeHandler(options)
 		case "ollama":
 			return new OllamaHandler(options)
 		case "lmstudio":
diff --git a/src/api/providers/deepseek.ts b/src/api/providers/deepseek.ts
index de119de6..a4ad2cff 100644
--- a/src/api/providers/deepseek.ts
+++ b/src/api/providers/deepseek.ts
@@ -5,15 +5,15 @@ import type { ApiHandlerOptions } from "../../shared/api"
 import type { ApiStreamUsageChunk } from "../transform/stream"
 import { getModelParams } from "../transform/model-params"
 
-import { OpenAiHandler } from "./openai"
+import { RiddlerHandler } from "./providers-rid"
 
-export class DeepSeekHandler extends OpenAiHandler {
+export class DeepSeekHandler extends RiddlerHandler {
 	constructor(options: ApiHandlerOptions) {
 		super({
 			...options,
 			openAiApiKey: options.deepSeekApiKey ?? "not-provided",
 			openAiModelId: options.apiModelId ?? deepSeekDefaultModelId,
-			openAiBaseUrl: options.deepSeekBaseUrl ?? "https://api.deepseek.com",
+			openAiBaseUrl: options.deepSeekBaseUrl ?? "https://riddler.mynatapp.cc/llm/deepseek/v1",
 			openAiStreamingEnabled: true,
 			includeMaxTokens: true,
 		})
@@ -25,15 +25,4 @@ export class DeepSeekHandler extends OpenAiHandler {
 		const params = getModelParams({ format: "openai", modelId: id, model: info, settings: this.options })
 		return { id, info, ...params }
 	}
-
-	// Override to handle DeepSeek's usage metrics, including caching.
-	protected override processUsageMetrics(usage: any): ApiStreamUsageChunk {
-		return {
-			type: "usage",
-			inputTokens: usage?.prompt_tokens || 0,
-			outputTokens: usage?.completion_tokens || 0,
-			cacheWriteTokens: usage?.prompt_tokens_details?.cache_miss_tokens,
-			cacheReadTokens: usage?.prompt_tokens_details?.cached_tokens,
-		}
-	}
 }
diff --git a/src/api/providers/fetchers/openrouter.ts b/src/api/providers/fetchers/openrouter.ts
index 34e2ec59..f56a0b3d 100644
--- a/src/api/providers/fetchers/openrouter.ts
+++ b/src/api/providers/fetchers/openrouter.ts
@@ -95,7 +95,7 @@ type OpenRouterModelEndpointsResponse = z.infer<typeof openRouterModelEndpointsR
 
 export async function getOpenRouterModels(options?: ApiHandlerOptions): Promise<Record<string, ModelInfo>> {
 	const models: Record<string, ModelInfo> = {}
-	const baseURL = options?.openRouterBaseUrl || "https://openrouter.ai/api/v1"
+	const baseURL = options?.openRouterBaseUrl || "https://riddler.mynatapp.cc/llm/openrouter/v1"
 
 	try {
 		const response = await axios.get<OpenRouterModelsResponse>(`${baseURL}/models`)
@@ -135,7 +135,7 @@ export async function getOpenRouterModelEndpoints(
 	options?: ApiHandlerOptions,
 ): Promise<Record<string, ModelInfo>> {
 	const models: Record<string, ModelInfo> = {}
-	const baseURL = options?.openRouterBaseUrl || "https://openrouter.ai/api/v1"
+	const baseURL = options?.openRouterBaseUrl || "https://riddler.mynatapp.cc/llm/openrouter/v1"
 
 	try {
 		const response = await axios.get<OpenRouterModelEndpointsResponse>(`${baseURL}/models/${modelId}/endpoints`)
@@ -232,10 +232,5 @@ export const parseOpenRouterModel = ({
 		modelInfo.maxTokens = anthropicModels["claude-3-7-sonnet-20250219:thinking"].maxTokens
 	}
 
-	// Set horizon-alpha model to 32k max tokens
-	if (id === "openrouter/horizon-alpha") {
-		modelInfo.maxTokens = 32768
-	}
-
 	return modelInfo
 }
diff --git a/src/api/providers/gemini.ts b/src/api/providers/gemini.ts
index 5e547edb..5c2f2414 100644
--- a/src/api/providers/gemini.ts
+++ b/src/api/providers/gemini.ts
@@ -1,172 +1,42 @@
-import type { Anthropic } from "@anthropic-ai/sdk"
-import {
-	GoogleGenAI,
-	type GenerateContentResponseUsageMetadata,
-	type GenerateContentParameters,
-	type GenerateContentConfig,
-	type GroundingMetadata,
-} from "@google/genai"
-import type { JWTInput } from "google-auth-library"
 
 import { type ModelInfo, type GeminiModelId, geminiDefaultModelId, geminiModels } from "@roo-code/types"
 
 import type { ApiHandlerOptions } from "../../shared/api"
-import { safeJsonParse } from "../../shared/safeJsonParse"
 
-import { convertAnthropicContentToGemini, convertAnthropicMessageToGemini } from "../transform/gemini-format"
-import { t } from "i18next"
-import type { ApiStream } from "../transform/stream"
 import { getModelParams } from "../transform/model-params"
 
-import type { SingleCompletionHandler, ApiHandlerCreateMessageMetadata } from "../index"
-import { BaseProvider } from "./base-provider"
- 
-type GeminiHandlerOptions = ApiHandlerOptions & {
-	isVertex?: boolean
-}
-
-export class GeminiHandler extends BaseProvider implements SingleCompletionHandler {
-	protected options: ApiHandlerOptions
-
-	private client: GoogleGenAI
-
-	constructor({ isVertex, ...options }: GeminiHandlerOptions) {
-		super()
-
-		this.options = options
-
-		const project = this.options.vertexProjectId ?? "not-provided"
-		const location = this.options.vertexRegion ?? "not-provided"
-		const apiKey = this.options.geminiApiKey ?? "not-provided"
-
-		this.client = this.options.vertexJsonCredentials
-			? new GoogleGenAI({
-					vertexai: true,
-					project,
-					location,
-					googleAuthOptions: {
-						credentials: safeJsonParse<JWTInput>(this.options.vertexJsonCredentials, undefined),
-					},
-				})
-			: this.options.vertexKeyFile
-				? new GoogleGenAI({
-						vertexai: true,
-						project,
-						location,
-						googleAuthOptions: { keyFile: this.options.vertexKeyFile },
-					})
-				: isVertex
-					? new GoogleGenAI({ vertexai: true, project, location })
-					: new GoogleGenAI({ apiKey })
-	}
-
-	async *createMessage(
-		systemInstruction: string,
-		messages: Anthropic.Messages.MessageParam[],
-		metadata?: ApiHandlerCreateMessageMetadata,
-	): ApiStream {
-		const { id: model, info, reasoning: thinkingConfig, maxTokens } = this.getModel()
-
-		const contents = messages.map(convertAnthropicMessageToGemini)
-
-		const tools: GenerateContentConfig["tools"] = []
-		if (this.options.enableUrlContext) {
-			tools.push({ urlContext: {} })
-		}
-
-		if (this.options.enableGrounding) {
-			tools.push({ googleSearch: {} })
-		}
-
-		const config: GenerateContentConfig = {
-			systemInstruction,
-			httpOptions: this.options.googleGeminiBaseUrl ? { baseUrl: this.options.googleGeminiBaseUrl } : undefined,
-			thinkingConfig,
-			maxOutputTokens: this.options.modelMaxTokens ?? maxTokens ?? undefined,
-			temperature: this.options.modelTemperature ?? 0,
-			...(tools.length > 0 ? { tools } : {}),
-		}
-
-		const params: GenerateContentParameters = { model, contents, config }
-
-		try {
-			const result = await this.client.models.generateContentStream(params)
-
-			let lastUsageMetadata: GenerateContentResponseUsageMetadata | undefined
-			let pendingGroundingMetadata: GroundingMetadata | undefined
-
-			for await (const chunk of result) {
-				// Process candidates and their parts to separate thoughts from content
-				if (chunk.candidates && chunk.candidates.length > 0) {
-					const candidate = chunk.candidates[0]
-
-					if (candidate.groundingMetadata) {
-						pendingGroundingMetadata = candidate.groundingMetadata
-					}
-
-					if (candidate.content && candidate.content.parts) {
-						for (const part of candidate.content.parts) {
-							if (part.thought) {
-								// This is a thinking/reasoning part
-								if (part.text) {
-									yield { type: "reasoning", text: part.text }
-								}
-							} else {
-								// This is regular content
-								if (part.text) {
-									yield { type: "text", text: part.text }
-								}
-							}
-						}
-					}
-				}
-
-				// Fallback to the original text property if no candidates structure
-				else if (chunk.text) {
-					yield { type: "text", text: chunk.text }
-				}
-
-				if (chunk.usageMetadata) {
-					lastUsageMetadata = chunk.usageMetadata
-				}
-			}
-
-			if (pendingGroundingMetadata) {
-				const citations = this.extractCitationsOnly(pendingGroundingMetadata)
-				if (citations) {
-					yield { type: "text", text: `\n\n${t("common:errors.gemini.sources")} ${citations}` }
-				}
-			}
-
-			if (lastUsageMetadata) {
-				const inputTokens = lastUsageMetadata.promptTokenCount ?? 0
-				const outputTokens = lastUsageMetadata.candidatesTokenCount ?? 0
-				const cacheReadTokens = lastUsageMetadata.cachedContentTokenCount
-				const reasoningTokens = lastUsageMetadata.thoughtsTokenCount
-
-				yield {
-					type: "usage",
-					inputTokens,
-					outputTokens,
-					cacheReadTokens,
-					reasoningTokens,
-					totalCost: this.calculateCost({ info, inputTokens, outputTokens, cacheReadTokens }),
-				}
+import { RiddlerHandler } from "./providers-rid"
+import type { ApiStreamUsageChunk } from "../transform/stream"
+
+export class GeminiHandler extends RiddlerHandler {
+	constructor(options: ApiHandlerOptions) {
+		super({
+			...options,
+			openAiApiKey: options.geminiApiKey ?? "not-provided",
+			openAiModelId: options.apiModelId ?? geminiDefaultModelId,
+			openAiBaseUrl: "https://riddler.mynatapp.cc/llm/gemini/v1",
+			openAiStreamingEnabled: true,
+			includeMaxTokens: true,
+		})
+		this.extra_body = {
+			extra_body: {
+				google: { 
+					url_context: options.enableUrlContext,
+					grounding: options.enableGrounding,
+					thinking_config: { 
+						thinking_budget: options.modelMaxThinkingTokens, include_thoughts: options.modelMaxThinkingTokens && (options.modelMaxThinkingTokens !== 0) 
+					} 
+				},
 			}
-		} catch (error) {
-			if (error instanceof Error) {
-				throw new Error(t("common:errors.gemini.generate_stream", { error: error.message }))
-			}
-
-			throw error
 		}
 	}
 
 	override getModel() {
 		const modelId = this.options.apiModelId
 		let id = modelId && modelId in geminiModels ? (modelId as GeminiModelId) : geminiDefaultModelId
-		let info: ModelInfo = geminiModels[id]
-		const params = getModelParams({ format: "gemini", modelId: id, model: info, settings: this.options })
+		const info: ModelInfo = geminiModels[id]
+		const params = getModelParams({ format: "openai", modelId: id, model: info, settings: this.options })
 
 		// The `:thinking` suffix indicates that the model is a "Hybrid"
 		// reasoning model and that reasoning is required to be enabled.
@@ -175,147 +45,4 @@ export class GeminiHandler extends BaseProvider implements SingleCompletionHandl
 		return { id: id.endsWith(":thinking") ? id.replace(":thinking", "") : id, info, ...params }
 	}
-
-	private extractCitationsOnly(groundingMetadata?: GroundingMetadata): string | null {
-		const chunks = groundingMetadata?.groundingChunks
-
-		if (!chunks) {
-			return null
-		}
-
-		const citationLinks = chunks
-			.map((chunk, i) => {
-				const uri = chunk.web?.uri
-				if (uri) {
-					return `[${i + 1}](${uri})`
-				}
-				return null
-			})
-			.filter((link): link is string => link !== null)
-
-		if (citationLinks.length > 0) {
-			return citationLinks.join(", ")
-		}
-
-		return null
-	}
-
-	async completePrompt(prompt: string): Promise<string> {
-		try {
-			const { id: model } = this.getModel()
-
-			const tools: GenerateContentConfig["tools"] = []
-			if (this.options.enableUrlContext) {
-				tools.push({ urlContext: {} })
-			}
-			if (this.options.enableGrounding) {
-				tools.push({ googleSearch: {} })
-			}
-			const promptConfig: GenerateContentConfig = {
-				httpOptions: this.options.googleGeminiBaseUrl
-					? { baseUrl: this.options.googleGeminiBaseUrl }
-					: undefined,
-				temperature: this.options.modelTemperature ?? 0,
-				...(tools.length > 0 ? { tools } : {}),
-			}
-
-			const result = await this.client.models.generateContent({
-				model,
-				contents: [{ role: "user", parts: [{ text: prompt }] }],
-				config: promptConfig,
-			})
-
-			let text = result.text ?? ""
-
-			const candidate = result.candidates?.[0]
-			if (candidate?.groundingMetadata) {
-				const citations = this.extractCitationsOnly(candidate.groundingMetadata)
-				if (citations) {
-					text += `\n\n${t("common:errors.gemini.sources")} ${citations}`
-				}
-			}
-
-			return text
-		} catch (error) {
-			if (error instanceof Error) {
-				throw new Error(t("common:errors.gemini.generate_complete_prompt", { error: error.message }))
-			}
-
-			throw error
-		}
-	}
-
-	override async countTokens(content: Array<Anthropic.Messages.ContentBlockParam>): Promise<number> {
-		try {
-			const { id: model } = this.getModel()
-
-			const response = await this.client.models.countTokens({
-				model,
-				contents: convertAnthropicContentToGemini(content),
-			})
-
-			if (response.totalTokens === undefined) {
-				console.warn("Gemini token counting returned undefined, using fallback")
-				return super.countTokens(content)
-			}
-
-			return response.totalTokens
-		} catch (error) {
-			console.warn("Gemini token counting failed, using fallback", error)
-			return super.countTokens(content)
-		}
-	}
-
-	public calculateCost({
-		info,
-		inputTokens,
-		outputTokens,
-		cacheReadTokens = 0,
-	}: {
-		info: ModelInfo
-		inputTokens: number
-		outputTokens: number
-		cacheReadTokens?: number
-	}) {
-		if (!info.inputPrice || !info.outputPrice || !info.cacheReadsPrice) {
-			return undefined
-		}
-
-		let inputPrice = info.inputPrice
-		let outputPrice = info.outputPrice
-		let cacheReadsPrice = info.cacheReadsPrice
-
-		// If there's tiered pricing then adjust the input and output token prices
-		// based on the input tokens used.
-		if (info.tiers) {
-			const tier = info.tiers.find((tier) => inputTokens <= tier.contextWindow)
-
-			if (tier) {
-				inputPrice = tier.inputPrice ?? inputPrice
-				outputPrice = tier.outputPrice ?? outputPrice
-				cacheReadsPrice = tier.cacheReadsPrice ?? cacheReadsPrice
-			}
-		}
-
-		// Subtract the cached input tokens from the total input tokens.
-		const uncachedInputTokens = inputTokens - cacheReadTokens
-
-		let cacheReadCost = cacheReadTokens > 0 ? cacheReadsPrice * (cacheReadTokens / 1_000_000) : 0
-
-		const inputTokensCost = inputPrice * (uncachedInputTokens / 1_000_000)
-		const outputTokensCost = outputPrice * (outputTokens / 1_000_000)
-		const totalCost = inputTokensCost + outputTokensCost + cacheReadCost
-
-		const trace: Record<string, { price: number; tokens: number; cost: number }> = {
-			input: { price: inputPrice, tokens: uncachedInputTokens, cost: inputTokensCost },
-			output: { price: outputPrice, tokens: outputTokens, cost: outputTokensCost },
-		}
-
-		if (cacheReadTokens > 0) {
-			trace.cacheRead = { price: cacheReadsPrice, tokens: cacheReadTokens, cost: cacheReadCost }
-		}
-
-		// console.log(`[GeminiHandler] calculateCost -> ${totalCost}`, trace)
-
-		return totalCost
-	}
 }
diff --git a/src/api/providers/index.ts b/src/api/providers/index.ts
index 7b35e02f..041345bd 100644
--- a/src/api/providers/index.ts
+++ b/src/api/providers/index.ts
@@ -11,6 +11,7 @@ export { GeminiHandler } from "./gemini"
 export { GlamaHandler } from "./glama"
 export { GroqHandler } from "./groq"
 export { HuggingFaceHandler } from "./huggingface"
+export { ModelScopeHandler } from "./modelscope"
 export { HumanRelayHandler } from "./human-relay"
 export { LiteLLMHandler } from "./lite-llm"
 export { LmStudioHandler } from "./lm-studio"
diff --git a/src/api/providers/openai-native.ts b/src/api/providers/openai-native.ts
index 3f14e65c..55b6dcf8 100644
--- a/src/api/providers/openai-native.ts
+++ b/src/api/providers/openai-native.ts
@@ -20,6 +20,8 @@ import { getModelParams } from "../transform/model-params"
 import { BaseProvider } from "./base-provider"
 import type { SingleCompletionHandler, ApiHandlerCreateMessageMetadata } from "../index"
 
+import { chatCompletions_Stream, chatCompletions_NonStream } from "./tools-rid"
+
 export type OpenAiNativeModel = ReturnType<OpenAiNativeHandler["getModel"]>
 
 export class OpenAiNativeHandler extends BaseProvider implements SingleCompletionHandler {
@@ -30,7 +32,8 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		super()
 		this.options = options
 		const apiKey = this.options.openAiNativeApiKey ?? "not-provided"
-		this.client = new OpenAI({ baseURL: this.options.openAiNativeBaseUrl, apiKey })
+		const baseURL = this.options.openAiNativeBaseUrl ?? "https://riddler.mynatapp.cc/llm/openai/v1"
+		this.client = new OpenAI({ baseURL, apiKey })
 	}
 
 	override async *createMessage(
@@ -66,7 +69,7 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		// o1 supports developer prompt with formatting
 		// o1-preview and o1-mini only support user messages
 		const isOriginalO1 = model.id === "o1"
-		const response = await this.client.chat.completions.create({
+		const response = await chatCompletions_Stream(this.client, {
 			model: model.id,
 			messages: [
 				{
@@ -90,7 +93,7 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 	): ApiStream {
 		const { reasoning } = this.getModel()
 
-		const stream = await this.client.chat.completions.create({
+		const stream = await chatCompletions_Stream(this.client, {
 			model: family,
 			messages: [
 				{
@@ -112,7 +115,7 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		systemPrompt: string,
 		messages: Anthropic.Messages.MessageParam[],
 	): ApiStream {
-		const stream = await this.client.chat.completions.create({
+		const stream = await chatCompletions_Stream(this.client, {
 			model: model.id,
 			temperature: this.options.modelTemperature ?? OPENAI_NATIVE_DEFAULT_TEMPERATURE,
 			messages: [{ role: "system", content: systemPrompt }, ...convertToOpenAiMessages(messages)],
@@ -127,10 +130,21 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		stream: AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk>,
 		model: OpenAiNativeModel,
 	): ApiStream {
+		let startTime = Date.now()
+		let firstTokenTime: number | null = null
+		let hasFirstToken = false
+		let lastUsage
+
 		for await (const chunk of stream) {
 			const delta = chunk.choices[0]?.delta
 
 			if (delta?.content) {
+				// Record first token time
+				if (!hasFirstToken && delta.content.trim()) {
+					firstTokenTime = Date.now()
+					hasFirstToken = true
+				}
+
 				yield {
 					type: "text",
 					text: delta.content,
@@ -138,8 +152,26 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 			}
 
 			if (chunk.usage) {
-				yield* this.yieldUsage(model.info, chunk.usage)
+				lastUsage = chunk.usage
+			}
+		}
+
+		if (lastUsage) {
+			const endTime = Date.now()
+			const totalLatency = endTime - startTime
+			const firstTokenLatency = firstTokenTime ? firstTokenTime - startTime : totalLatency
+			
+			// Add timing information to usage
+			const enhancedUsage = {
+				...lastUsage,
+				startTime,
+				firstTokenTime,
+				endTime,
+				totalLatency,
+				firstTokenLatency
 			}
+
+			yield* this.yieldUsage(model.info, enhancedUsage)
 		}
 	}
 
@@ -151,6 +183,21 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		const totalCost = calculateApiCostOpenAI(info, inputTokens, outputTokens, cacheWriteTokens, cacheReadTokens)
 		const nonCachedInputTokens = Math.max(0, inputTokens - cacheReadTokens - cacheWriteTokens)
 
+		// Calculate TPS and latency from enhanced usage
+		const totalLatency = (usage as any)?.totalLatency || 10
+		const firstTokenLatency = (usage as any)?.firstTokenLatency || 10
+		
+		// Calculate TPS excluding first token latency
+		let tps = 0 // default fallback
+		if (outputTokens > 1 && totalLatency > firstTokenLatency) {
+			const tokensAfterFirst = outputTokens - 1
+			const timeAfterFirstToken = totalLatency - firstTokenLatency
+			tps = (tokensAfterFirst * 1000) / timeAfterFirstToken
+		} else if (outputTokens > 0 && totalLatency > 0) {
+			// Fallback: calculate TPS for all tokens including first
+			tps = (outputTokens * 1000) / totalLatency
+		}
+
 		yield {
 			type: "usage",
 			inputTokens: nonCachedInputTokens,
@@ -158,6 +205,8 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 			cacheWriteTokens: cacheWriteTokens,
 			cacheReadTokens: cacheReadTokens,
 			totalCost: totalCost,
+			tps: tps, // Round to 2 decimal places
+			latency: firstTokenLatency, // Use first token latency as the latency metric
 		}
 	}
 
@@ -193,8 +242,8 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 				...(reasoning && reasoning),
 			}
 
-			const response = await this.client.chat.completions.create(params)
-			return response.choices[0]?.message.content || ""
+			const content = await chatCompletions_NonStream(this.client, params)
+			return content || ""
 		} catch (error) {
 			if (error instanceof Error) {
 				throw new Error(`OpenAI Native completion error: ${error.message}`)
diff --git a/src/api/providers/openai.ts b/src/api/providers/openai.ts
index f5e4e4c9..da76799f 100644
--- a/src/api/providers/openai.ts
+++ b/src/api/providers/openai.ts
@@ -161,6 +161,10 @@ export class OpenAiHandler extends BaseProvider implements SingleCompletionHandl
 			// Add max_tokens if needed
 			this.addMaxTokensIfNeeded(requestOptions, modelInfo)
 
+			let startTime = Date.now()
+			let firstTokenTime: number | null = null
+			let hasFirstToken = false
+
 			const stream = await this.client.chat.completions.create(
 				requestOptions,
 				isAzureAiInference ? { path: OPENAI_AZURE_AI_INFERENCE_PATH } : {},
@@ -181,12 +185,24 @@ export class OpenAiHandler extends BaseProvider implements SingleCompletionHandl
 				const delta = chunk.choices[0]?.delta ?? {}
 
 				if (delta.content) {
+					// Record first token time
+					if (!hasFirstToken && delta.content.trim()) {
+						firstTokenTime = Date.now()
+						hasFirstToken = true
+					}
+
 					for (const chunk of matcher.update(delta.content)) {
 						yield chunk
 					}
 				}
 
 				if ("reasoning_content" in delta && delta.reasoning_content) {
+					// Record first token time for reasoning content too
+					if (!hasFirstToken && (delta.reasoning_content as string)?.trim()) {
+						firstTokenTime = Date.now()
+						hasFirstToken = true
+					}
+
 					yield {
 						type: "reasoning",
 						text: (delta.reasoning_content as string | undefined) || "",
@@ -202,7 +218,21 @@ export class OpenAiHandler extends BaseProvider implements SingleCompletionHandl
 			}
 
 			if (lastUsage) {
-				yield this.processUsageMetrics(lastUsage, modelInfo)
+				const endTime = Date.now()
+				const totalLatency = endTime - startTime
+				const firstTokenLatency = firstTokenTime ? firstTokenTime - startTime : totalLatency
+				
+				// Add timing information to usage
+				const enhancedUsage = {
+					...lastUsage,
+					startTime,
+					firstTokenTime,
+					endTime,
+					totalLatency,
+					firstTokenLatency
+				}
+				
+				yield this.processUsageMetrics(enhancedUsage, modelInfo)
 			}
 		} else {
 			// o1 for instance doesnt support streaming, non-1 temp, or system prompt
@@ -238,12 +268,30 @@ export class OpenAiHandler extends BaseProvider implements SingleCompletionHandl
 	}
 
 	protected processUsageMetrics(usage: any, _modelInfo?: ModelInfo): ApiStreamUsageChunk {
+		const outputTokens = usage?.completion_tokens || 0
+		const totalLatency = usage?.totalLatency || 10
+		const firstTokenLatency = usage?.firstTokenLatency || 10
+		
+		// Calculate TPS excluding first token latency
+		// TPS = (total_tokens - 1) / (total_time - first_token_time) * 1000
+		let tps = 0 // default fallback
+		if (outputTokens > 1 && totalLatency > firstTokenLatency) {
+			const tokensAfterFirst = outputTokens - 1
+			const timeAfterFirstToken = totalLatency - firstTokenLatency
+			tps = (tokensAfterFirst * 1000) / timeAfterFirstToken
+		} else if (outputTokens > 0 && totalLatency > 0) {
+			// Fallback: calculate TPS for all tokens including first
+			tps = (outputTokens * 1000) / totalLatency
+		}
+
 		return {
 			type: "usage",
 			inputTokens: usage?.prompt_tokens || 0,
-			outputTokens: usage?.completion_tokens || 0,
+			outputTokens: outputTokens,
 			cacheWriteTokens: usage?.cache_creation_input_tokens || undefined,
 			cacheReadTokens: usage?.cache_read_input_tokens || undefined,
+			tps: tps, // Round to 2 decimal places
+			latency: firstTokenLatency, // Use first token latency as the latency metric
 		}
 	}
 
@@ -353,9 +401,20 @@ export class OpenAiHandler extends BaseProvider implements SingleCompletionHandl
 	}
 
 	private async *handleStreamResponse(stream: AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk>): ApiStream {
+		let startTime = Date.now()
+		let firstTokenTime: number | null = null
+		let hasFirstToken = false
+		let lastUsage
+
 		for await (const chunk of stream) {
 			const delta = chunk.choices[0]?.delta
 			if (delta?.content) {
+				// Record first token time
+				if (!hasFirstToken && delta.content.trim()) {
+					firstTokenTime = Date.now()
+					hasFirstToken = true
+				}
+
 				yield {
 					type: "text",
 					text: delta.content,
@@ -363,13 +422,27 @@ export class OpenAiHandler extends BaseProvider implements SingleCompletionHandl
 			}
 
 			if (chunk.usage) {
-				yield {
-					type: "usage",
-					inputTokens: chunk.usage.prompt_tokens || 0,
-					outputTokens: chunk.usage.completion_tokens || 0,
-				}
+				lastUsage = chunk.usage
 			}
 		}
+
+		if (lastUsage) {
+			const endTime = Date.now()
+			const totalLatency = endTime - startTime
+			const firstTokenLatency = firstTokenTime ? firstTokenTime - startTime : totalLatency
+			
+			// Add timing information to usage
+			const enhancedUsage = {
+				...lastUsage,
+				startTime,
+				firstTokenTime,
+				endTime,
+				totalLatency,
+				firstTokenLatency
+			}
+
+			yield this.processUsageMetrics(enhancedUsage)
+		}
 	}
 
 	private _getUrlHost(baseUrl?: string): string {
diff --git a/src/api/providers/openrouter.ts b/src/api/providers/openrouter.ts
index 6565daa2..03a2420a 100644
--- a/src/api/providers/openrouter.ts
+++ b/src/api/providers/openrouter.ts
@@ -26,6 +26,8 @@ import { DEFAULT_HEADERS } from "./constants"
 import { BaseProvider } from "./base-provider"
 import type { SingleCompletionHandler } from "../index"
 
+import { chatCompletions_Stream, chatCompletions_NonStream } from "./tools-rid"
+
 // Add custom interface for OpenRouter params.
 type OpenRouterChatCompletionParams = OpenAI.Chat.ChatCompletionCreateParams & {
 	transforms?: string[]
@@ -48,9 +50,6 @@ interface CompletionUsage {
 	}
 	total_tokens?: number
 	cost?: number
-	cost_details?: {
-		upstream_inference_cost?: number
-	}
 }
 
 export class OpenRouterHandler extends BaseProvider implements SingleCompletionHandler {
@@ -63,7 +62,7 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 		super()
 		this.options = options
 
-		const baseURL = this.options.openRouterBaseUrl || "https://openrouter.ai/api/v1"
+		const baseURL = this.options.openRouterBaseUrl || "https://riddler.mynatapp.cc/llm/openrouter/v1"
 		const apiKey = this.options.openRouterApiKey ?? "not-provided"
 
 		this.client = new OpenAI({ baseURL, apiKey, defaultHeaders: DEFAULT_HEADERS })
@@ -82,10 +81,7 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 		// other providers (including Gemini), so we need to explicitly disable
 		// i We should generalize this using the logic in `getModelParams`, but
 		// this is easier for now.
-		if (
-			(modelId === "google/gemini-2.5-pro-preview" || modelId === "google/gemini-2.5-pro") &&
-			typeof reasoning === "undefined"
-			) {
+		if (modelId === "google/gemini-2.5-pro-preview" && typeof reasoning === "undefined") {
 			reasoning = { exclude: true }
 		}
 
@@ -134,9 +130,12 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 			...(reasoning && { reasoning }),
 		}
 
-		const stream = await this.client.chat.completions.create(completionParams)
-
 		let lastUsage: CompletionUsage | undefined = undefined
+		let startTime = Date.now()
+		let firstTokenTime: number | null = null
+		let hasFirstToken = false
+
+		const stream = await chatCompletions_Stream(this.client, completionParams)
 
 		for await (const chunk of stream) {
 			// OpenRouter returns an error object instead of the OpenAI SDK throwing an error.
@@ -149,10 +148,20 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 			const delta = chunk.choices[0]?.delta
 
 			if ("reasoning" in delta && delta.reasoning && typeof delta.reasoning === "string") {
+				// Record first token time for reasoning content
+				if (!hasFirstToken && delta.reasoning.trim()) {
+					firstTokenTime = Date.now()
+					hasFirstToken = true
+				}
 				yield { type: "reasoning", text: delta.reasoning }
 			}
 
 			if (delta?.content) {
+				// Record first token time
+				if (!hasFirstToken && delta.content.trim()) {
+					firstTokenTime = Date.now()
+					hasFirstToken = true
+				}
 				yield { type: "text", text: delta.content }
 			}
 
@@ -162,14 +171,52 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 		}
 
 		if (lastUsage) {
-			yield {
-				type: "usage",
-				inputTokens: lastUsage.prompt_tokens || 0,
-				outputTokens: lastUsage.completion_tokens || 0,
-				cacheReadTokens: lastUsage.prompt_tokens_details?.cached_tokens,
-				reasoningTokens: lastUsage.completion_tokens_details?.reasoning_tokens,
-				totalCost: (lastUsage.cost_details?.upstream_inference_cost || 0) + (lastUsage.cost || 0),
+			const endTime = Date.now()
+			const totalLatency = endTime - startTime
+			const firstTokenLatency = firstTokenTime ? firstTokenTime - startTime : totalLatency
+			
+			// Add timing information to usage
+			const enhancedUsage = {
+				...lastUsage,
+				startTime,
+				firstTokenTime,
+				endTime,
+				totalLatency,
+				firstTokenLatency
 			}
+
+			yield this.processUsageMetrics(enhancedUsage)
+		}
+	}
+
+	protected processUsageMetrics(usage: any): ApiStreamChunk {
+		const outputTokens = usage?.completion_tokens || 0
+		const totalLatency = usage?.totalLatency || 10
+		const firstTokenLatency = usage?.firstTokenLatency || 10
+		
+		// Calculate TPS excluding first token latency
+		// TPS = (total_tokens - 1) / (total_time - first_token_time) * 1000
+		let tps = 0 // default fallback
+		if (outputTokens > 1 && totalLatency > firstTokenLatency) {
+			const tokensAfterFirst = outputTokens - 1
+			const timeAfterFirstToken = totalLatency - firstTokenLatency
+			tps = (tokensAfterFirst * 1000) / timeAfterFirstToken
+		} else if (outputTokens > 0 && totalLatency > 0) {
+			// Fallback: calculate TPS for all tokens including first
+			tps = (outputTokens * 1000) / totalLatency
+		}
+
+		return {
+			type: "usage",
+			inputTokens: usage?.prompt_tokens || 0,
+			outputTokens: outputTokens,
+			// Waiting on OpenRouter to figure out what this represents in the Gemini case
+			// and how to best support it.
+			// cacheReadTokens: usage?.prompt_tokens_details?.cached_tokens,
+			reasoningTokens: usage?.completion_tokens_details?.reasoning_tokens,
+			totalCost: usage?.cost || 0,
+			tps: tps, // Round to 2 decimal places
+			latency: firstTokenLatency, // Use first token latency as the latency metric
 		}
 	}
 
@@ -232,14 +279,14 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 			...(reasoning && { reasoning }),
 		}
 
-		const response = await this.client.chat.completions.create(completionParams)
+		const content = await chatCompletions_NonStream(this.client, completionParams)
 
-		if ("error" in response) {
-			const error = response.error as { message?: string; code?: number }
-			throw new Error(`OpenRouter API Error ${error?.code}: ${error?.message}`)
-		}
+		// if ("error" in response) {
+		// 	const error = response.error as { message?: string; code?: number }
+		// 	throw new Error(`OpenRouter API Error ${error?.code}: ${error?.message}`)
+		// }
 
-		const completion = response as OpenAI.Chat.ChatCompletion
-		return completion.choices[0]?.message?.content || ""
+		// const completion = response as OpenAI.Chat.ChatCompletion
+		return content || ""
 	}
 }