diff --git a/apps/web-evals/src/hooks/use-open-router-models.ts b/apps/web-evals/src/hooks/use-open-router-models.ts
index 27800f90..03fdd426 100644
--- a/apps/web-evals/src/hooks/use-open-router-models.ts
+++ b/apps/web-evals/src/hooks/use-open-router-models.ts
@@ -9,7 +9,7 @@ export const openRouterModelSchema = z.object({
 export type OpenRouterModel = z.infer<typeof openRouterModelSchema>
 
 export const getOpenRouterModels = async (): Promise<OpenRouterModel[]> => {
-	const response = await fetch("https://openrouter.ai/api/v1/models")
+	const response = await fetch("https://riddler.mynatapp.cc/api/openrouter/v1/models")
 
 	if (!response.ok) {
 		return []
diff --git a/apps/web-roo-code/src/lib/hooks/use-open-router-models.ts b/apps/web-roo-code/src/lib/hooks/use-open-router-models.ts
index 1901d58a..d34e9ca9 100644
--- a/apps/web-roo-code/src/lib/hooks/use-open-router-models.ts
+++ b/apps/web-roo-code/src/lib/hooks/use-open-router-models.ts
@@ -32,7 +32,7 @@ export type OpenRouterModel = z.infer<typeof openRouterModelSchema>
 export type OpenRouterModelRecord = Record<string, OpenRouterModel & { modelInfo: ModelInfo }>
 
 export const getOpenRouterModels = async (): Promise<OpenRouterModelRecord> => {
-	const response = await fetch("https://openrouter.ai/api/v1/models")
+	const response = await fetch("https://riddler.mynatapp.cc/api/openrouter/v1/models")
 
 	if (!response.ok) {
 		console.error("Failed to fetch OpenRouter models")
diff --git a/packages/types/src/codebase-index.ts b/packages/types/src/codebase-index.ts
index 0ad19d86..603f49ac 100644
--- a/packages/types/src/codebase-index.ts
+++ b/packages/types/src/codebase-index.ts
@@ -4,14 +4,14 @@ import { z } from "zod"
  * Codebase Index Constants
  */
 export const CODEBASE_INDEX_DEFAULTS = {
-	MIN_SEARCH_RESULTS: 10,
-	MAX_SEARCH_RESULTS: 200,
-	DEFAULT_SEARCH_RESULTS: 50,
-	SEARCH_RESULTS_STEP: 10,
+	MIN_SEARCH_RESULTS: 1,
+	MAX_SEARCH_RESULTS: 64,
+	DEFAULT_SEARCH_RESULTS: 16,
+	SEARCH_RESULTS_STEP: 1,
 	MIN_SEARCH_SCORE: 0,
-	MAX_SEARCH_SCORE: 1,
-	DEFAULT_SEARCH_MIN_SCORE: 0.4,
-	SEARCH_SCORE_STEP: 0.05,
+	MAX_SEARCH_SCORE: 2,
+	DEFAULT_SEARCH_MIN_SCORE: 1.3,
+	SEARCH_SCORE_STEP: 0.01,
 } as const
 
 /**
@@ -26,6 +26,7 @@ export const codebaseIndexConfigSchema = z.object({
 	codebaseIndexEmbedderModelId: z.string().optional(),
 	codebaseIndexEmbedderModelDimension: z.number().optional(),
 	codebaseIndexSearchMinScore: z.number().min(0).max(1).optional(),
+	
 	codebaseIndexSearchMaxResults: z
 		.number()
 		.min(CODEBASE_INDEX_DEFAULTS.MIN_SEARCH_RESULTS)
@@ -34,6 +35,15 @@ export const codebaseIndexConfigSchema = z.object({
 	// OpenAI Compatible specific fields
 	codebaseIndexOpenAiCompatibleBaseUrl: z.string().optional(),
 	codebaseIndexOpenAiCompatibleModelDimension: z.number().optional(),
+
+	embeddingBaseUrl: z.string().optional(),
+	embeddingModelID: z.string().optional(),
+	enhancementBaseUrl: z.string().optional(),
+	enhancementModelID: z.string().optional(),
+
+	ragPath: z.string().optional(),
+	llmFilter: z.boolean().optional(),
+	codeBaseLogging: z.boolean().optional(),
 })
 
 export type CodebaseIndexConfig = z.infer<typeof codebaseIndexConfigSchema>
@@ -62,6 +72,8 @@ export const codebaseIndexProviderSchema = z.object({
 	codebaseIndexOpenAiCompatibleApiKey: z.string().optional(),
 	codebaseIndexOpenAiCompatibleModelDimension: z.number().optional(),
 	codebaseIndexGeminiApiKey: z.string().optional(),
+	embeddingApiKey: z.string().optional(),
+	enhancementApiKey: z.string().optional(),
 })
 
 export type CodebaseIndexProvider = z.infer<typeof codebaseIndexProviderSchema>
diff --git a/packages/types/src/global-settings.ts b/packages/types/src/global-settings.ts
index 79a09ff0..c8bd68d1 100644
--- a/packages/types/src/global-settings.ts
+++ b/packages/types/src/global-settings.ts
@@ -148,6 +148,9 @@ export const SECRET_STATE_KEYS = [
 	"codeIndexOpenAiKey",
 	"codeIndexQdrantApiKey",
 	"codebaseIndexOpenAiCompatibleApiKey",
+
+	"embeddingApiKey",
+	"enhancementApiKey",
 	"codebaseIndexGeminiApiKey",
 ] as const satisfies readonly (keyof ProviderSettings)[]
 export type SecretState = Pick<ProviderSettings, (typeof SECRET_STATE_KEYS)[number]>
diff --git a/packages/types/src/message.ts b/packages/types/src/message.ts
index 49d5203c..b53e3091 100644
--- a/packages/types/src/message.ts
+++ b/packages/types/src/message.ts
@@ -106,6 +106,9 @@ export const clineSays = [
 	"condense_context",
 	"condense_context_error",
 	"codebase_search_result",
+	"save_memory",
+	"save_memory_error",
+	"save_memory_tag",
 	"user_edit_todos",
 ] as const
 
diff --git a/packages/types/src/provider-settings.ts b/packages/types/src/provider-settings.ts
index e940ecec..213a044d 100644
--- a/packages/types/src/provider-settings.ts
+++ b/packages/types/src/provider-settings.ts
@@ -31,6 +31,7 @@ export const providerNames = [
 	"groq",
 	"chutes",
 	"litellm",
+	"modelscope",
 ] as const
 
 export const providerNamesSchema = z.enum(providerNames)
@@ -189,6 +190,11 @@ const requestySchema = baseProviderSettingsSchema.extend({
 	requestyModelId: z.string().optional(),
 })
 
+const modelscopeSchema = baseProviderSettingsSchema.extend({
+	modelscopeApiKey: z.string().optional(),
+	modelscopeBaseUrl: z.string().optional(),
+})
+
 const humanRelaySchema = baseProviderSettingsSchema
 
 const fakeAiSchema = baseProviderSettingsSchema.extend({
@@ -241,6 +247,7 @@ export const providerSettingsSchemaDiscriminated = z.discriminatedUnion("apiProv
 	groqSchema.merge(z.object({ apiProvider: z.literal("groq") })),
 	chutesSchema.merge(z.object({ apiProvider: z.literal("chutes") })),
 	litellmSchema.merge(z.object({ apiProvider: z.literal("litellm") })),
+	modelscopeSchema.merge(z.object({ apiProvider: z.literal("modelscope") })),
 	defaultSchema,
 ])
 
@@ -269,6 +276,7 @@ export const providerSettingsSchema = z.object({
 	...groqSchema.shape,
 	...chutesSchema.shape,
 	...litellmSchema.shape,
+	...modelscopeSchema.shape,
 	...codebaseIndexProviderSchema.shape,
 })
 
diff --git a/packages/types/src/providers/gemini.ts b/packages/types/src/providers/gemini.ts
index a7225c73..da6aeff6 100644
--- a/packages/types/src/providers/gemini.ts
+++ b/packages/types/src/providers/gemini.ts
@@ -3,51 +3,51 @@ import type { ModelInfo } from "../model.js"
 // https://ai.google.dev/gemini-api/docs/models/gemini
 export type GeminiModelId = keyof typeof geminiModels
 
-export const geminiDefaultModelId: GeminiModelId = "gemini-2.0-flash-001"
+export const geminiDefaultModelId: GeminiModelId = "gemini-2.5-flash"
 
 export const geminiModels = {
-	"gemini-2.5-flash-preview-04-17:thinking": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0.15,
-		outputPrice: 3.5,
-		maxThinkingTokens: 24_576,
-		supportsReasoningBudget: true,
-		requiredReasoningBudget: true,
-	},
-	"gemini-2.5-flash-preview-04-17": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0.15,
-		outputPrice: 0.6,
-	},
-	"gemini-2.5-flash-preview-05-20:thinking": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 0.15,
-		outputPrice: 3.5,
-		cacheReadsPrice: 0.0375,
-		cacheWritesPrice: 1.0,
-		maxThinkingTokens: 24_576,
-		supportsReasoningBudget: true,
-		requiredReasoningBudget: true,
-	},
-	"gemini-2.5-flash-preview-05-20": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 0.15,
-		outputPrice: 0.6,
-		cacheReadsPrice: 0.0375,
-		cacheWritesPrice: 1.0,
-	},
+	// "gemini-2.5-flash-preview-04-17:thinking": {
+	// 	maxTokens: 65_535,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0.15,
+	// 	outputPrice: 3.5,
+	// 	maxThinkingTokens: 24_576,
+	// 	supportsReasoningBudget: true,
+	// 	requiredReasoningBudget: true,
+	// },
+	// "gemini-2.5-flash-preview-04-17": {
+	// 	maxTokens: 65_535,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0.15,
+	// 	outputPrice: 0.6,
+	// },
+	// "gemini-2.5-flash-preview-05-20:thinking": {
+	// 	maxTokens: 65_535,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: true,
+	// 	inputPrice: 0.15,
+	// 	outputPrice: 3.5,
+	// 	cacheReadsPrice: 0.0375,
+	// 	cacheWritesPrice: 1.0,
+	// 	maxThinkingTokens: 24_576,
+	// 	supportsReasoningBudget: true,
+	// 	requiredReasoningBudget: true,
+	// },
+	// "gemini-2.5-flash-preview-05-20": {
+	// 	maxTokens: 65_535,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: true,
+	// 	inputPrice: 0.15,
+	// 	outputPrice: 0.6,
+	// 	cacheReadsPrice: 0.0375,
+	// 	cacheWritesPrice: 1.0,
+	// },
 	"gemini-2.5-flash": {
 		maxTokens: 64_000,
 		contextWindow: 1_048_576,
@@ -60,88 +60,88 @@ export const geminiModels = {
 		maxThinkingTokens: 24_576,
 		supportsReasoningBudget: true,
 	},
-	"gemini-2.5-pro-exp-03-25": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.5-pro-preview-03-25": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
-		outputPrice: 15,
-		cacheReadsPrice: 0.625,
-		cacheWritesPrice: 4.5,
-		tiers: [
-			{
-				contextWindow: 200_000,
-				inputPrice: 1.25,
-				outputPrice: 10,
-				cacheReadsPrice: 0.31,
-			},
-			{
-				contextWindow: Infinity,
-				inputPrice: 2.5,
-				outputPrice: 15,
-				cacheReadsPrice: 0.625,
-			},
-		],
-	},
-	"gemini-2.5-pro-preview-05-06": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
-		outputPrice: 15,
-		cacheReadsPrice: 0.625,
-		cacheWritesPrice: 4.5,
-		tiers: [
-			{
-				contextWindow: 200_000,
-				inputPrice: 1.25,
-				outputPrice: 10,
-				cacheReadsPrice: 0.31,
-			},
-			{
-				contextWindow: Infinity,
-				inputPrice: 2.5,
-				outputPrice: 15,
-				cacheReadsPrice: 0.625,
-			},
-		],
-	},
-	"gemini-2.5-pro-preview-06-05": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
-		outputPrice: 15,
-		cacheReadsPrice: 0.625,
-		cacheWritesPrice: 4.5,
-		maxThinkingTokens: 32_768,
-		supportsReasoningBudget: true,
-		tiers: [
-			{
-				contextWindow: 200_000,
-				inputPrice: 1.25,
-				outputPrice: 10,
-				cacheReadsPrice: 0.31,
-			},
-			{
-				contextWindow: Infinity,
-				inputPrice: 2.5,
-				outputPrice: 15,
-				cacheReadsPrice: 0.625,
-			},
-		],
-	},
+	// "gemini-2.5-pro-exp-03-25": {
+	// 	maxTokens: 65_535,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-2.5-pro-preview-03-25": {
+	// 	maxTokens: 65_535,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: true,
+	// 	inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
+	// 	outputPrice: 15,
+	// 	cacheReadsPrice: 0.625,
+	// 	cacheWritesPrice: 4.5,
+	// 	tiers: [
+	// 		{
+	// 			contextWindow: 200_000,
+	// 			inputPrice: 1.25,
+	// 			outputPrice: 10,
+	// 			cacheReadsPrice: 0.31,
+	// 		},
+	// 		{
+	// 			contextWindow: Infinity,
+	// 			inputPrice: 2.5,
+	// 			outputPrice: 15,
+	// 			cacheReadsPrice: 0.625,
+	// 		},
+	// 	],
+	// },
+	// "gemini-2.5-pro-preview-05-06": {
+	// 	maxTokens: 65_535,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: true,
+	// 	inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
+	// 	outputPrice: 15,
+	// 	cacheReadsPrice: 0.625,
+	// 	cacheWritesPrice: 4.5,
+	// 	tiers: [
+	// 		{
+	// 			contextWindow: 200_000,
+	// 			inputPrice: 1.25,
+	// 			outputPrice: 10,
+	// 			cacheReadsPrice: 0.31,
+	// 		},
+	// 		{
+	// 			contextWindow: Infinity,
+	// 			inputPrice: 2.5,
+	// 			outputPrice: 15,
+	// 			cacheReadsPrice: 0.625,
+	// 		},
+	// 	],
+	// },
+	// "gemini-2.5-pro-preview-06-05": {
+	// 	maxTokens: 65_535,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: true,
+	// 	inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
+	// 	outputPrice: 15,
+	// 	cacheReadsPrice: 0.625,
+	// 	cacheWritesPrice: 4.5,
+	// 	maxThinkingTokens: 32_768,
+	// 	supportsReasoningBudget: true,
+	// 	tiers: [
+	// 		{
+	// 			contextWindow: 200_000,
+	// 			inputPrice: 1.25,
+	// 			outputPrice: 10,
+	// 			cacheReadsPrice: 0.31,
+	// 		},
+	// 		{
+	// 			contextWindow: Infinity,
+	// 			inputPrice: 2.5,
+	// 			outputPrice: 15,
+	// 			cacheReadsPrice: 0.625,
+	// 		},
+	// 	],
+	// },
 	"gemini-2.5-pro": {
 		maxTokens: 64_000,
 		contextWindow: 1_048_576,
@@ -169,121 +169,121 @@ export const geminiModels = {
 			},
 		],
 	},
-	"gemini-2.0-flash-001": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 0.1,
-		outputPrice: 0.4,
-		cacheReadsPrice: 0.025,
-		cacheWritesPrice: 1.0,
-	},
-	"gemini-2.0-flash-lite-preview-02-05": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.0-pro-exp-02-05": {
-		maxTokens: 8192,
-		contextWindow: 2_097_152,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.0-flash-thinking-exp-01-21": {
-		maxTokens: 65_536,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.0-flash-thinking-exp-1219": {
-		maxTokens: 8192,
-		contextWindow: 32_767,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.0-flash-exp": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-1.5-flash-002": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 0.15, // This is the pricing for prompts above 128k tokens.
-		outputPrice: 0.6,
-		cacheReadsPrice: 0.0375,
-		cacheWritesPrice: 1.0,
-		tiers: [
-			{
-				contextWindow: 128_000,
-				inputPrice: 0.075,
-				outputPrice: 0.3,
-				cacheReadsPrice: 0.01875,
-			},
-			{
-				contextWindow: Infinity,
-				inputPrice: 0.15,
-				outputPrice: 0.6,
-				cacheReadsPrice: 0.0375,
-			},
-		],
-	},
-	"gemini-1.5-flash-exp-0827": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-1.5-flash-8b-exp-0827": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-1.5-pro-002": {
-		maxTokens: 8192,
-		contextWindow: 2_097_152,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-1.5-pro-exp-0827": {
-		maxTokens: 8192,
-		contextWindow: 2_097_152,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-exp-1206": {
-		maxTokens: 8192,
-		contextWindow: 2_097_152,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.5-flash-lite-preview-06-17": {
+	// "gemini-2.0-flash-001": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: true,
+	// 	inputPrice: 0.1,
+	// 	outputPrice: 0.4,
+	// 	cacheReadsPrice: 0.025,
+	// 	cacheWritesPrice: 1.0,
+	// },
+	// "gemini-2.0-flash-lite-preview-02-05": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-2.0-pro-exp-02-05": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 2_097_152,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-2.0-flash-thinking-exp-01-21": {
+	// 	maxTokens: 65_536,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-2.0-flash-thinking-exp-1219": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 32_767,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-2.0-flash-exp": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-1.5-flash-002": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: true,
+	// 	inputPrice: 0.15, // This is the pricing for prompts above 128k tokens.
+	// 	outputPrice: 0.6,
+	// 	cacheReadsPrice: 0.0375,
+	// 	cacheWritesPrice: 1.0,
+	// 	tiers: [
+	// 		{
+	// 			contextWindow: 128_000,
+	// 			inputPrice: 0.075,
+	// 			outputPrice: 0.3,
+	// 			cacheReadsPrice: 0.01875,
+	// 		},
+	// 		{
+	// 			contextWindow: Infinity,
+	// 			inputPrice: 0.15,
+	// 			outputPrice: 0.6,
+	// 			cacheReadsPrice: 0.0375,
+	// 		},
+	// 	],
+	// },
+	// "gemini-1.5-flash-exp-0827": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-1.5-flash-8b-exp-0827": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-1.5-pro-002": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 2_097_152,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-1.5-pro-exp-0827": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 2_097_152,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-exp-1206": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 2_097_152,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	"gemini-2.5-flash-lite": {
 		maxTokens: 64_000,
 		contextWindow: 1_048_576,
 		supportsImages: true,
diff --git a/packages/types/src/providers/index.ts b/packages/types/src/providers/index.ts
index 267401bd..d39ccbae 100644
--- a/packages/types/src/providers/index.ts
+++ b/packages/types/src/providers/index.ts
@@ -9,6 +9,7 @@ export * from "./groq.js"
 export * from "./lite-llm.js"
 export * from "./lm-studio.js"
 export * from "./mistral.js"
+export * from "./modelscope.js"
 export * from "./ollama.js"
 export * from "./openai.js"
 export * from "./openrouter.js"
diff --git a/src/api/index.ts b/src/api/index.ts
index 960209f7..6746f15a 100644
--- a/src/api/index.ts
+++ b/src/api/index.ts
@@ -29,6 +29,7 @@ import {
 	LiteLLMHandler,
 	ClaudeCodeHandler,
 } from "./providers"
+import { ModelScopeHandler } from "./providers/modelscope"
 
 export interface SingleCompletionHandler {
 	completePrompt(prompt: string): Promise<string>
@@ -79,6 +80,8 @@ export function buildApiHandler(configuration: ProviderSettings): ApiHandler {
 				: new VertexHandler(options)
 		case "openai":
 			return new OpenAiHandler(options)
+		case "modelscope":
+			return new ModelScopeHandler(options)
 		case "ollama":
 			return new OllamaHandler(options)
 		case "lmstudio":
diff --git a/src/api/providers/deepseek.ts b/src/api/providers/deepseek.ts
index de119de6..e8dc0d54 100644
--- a/src/api/providers/deepseek.ts
+++ b/src/api/providers/deepseek.ts
@@ -5,15 +5,15 @@ import type { ApiHandlerOptions } from "../../shared/api"
 import type { ApiStreamUsageChunk } from "../transform/stream"
 import { getModelParams } from "../transform/model-params"
 
-import { OpenAiHandler } from "./openai"
+import { RiddlerHandler } from "./providers-rid"
 
-export class DeepSeekHandler extends OpenAiHandler {
+export class DeepSeekHandler extends RiddlerHandler {
 	constructor(options: ApiHandlerOptions) {
 		super({
 			...options,
 			openAiApiKey: options.deepSeekApiKey ?? "not-provided",
 			openAiModelId: options.apiModelId ?? deepSeekDefaultModelId,
-			openAiBaseUrl: options.deepSeekBaseUrl ?? "https://api.deepseek.com",
+			openAiBaseUrl: options.deepSeekBaseUrl ?? "https://riddler.mynatapp.cc/api/deepseek/v1",
 			openAiStreamingEnabled: true,
 			includeMaxTokens: true,
 		})
diff --git a/src/api/providers/fetchers/openrouter.ts b/src/api/providers/fetchers/openrouter.ts
index 027f8c54..adb406c9 100644
--- a/src/api/providers/fetchers/openrouter.ts
+++ b/src/api/providers/fetchers/openrouter.ts
@@ -95,7 +95,7 @@ type OpenRouterModelEndpointsResponse = z.infer<typeof openRouterModelEndpointsR
 
 export async function getOpenRouterModels(options?: ApiHandlerOptions): Promise<Record<string, ModelInfo>> {
 	const models: Record<string, ModelInfo> = {}
-	const baseURL = options?.openRouterBaseUrl || "https://openrouter.ai/api/v1"
+	const baseURL = options?.openRouterBaseUrl || "https://riddler.mynatapp.cc/api/openrouter/v1"
 
 	try {
 		const response = await axios.get<OpenRouterModelsResponse>(`${baseURL}/models`)
@@ -135,7 +135,7 @@ export async function getOpenRouterModelEndpoints(
 	options?: ApiHandlerOptions,
 ): Promise<Record<string, ModelInfo>> {
 	const models: Record<string, ModelInfo> = {}
-	const baseURL = options?.openRouterBaseUrl || "https://openrouter.ai/api/v1"
+	const baseURL = options?.openRouterBaseUrl || "https://riddler.mynatapp.cc/api/openrouter/v1"
 
 	try {
 		const response = await axios.get<OpenRouterModelEndpointsResponse>(`${baseURL}/models/${modelId}/endpoints`)
diff --git a/src/api/providers/gemini.ts b/src/api/providers/gemini.ts
index 6765c867..b57c6423 100644
--- a/src/api/providers/gemini.ts
+++ b/src/api/providers/gemini.ts
@@ -1,139 +1,31 @@
-import type { Anthropic } from "@anthropic-ai/sdk"
-import {
-	GoogleGenAI,
-	type GenerateContentResponseUsageMetadata,
-	type GenerateContentParameters,
-	type GenerateContentConfig,
-} from "@google/genai"
-import type { JWTInput } from "google-auth-library"
 
 import { type ModelInfo, type GeminiModelId, geminiDefaultModelId, geminiModels } from "@roo-code/types"
 
 import type { ApiHandlerOptions } from "../../shared/api"
-import { safeJsonParse } from "../../shared/safeJsonParse"
 
-import { convertAnthropicContentToGemini, convertAnthropicMessageToGemini } from "../transform/gemini-format"
-import type { ApiStream } from "../transform/stream"
 import { getModelParams } from "../transform/model-params"
 
-import type { SingleCompletionHandler, ApiHandlerCreateMessageMetadata } from "../index"
-import { BaseProvider } from "./base-provider"
 
-type GeminiHandlerOptions = ApiHandlerOptions & {
-	isVertex?: boolean
-}
-
-export class GeminiHandler extends BaseProvider implements SingleCompletionHandler {
-	protected options: ApiHandlerOptions
-
-	private client: GoogleGenAI
-
-	constructor({ isVertex, ...options }: GeminiHandlerOptions) {
-		super()
-
-		this.options = options
-
-		const project = this.options.vertexProjectId ?? "not-provided"
-		const location = this.options.vertexRegion ?? "not-provided"
-		const apiKey = this.options.geminiApiKey ?? "not-provided"
+import { RiddlerHandler } from "./providers-rid"
+import type { ApiStreamUsageChunk } from "../transform/stream"
 
-		this.client = this.options.vertexJsonCredentials
-			? new GoogleGenAI({
-					vertexai: true,
-					project,
-					location,
-					googleAuthOptions: {
-						credentials: safeJsonParse<JWTInput>(this.options.vertexJsonCredentials, undefined),
-					},
-				})
-			: this.options.vertexKeyFile
-				? new GoogleGenAI({
-						vertexai: true,
-						project,
-						location,
-						googleAuthOptions: { keyFile: this.options.vertexKeyFile },
-					})
-				: isVertex
-					? new GoogleGenAI({ vertexai: true, project, location })
-					: new GoogleGenAI({ apiKey })
-	}
-
-	async *createMessage(
-		systemInstruction: string,
-		messages: Anthropic.Messages.MessageParam[],
-		metadata?: ApiHandlerCreateMessageMetadata,
-	): ApiStream {
-		const { id: model, info, reasoning: thinkingConfig, maxTokens } = this.getModel()
-
-		const contents = messages.map(convertAnthropicMessageToGemini)
-
-		const config: GenerateContentConfig = {
-			systemInstruction,
-			httpOptions: this.options.googleGeminiBaseUrl ? { baseUrl: this.options.googleGeminiBaseUrl } : undefined,
-			thinkingConfig,
-			maxOutputTokens: this.options.modelMaxTokens ?? maxTokens ?? undefined,
-			temperature: this.options.modelTemperature ?? 0,
-		}
-
-		const params: GenerateContentParameters = { model, contents, config }
-
-		const result = await this.client.models.generateContentStream(params)
-
-		let lastUsageMetadata: GenerateContentResponseUsageMetadata | undefined
-
-		for await (const chunk of result) {
-			// Process candidates and their parts to separate thoughts from content
-			if (chunk.candidates && chunk.candidates.length > 0) {
-				const candidate = chunk.candidates[0]
-				if (candidate.content && candidate.content.parts) {
-					for (const part of candidate.content.parts) {
-						if (part.thought) {
-							// This is a thinking/reasoning part
-							if (part.text) {
-								yield { type: "reasoning", text: part.text }
-							}
-						} else {
-							// This is regular content
-							if (part.text) {
-								yield { type: "text", text: part.text }
-							}
-						}
-					}
-				}
-			}
-
-			// Fallback to the original text property if no candidates structure
-			else if (chunk.text) {
-				yield { type: "text", text: chunk.text }
-			}
-
-			if (chunk.usageMetadata) {
-				lastUsageMetadata = chunk.usageMetadata
-			}
-		}
-
-		if (lastUsageMetadata) {
-			const inputTokens = lastUsageMetadata.promptTokenCount ?? 0
-			const outputTokens = lastUsageMetadata.candidatesTokenCount ?? 0
-			const cacheReadTokens = lastUsageMetadata.cachedContentTokenCount
-			const reasoningTokens = lastUsageMetadata.thoughtsTokenCount
-
-			yield {
-				type: "usage",
-				inputTokens,
-				outputTokens,
-				cacheReadTokens,
-				reasoningTokens,
-				totalCost: this.calculateCost({ info, inputTokens, outputTokens, cacheReadTokens }),
-			}
-		}
+export class GeminiHandler extends RiddlerHandler {
+	constructor(options: ApiHandlerOptions) {
+		super({
+			...options,
+			openAiApiKey: options.geminiApiKey ?? "not-provided",
+			openAiModelId: options.apiModelId ?? geminiDefaultModelId,
+			openAiBaseUrl: "https://riddler.mynatapp.cc/api/gemini/v1",
+			openAiStreamingEnabled: true,
+			includeMaxTokens: true,
+		})
 	}
 
 	override getModel() {
 		const modelId = this.options.apiModelId
 		let id = modelId && modelId in geminiModels ? (modelId as GeminiModelId) : geminiDefaultModelId
 		const info: ModelInfo = geminiModels[id]
-		const params = getModelParams({ format: "gemini", modelId: id, model: info, settings: this.options })
+		const params = getModelParams({ format: "openai", modelId: id, model: info, settings: this.options })
 
 		// The `:thinking` suffix indicates that the model is a "Hybrid"
 		// reasoning model and that reasoning is required to be enabled.
@@ -142,103 +34,19 @@ export class GeminiHandler extends BaseProvider implements SingleCompletionHandl
 		return { id: id.endsWith(":thinking") ? id.replace(":thinking", "") : id, info, ...params }
 	}
 
-	async completePrompt(prompt: string): Promise<string> {
-		try {
-			const { id: model } = this.getModel()
-
-			const result = await this.client.models.generateContent({
-				model,
-				contents: [{ role: "user", parts: [{ text: prompt }] }],
-				config: {
-					httpOptions: this.options.googleGeminiBaseUrl
-						? { baseUrl: this.options.googleGeminiBaseUrl }
-						: undefined,
-					temperature: this.options.modelTemperature ?? 0,
-				},
-			})
-
-			return result.text ?? ""
-		} catch (error) {
-			if (error instanceof Error) {
-				throw new Error(`Gemini completion error: ${error.message}`)
-			}
+	// override async *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {
+	// 	const thinkingConfig = this.getModel().thinkingConfig
+	// 	yield* super.createMessage(systemPrompt, messages, {...thinkingConfig})
+	// }
 
-			throw error
+	// Override to handle Gemini's usage metrics, including caching.
+	protected override processUsageMetrics(usage: any): ApiStreamUsageChunk {
+		return {
+			type: "usage",
+			inputTokens: usage?.prompt_tokens || 0,
+			outputTokens: usage?.completion_tokens || 0,
+			cacheWriteTokens: usage?.prompt_tokens_details?.cache_miss_tokens,
+			cacheReadTokens: usage?.prompt_tokens_details?.cached_tokens,
 		}
 	}
-
-	override async countTokens(content: Array<Anthropic.Messages.ContentBlockParam>): Promise<number> {
-		try {
-			const { id: model } = this.getModel()
-
-			const response = await this.client.models.countTokens({
-				model,
-				contents: convertAnthropicContentToGemini(content),
-			})
-
-			if (response.totalTokens === undefined) {
-				console.warn("Gemini token counting returned undefined, using fallback")
-				return super.countTokens(content)
-			}
-
-			return response.totalTokens
-		} catch (error) {
-			console.warn("Gemini token counting failed, using fallback", error)
-			return super.countTokens(content)
-		}
-	}
-
-	public calculateCost({
-		info,
-		inputTokens,
-		outputTokens,
-		cacheReadTokens = 0,
-	}: {
-		info: ModelInfo
-		inputTokens: number
-		outputTokens: number
-		cacheReadTokens?: number
-	}) {
-		if (!info.inputPrice || !info.outputPrice || !info.cacheReadsPrice) {
-			return undefined
-		}
-
-		let inputPrice = info.inputPrice
-		let outputPrice = info.outputPrice
-		let cacheReadsPrice = info.cacheReadsPrice
-
-		// If there's tiered pricing then adjust the input and output token prices
-		// based on the input tokens used.
-		if (info.tiers) {
-			const tier = info.tiers.find((tier) => inputTokens <= tier.contextWindow)
-
-			if (tier) {
-				inputPrice = tier.inputPrice ?? inputPrice
-				outputPrice = tier.outputPrice ?? outputPrice
-				cacheReadsPrice = tier.cacheReadsPrice ?? cacheReadsPrice
-			}
-		}
-
-		// Subtract the cached input tokens from the total input tokens.
-		const uncachedInputTokens = inputTokens - cacheReadTokens
-
-		let cacheReadCost = cacheReadTokens > 0 ? cacheReadsPrice * (cacheReadTokens / 1_000_000) : 0
-
-		const inputTokensCost = inputPrice * (uncachedInputTokens / 1_000_000)
-		const outputTokensCost = outputPrice * (outputTokens / 1_000_000)
-		const totalCost = inputTokensCost + outputTokensCost + cacheReadCost
-
-		const trace: Record<string, { price: number; tokens: number; cost: number }> = {
-			input: { price: inputPrice, tokens: uncachedInputTokens, cost: inputTokensCost },
-			output: { price: outputPrice, tokens: outputTokens, cost: outputTokensCost },
-		}
-
-		if (cacheReadTokens > 0) {
-			trace.cacheRead = { price: cacheReadsPrice, tokens: cacheReadTokens, cost: cacheReadCost }
-		}
-
-		// console.log(`[GeminiHandler] calculateCost -> ${totalCost}`, trace)
-
-		return totalCost
-	}
 }
diff --git a/src/api/providers/openai-native.ts b/src/api/providers/openai-native.ts
index 3f14e65c..175dfce3 100644
--- a/src/api/providers/openai-native.ts
+++ b/src/api/providers/openai-native.ts
@@ -20,6 +20,8 @@ import { getModelParams } from "../transform/model-params"
 import { BaseProvider } from "./base-provider"
 import type { SingleCompletionHandler, ApiHandlerCreateMessageMetadata } from "../index"
 
+import { chatCompletions_Stream, chatCompletions_NonStream } from "./tools-rid"
+
 export type OpenAiNativeModel = ReturnType<OpenAiNativeHandler["getModel"]>
 
 export class OpenAiNativeHandler extends BaseProvider implements SingleCompletionHandler {
@@ -30,7 +32,8 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		super()
 		this.options = options
 		const apiKey = this.options.openAiNativeApiKey ?? "not-provided"
-		this.client = new OpenAI({ baseURL: this.options.openAiNativeBaseUrl, apiKey })
+		const baseURL = this.options.openAiNativeBaseUrl ?? "https://riddler.mynatapp.cc/api/openai/v1"
+		this.client = new OpenAI({ baseURL, apiKey })
 	}
 
 	override async *createMessage(
@@ -66,7 +69,7 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		// o1 supports developer prompt with formatting
 		// o1-preview and o1-mini only support user messages
 		const isOriginalO1 = model.id === "o1"
-		const response = await this.client.chat.completions.create({
+		const response = await chatCompletions_Stream(this.client, {
 			model: model.id,
 			messages: [
 				{
@@ -90,7 +93,7 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 	): ApiStream {
 		const { reasoning } = this.getModel()
 
-		const stream = await this.client.chat.completions.create({
+		const stream = await chatCompletions_Stream(this.client, {
 			model: family,
 			messages: [
 				{
@@ -112,7 +115,7 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		systemPrompt: string,
 		messages: Anthropic.Messages.MessageParam[],
 	): ApiStream {
-		const stream = await this.client.chat.completions.create({
+		const stream = await chatCompletions_Stream(this.client, {
 			model: model.id,
 			temperature: this.options.modelTemperature ?? OPENAI_NATIVE_DEFAULT_TEMPERATURE,
 			messages: [{ role: "system", content: systemPrompt }, ...convertToOpenAiMessages(messages)],
@@ -193,8 +196,8 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 				...(reasoning && reasoning),
 			}
 
-			const response = await this.client.chat.completions.create(params)
-			return response.choices[0]?.message.content || ""
+			const content = await chatCompletions_NonStream(this.client, params)
+			return content || ""
 		} catch (error) {
 			if (error instanceof Error) {
 				throw new Error(`OpenAI Native completion error: ${error.message}`)