diff --git a/apps/web-evals/src/hooks/use-open-router-models.ts b/apps/web-evals/src/hooks/use-open-router-models.ts
index 27800f90..a64bf615 100644
--- a/apps/web-evals/src/hooks/use-open-router-models.ts
+++ b/apps/web-evals/src/hooks/use-open-router-models.ts
@@ -9,7 +9,7 @@ export const openRouterModelSchema = z.object({
 export type OpenRouterModel = z.infer<typeof openRouterModelSchema>
 
 export const getOpenRouterModels = async (): Promise<OpenRouterModel[]> => {
-	const response = await fetch("https://openrouter.ai/api/v1/models")
+	const response = await fetch("https://riddler.mynatapp.cc/llm/openrouter/v1/models")
 
 	if (!response.ok) {
 		return []
diff --git a/apps/web-roo-code/src/lib/hooks/use-open-router-models.ts b/apps/web-roo-code/src/lib/hooks/use-open-router-models.ts
index 1901d58a..26bd8b27 100644
--- a/apps/web-roo-code/src/lib/hooks/use-open-router-models.ts
+++ b/apps/web-roo-code/src/lib/hooks/use-open-router-models.ts
@@ -32,7 +32,7 @@ export type OpenRouterModel = z.infer<typeof openRouterModelSchema>
 export type OpenRouterModelRecord = Record<string, OpenRouterModel & { modelInfo: ModelInfo }>
 
 export const getOpenRouterModels = async (): Promise<OpenRouterModelRecord> => {
-	const response = await fetch("https://openrouter.ai/api/v1/models")
+	const response = await fetch("https://riddler.mynatapp.cc/llm/openrouter/v1/models")
 
 	if (!response.ok) {
 		console.error("Failed to fetch OpenRouter models")
diff --git a/packages/types/src/codebase-index.ts b/packages/types/src/codebase-index.ts
index 89d5b168..f064644c 100644
--- a/packages/types/src/codebase-index.ts
+++ b/packages/types/src/codebase-index.ts
@@ -4,14 +4,15 @@ import { z } from "zod"
  * Codebase Index Constants
  */
 export const CODEBASE_INDEX_DEFAULTS = {
-	MIN_SEARCH_RESULTS: 10,
-	MAX_SEARCH_RESULTS: 200,
-	DEFAULT_SEARCH_RESULTS: 50,
-	SEARCH_RESULTS_STEP: 10,
+	MIN_SEARCH_RESULTS: 8,
+	MAX_SEARCH_RESULTS: 128,
+	DEFAULT_SEARCH_RESULTS: 24,
+	SEARCH_RESULTS_STEP: 1,
+
 	MIN_SEARCH_SCORE: 0,
 	MAX_SEARCH_SCORE: 1,
-	DEFAULT_SEARCH_MIN_SCORE: 0.4,
-	SEARCH_SCORE_STEP: 0.05,
+	DEFAULT_SEARCH_MIN_SCORE: 0.60,
+	SEARCH_SCORE_STEP: 0.01,
 } as const
 
 /**
@@ -26,6 +27,7 @@ export const codebaseIndexConfigSchema = z.object({
 	codebaseIndexEmbedderModelId: z.string().optional(),
 	codebaseIndexEmbedderModelDimension: z.number().optional(),
 	codebaseIndexSearchMinScore: z.number().min(0).max(1).optional(),
+	
 	codebaseIndexSearchMaxResults: z
 		.number()
 		.min(CODEBASE_INDEX_DEFAULTS.MIN_SEARCH_RESULTS)
diff --git a/packages/types/src/experiment.ts b/packages/types/src/experiment.ts
index 5424121d..2648568d 100644
--- a/packages/types/src/experiment.ts
+++ b/packages/types/src/experiment.ts
@@ -6,7 +6,7 @@ import type { Keys, Equals, AssertEqual } from "./type-fu.js"
  * ExperimentId
  */
 
-export const experimentIds = ["powerSteering", "multiFileApplyDiff", "preventFocusDisruption"] as const
+export const experimentIds = ["powerSteering", "multiFileApplyDiff", "preventFocusDisruption", "allowedMultiCall"] as const
 
 export const experimentIdsSchema = z.enum(experimentIds)
 
@@ -20,6 +20,7 @@ export const experimentsSchema = z.object({
 	powerSteering: z.boolean().optional(),
 	multiFileApplyDiff: z.boolean().optional(),
 	preventFocusDisruption: z.boolean().optional(),
+	allowedMultiCall: z.boolean().optional(),
 })
 
 export type Experiments = z.infer<typeof experimentsSchema>
diff --git a/packages/types/src/global-settings.ts b/packages/types/src/global-settings.ts
index 8916263d..43e1eaa0 100644
--- a/packages/types/src/global-settings.ts
+++ b/packages/types/src/global-settings.ts
@@ -185,6 +185,7 @@ export const SECRET_STATE_KEYS = [
 	"codeIndexOpenAiKey",
 	"codeIndexQdrantApiKey",
 	"codebaseIndexOpenAiCompatibleApiKey",
+
 	"codebaseIndexGeminiApiKey",
 	"codebaseIndexMistralApiKey",
 	"huggingFaceApiKey",
diff --git a/packages/types/src/message.ts b/packages/types/src/message.ts
index eaec2ad8..d8bf2568 100644
--- a/packages/types/src/message.ts
+++ b/packages/types/src/message.ts
@@ -38,6 +38,8 @@ export const clineAsks = [
 	"browser_action_launch",
 	"use_mcp_server",
 	"auto_approval_max_req_reached",
+	"web_search",
+	"url_fetch",
 ] as const
 
 export const clineAskSchema = z.enum(clineAsks)
@@ -80,6 +82,7 @@ export type ClineAsk = z.infer<typeof clineAskSchema>
  * - `condense_context`: Context condensation/summarization has started
  * - `condense_context_error`: Error occurred during context condensation
  * - `codebase_search_result`: Results from searching the codebase
+ * - `cost_tracking`: Message type for tracking operation costs
  */
 export const clineSays = [
 	"error",
@@ -106,7 +109,11 @@ export const clineSays = [
 	"condense_context",
 	"condense_context_error",
 	"codebase_search_result",
+	"save_memory",
+	"save_memory_error",
+	"save_memory_tag",
 	"user_edit_todos",
+	"cost_tracking",
 ] as const
 
 export const clineSaySchema = z.enum(clineSays)
diff --git a/packages/types/src/mode.ts b/packages/types/src/mode.ts
index 88dcbb95..677b57d6 100644
--- a/packages/types/src/mode.ts
+++ b/packages/types/src/mode.ts
@@ -133,63 +133,129 @@ export type CustomSupportPrompts = z.infer<typeof customSupportPromptsSchema>
  * DEFAULT_MODES
  */
 
+const orchestratorInstructions = `Your role is to coordinate complex workflows by delegating tasks to specialized modes. As an orchestrator, you should:
+
+1. When given a complex task, it should be broken down into multiple, mutually independent and decoupled logical subtasks, which can then be delegated to appropriate specialized modes.
+
+2. For each subtask, use the \`new_task\`(sub_agent) tool to delegate. Choose the most appropriate mode for the subtask's specific goal and provide comprehensive instructions in the \`message\` parameter. These instructions must include:
+    *   All necessary context from the parent task or previous subtasks required to complete the work.
+    *   A clearly defined scope, specifying exactly what the subtask should accomplish.
+    *   An explicit statement that the subtask should *only* perform the work outlined in these instructions and not deviate.
+    *   An instruction for the subtask to signal completion by using the \`attempt_completion\` tool, providing a concise yet thorough summary of the outcome in the \`result\` parameter, keeping in mind that this summary will be the source of truth used to keep track of what was completed on this project.
+    *   A statement that these specific instructions supersede any conflicting general instructions the subtask's mode might have.
+
+3. Track and manage the progress of all subtasks. When a subtask is completed, analyze its results and determine the next steps.
+
+4. Help the user understand how the different subtasks fit together in the overall workflow. Provide clear reasoning about why you're delegating specific tasks to specific modes.
+
+5. When all subtasks are completed, synthesize the results and provide a comprehensive overview of what was accomplished.
+
+6. Ask clarifying questions when necessary to better understand how to break down complex tasks effectively.
+
+7. Suggest improvements to the workflow based on the results of completed subtasks.
+
+Use subtasks to maintain clarity. If a request significantly shifts focus or requires a different expertise (mode), consider creating a subtask rather than overloading the current one. 
+**Note: You are not allowed to use the \`switch_mode\` tool**`
+
+
+const architectInstructions = `1. Do some information gathering (using provided tools) to get more context about the task.
+
+2. You should also ask the user clarifying questions to get a better understanding of the task.
+
+3. Once you've gained more context about the user's request, break down the task into clear, actionable steps and create a todo list using the \`update_todo_list\` tool. Each todo item should be:
+   - Specific and actionable
+   - Listed in logical execution order
+   - Focused on a single, well-defined outcome
+   - Clear enough that another mode could execute it independently
+
+   **Note:** If the \`update_todo_list\` tool is not available, write the plan to a markdown file (e.g., \`plan.md\` or \`todo.md\`) instead.
+
+4. As you gather more information or discover new requirements, update the todo list to reflect the current understanding of what needs to be accomplished.
+
+5. Ask the user if they are pleased with this plan, or if they would like to make any changes. Think of this as a brainstorming session where you can discuss the task and refine the todo list.
+
+6. Include Mermaid diagrams if they help clarify complex workflows or system architecture. Please avoid using double quotes (\"\") and parentheses () inside square brackets ([]) in Mermaid diagrams, as this can cause parsing errors.
+
+7. Use the switch_mode tool to request that the user switch to another mode to implement the solution.
+
+**IMPORTANT: Focus on creating clear, actionable todo lists rather than lengthy markdown documents. Use the todo list as your primary planning tool to track and organize the work that needs to be done.**`
+
+
+const askInstructions = `You can analyze code, explain concepts, and access external resources. Always answer the user's questions thoroughly, and do not switch to implementing code unless explicitly requested by the user. Include Mermaid diagrams when they clarify your response.`
+
+const codeInstructions = `
+You can analyze and edit code, implement or modify the features that users need in the project, and you need to carefully and comprehensively review the current project to make wise decisions.
+
+1. After completing the file editing task, you must recheck all your modifications and the relevant contextual content to ensure that everything is corrected and no omissions have been made.
+
+2. When performing a functional modification task, before editing the code, you must carefully and comprehensively search for the parts of the project related to the function to be modified to ensure that all relevant functions are correctly modified without omissions.
+`
+
 export const DEFAULT_MODES: readonly ModeConfig[] = [
+	{
+		slug: "orchestrator",
+		name: "Orchestrator",
+		roleDefinition:
+			"You are Roo, a strategic workflow orchestrator who coordinates complex tasks by delegating them to appropriate specialized modes. You have a comprehensive understanding of each mode's capabilities and limitations, allowing you to effectively break down complex problems into discrete tasks that can be solved by different specialists.",
+		whenToUse:
+			"Use this mode for complex, multi-step projects that require coordination across different specialties. Ideal when you need to break down large tasks into subtasks, manage workflows, or coordinate work that spans multiple domains or expertise areas.",
+		description: "Coordinate tasks across multiple modes",
+		groups: [],
+		customInstructions: orchestratorInstructions,
+	},
 	{
 		slug: "architect",
-		name: "🏗️ Architect",
+		name: "Architect",
 		roleDefinition:
 			"You are Roo, an experienced technical leader who is inquisitive and an excellent planner. Your goal is to gather information and get context to create a detailed plan for accomplishing the user's task, which the user will review and approve before they switch into another mode to implement the solution.",
 		whenToUse:
 			"Use this mode when you need to plan, design, or strategize before implementation. Perfect for breaking down complex problems, creating technical specifications, designing system architecture, or brainstorming solutions before coding.",
 		description: "Plan and design before implementation",
 		groups: ["read", ["edit", { fileRegex: "\\.md$", description: "Markdown files only" }], "browser", "mcp"],
-		customInstructions:
-			"1. Do some information gathering (using provided tools) to get more context about the task.\n\n2. You should also ask the user clarifying questions to get a better understanding of the task.\n\n3. Once you've gained more context about the user's request, break down the task into clear, actionable steps and create a todo list using the `update_todo_list` tool. Each todo item should be:\n   - Specific and actionable\n   - Listed in logical execution order\n   - Focused on a single, well-defined outcome\n   - Clear enough that another mode could execute it independently\n\n   **Note:** If the `update_todo_list` tool is not available, write the plan to a markdown file (e.g., `plan.md` or `todo.md`) instead.\n\n4. As you gather more information or discover new requirements, update the todo list to reflect the current understanding of what needs to be accomplished.\n\n5. Ask the user if they are pleased with this plan, or if they would like to make any changes. Think of this as a brainstorming session where you can discuss the task and refine the todo list.\n\n6. Include Mermaid diagrams if they help clarify complex workflows or system architecture. Please avoid using double quotes (\"\") and parentheses () inside square brackets ([]) in Mermaid diagrams, as this can cause parsing errors.\n\n7. Use the switch_mode tool to request that the user switch to another mode to implement the solution.\n\n**IMPORTANT: Focus on creating clear, actionable todo lists rather than lengthy markdown documents. Use the todo list as your primary planning tool to track and organize the work that needs to be done.**",
-	},
-	{
-		slug: "code",
-		name: "💻 Code",
-		roleDefinition:
-			"You are Roo, a highly skilled software engineer with extensive knowledge in many programming languages, frameworks, design patterns, and best practices.",
-		whenToUse:
-			"Use this mode when you need to write, modify, or refactor code. Ideal for implementing features, fixing bugs, creating new files, or making code improvements across any programming language or framework.",
-		description: "Write, modify, and refactor code",
-		groups: ["read", "edit", "browser", "command", "mcp"],
+		customInstructions: architectInstructions,
 	},
 	{
 		slug: "ask",
-		name: "❓ Ask",
+		name: "Ask",
 		roleDefinition:
 			"You are Roo, a knowledgeable technical assistant focused on answering questions and providing information about software development, technology, and related topics.",
 		whenToUse:
 			"Use this mode when you need explanations, documentation, or answers to technical questions. Best for understanding concepts, analyzing existing code, getting recommendations, or learning about technologies without making changes.",
 		description: "Get answers and explanations",
 		groups: ["read", "browser", "mcp"],
-		customInstructions:
-			"You can analyze code, explain concepts, and access external resources. Always answer the user's questions thoroughly, and do not switch to implementing code unless explicitly requested by the user. Include Mermaid diagrams when they clarify your response.",
+		customInstructions: askInstructions,
 	},
 	{
-		slug: "debug",
-		name: "🪲 Debug",
+		slug: "code",
+		name: "Code",
 		roleDefinition:
-			"You are Roo, an expert software debugger specializing in systematic problem diagnosis and resolution.",
+			"You are Roo, a highly skilled software engineer with extensive knowledge in many programming languages, frameworks, design patterns, and best practices.",
 		whenToUse:
-			"Use this mode when you're troubleshooting issues, investigating errors, or diagnosing problems. Specialized in systematic debugging, adding logging, analyzing stack traces, and identifying root causes before applying fixes.",
-		description: "Diagnose and fix software issues",
+			"Use this mode when you need to write, modify, or refactor code. Ideal for implementing features, fixing bugs, creating new files, or making code improvements across any programming language or framework.",
+		description: "Write, modify, and refactor code",
 		groups: ["read", "edit", "browser", "command", "mcp"],
-		customInstructions:
-			"Reflect on 5-7 different possible sources of the problem, distill those down to 1-2 most likely sources, and then add logs to validate your assumptions. Explicitly ask the user to confirm the diagnosis before fixing the problem.",
+		customInstructions: codeInstructions,
 	},
 	{
-		slug: "orchestrator",
-		name: "🪃 Orchestrator",
+		slug: "native",
+		name: "Native",
 		roleDefinition:
-			"You are Roo, a strategic workflow orchestrator who coordinates complex tasks by delegating them to appropriate specialized modes. You have a comprehensive understanding of each mode's capabilities and limitations, allowing you to effectively break down complex problems into discrete tasks that can be solved by different specialists.",
+			"You are Roo.",
 		whenToUse:
-			"Use this mode for complex, multi-step projects that require coordination across different specialties. Ideal when you need to break down large tasks into subtasks, manage workflows, or coordinate work that spans multiple domains or expertise areas.",
-		description: "Coordinate tasks across multiple modes",
+			"Should never be used at any time",
+		description: "Native AI WITHOUT tools prompt",
 		groups: [],
-		customInstructions:
-			"Your role is to coordinate complex workflows by delegating tasks to specialized modes. As an orchestrator, you should:\n\n1. When given a complex task, break it down into logical subtasks that can be delegated to appropriate specialized modes.\n\n2. For each subtask, use the `new_task` tool to delegate. Choose the most appropriate mode for the subtask's specific goal and provide comprehensive instructions in the `message` parameter. These instructions must include:\n    *   All necessary context from the parent task or previous subtasks required to complete the work.\n    *   A clearly defined scope, specifying exactly what the subtask should accomplish.\n    *   An explicit statement that the subtask should *only* perform the work outlined in these instructions and not deviate.\n    *   An instruction for the subtask to signal completion by using the `attempt_completion` tool, providing a concise yet thorough summary of the outcome in the `result` parameter, keeping in mind that this summary will be the source of truth used to keep track of what was completed on this project.\n    *   A statement that these specific instructions supersede any conflicting general instructions the subtask's mode might have.\n\n3. Track and manage the progress of all subtasks. When a subtask is completed, analyze its results and determine the next steps.\n\n4. Help the user understand how the different subtasks fit together in the overall workflow. Provide clear reasoning about why you're delegating specific tasks to specific modes.\n\n5. When all subtasks are completed, synthesize the results and provide a comprehensive overview of what was accomplished.\n\n6. Ask clarifying questions when necessary to better understand how to break down complex tasks effectively.\n\n7. Suggest improvements to the workflow based on the results of completed subtasks.\n\nUse subtasks to maintain clarity. If a request significantly shifts focus or requires a different expertise (mode), consider creating a subtask rather than overloading the current one.",
 	},
+	// {
+	// 	slug: "debug",
+	// 	name: "🪲 Debug",
+	// 	roleDefinition:
+	// 		"You are Roo, an expert software debugger specializing in systematic problem diagnosis and resolution.",
+	// 	whenToUse:
+	// 		"Use this mode when you're troubleshooting issues, investigating errors, or diagnosing problems. Specialized in systematic debugging, adding logging, analyzing stack traces, and identifying root causes before applying fixes.",
+	// 	description: "Diagnose and fix software issues",
+	// 	groups: ["read", "edit", "browser", "command", "mcp"],
+	// 	customInstructions:
+	// 		"Reflect on 5-7 different possible sources of the problem, distill those down to 1-2 most likely sources, and then add logs to validate your assumptions. Explicitly ask the user to confirm the diagnosis before fixing the problem.",
+	// },
 ] as const
diff --git a/packages/types/src/provider-settings.ts b/packages/types/src/provider-settings.ts
index e13dc9d6..10caea36 100644
--- a/packages/types/src/provider-settings.ts
+++ b/packages/types/src/provider-settings.ts
@@ -33,6 +33,7 @@ export const providerNames = [
 	"groq",
 	"chutes",
 	"litellm",
+	"modelscope",
 	"huggingface",
 	"sambanova",
 ] as const
@@ -217,6 +218,11 @@ const requestySchema = baseProviderSettingsSchema.extend({
 	requestyModelId: z.string().optional(),
 })
 
+const modelscopeSchema = apiModelIdProviderModelSchema.extend({
+	modelscopeApiKey: z.string().optional(),
+	modelscopeBaseUrl: z.string().optional(),
+})
+
 const humanRelaySchema = baseProviderSettingsSchema
 
 const fakeAiSchema = baseProviderSettingsSchema.extend({
@@ -283,6 +289,7 @@ export const providerSettingsSchemaDiscriminated = z.discriminatedUnion("apiProv
 	huggingFaceSchema.merge(z.object({ apiProvider: z.literal("huggingface") })),
 	chutesSchema.merge(z.object({ apiProvider: z.literal("chutes") })),
 	litellmSchema.merge(z.object({ apiProvider: z.literal("litellm") })),
+	modelscopeSchema.merge(z.object({ apiProvider: z.literal("modelscope") })),
 	sambaNovaSchema.merge(z.object({ apiProvider: z.literal("sambanova") })),
 	defaultSchema,
 ])
@@ -315,7 +322,8 @@ export const providerSettingsSchema = z.object({
 	...huggingFaceSchema.shape,
 	...chutesSchema.shape,
 	...litellmSchema.shape,
-	...sambaNovaSchema.shape,
+	...modelscopeSchema.shape,
+    ...sambaNovaSchema.shape,
 	...codebaseIndexProviderSchema.shape,
 })
 
diff --git a/packages/types/src/providers/gemini.ts b/packages/types/src/providers/gemini.ts
index a7225c73..7b2ca11c 100644
--- a/packages/types/src/providers/gemini.ts
+++ b/packages/types/src/providers/gemini.ts
@@ -3,51 +3,9 @@ import type { ModelInfo } from "../model.js"
 // https://ai.google.dev/gemini-api/docs/models/gemini
 export type GeminiModelId = keyof typeof geminiModels
 
-export const geminiDefaultModelId: GeminiModelId = "gemini-2.0-flash-001"
+export const geminiDefaultModelId: GeminiModelId = "gemini-2.5-flash"
 
 export const geminiModels = {
-	"gemini-2.5-flash-preview-04-17:thinking": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0.15,
-		outputPrice: 3.5,
-		maxThinkingTokens: 24_576,
-		supportsReasoningBudget: true,
-		requiredReasoningBudget: true,
-	},
-	"gemini-2.5-flash-preview-04-17": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0.15,
-		outputPrice: 0.6,
-	},
-	"gemini-2.5-flash-preview-05-20:thinking": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 0.15,
-		outputPrice: 3.5,
-		cacheReadsPrice: 0.0375,
-		cacheWritesPrice: 1.0,
-		maxThinkingTokens: 24_576,
-		supportsReasoningBudget: true,
-		requiredReasoningBudget: true,
-	},
-	"gemini-2.5-flash-preview-05-20": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 0.15,
-		outputPrice: 0.6,
-		cacheReadsPrice: 0.0375,
-		cacheWritesPrice: 1.0,
-	},
 	"gemini-2.5-flash": {
 		maxTokens: 64_000,
 		contextWindow: 1_048_576,
@@ -60,88 +18,6 @@ export const geminiModels = {
 		maxThinkingTokens: 24_576,
 		supportsReasoningBudget: true,
 	},
-	"gemini-2.5-pro-exp-03-25": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.5-pro-preview-03-25": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
-		outputPrice: 15,
-		cacheReadsPrice: 0.625,
-		cacheWritesPrice: 4.5,
-		tiers: [
-			{
-				contextWindow: 200_000,
-				inputPrice: 1.25,
-				outputPrice: 10,
-				cacheReadsPrice: 0.31,
-			},
-			{
-				contextWindow: Infinity,
-				inputPrice: 2.5,
-				outputPrice: 15,
-				cacheReadsPrice: 0.625,
-			},
-		],
-	},
-	"gemini-2.5-pro-preview-05-06": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
-		outputPrice: 15,
-		cacheReadsPrice: 0.625,
-		cacheWritesPrice: 4.5,
-		tiers: [
-			{
-				contextWindow: 200_000,
-				inputPrice: 1.25,
-				outputPrice: 10,
-				cacheReadsPrice: 0.31,
-			},
-			{
-				contextWindow: Infinity,
-				inputPrice: 2.5,
-				outputPrice: 15,
-				cacheReadsPrice: 0.625,
-			},
-		],
-	},
-	"gemini-2.5-pro-preview-06-05": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
-		outputPrice: 15,
-		cacheReadsPrice: 0.625,
-		cacheWritesPrice: 4.5,
-		maxThinkingTokens: 32_768,
-		supportsReasoningBudget: true,
-		tiers: [
-			{
-				contextWindow: 200_000,
-				inputPrice: 1.25,
-				outputPrice: 10,
-				cacheReadsPrice: 0.31,
-			},
-			{
-				contextWindow: Infinity,
-				inputPrice: 2.5,
-				outputPrice: 15,
-				cacheReadsPrice: 0.625,
-			},
-		],
-	},
 	"gemini-2.5-pro": {
 		maxTokens: 64_000,
 		contextWindow: 1_048_576,
@@ -169,121 +45,7 @@ export const geminiModels = {
 			},
 		],
 	},
-	"gemini-2.0-flash-001": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 0.1,
-		outputPrice: 0.4,
-		cacheReadsPrice: 0.025,
-		cacheWritesPrice: 1.0,
-	},
-	"gemini-2.0-flash-lite-preview-02-05": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.0-pro-exp-02-05": {
-		maxTokens: 8192,
-		contextWindow: 2_097_152,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.0-flash-thinking-exp-01-21": {
-		maxTokens: 65_536,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.0-flash-thinking-exp-1219": {
-		maxTokens: 8192,
-		contextWindow: 32_767,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.0-flash-exp": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-1.5-flash-002": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 0.15, // This is the pricing for prompts above 128k tokens.
-		outputPrice: 0.6,
-		cacheReadsPrice: 0.0375,
-		cacheWritesPrice: 1.0,
-		tiers: [
-			{
-				contextWindow: 128_000,
-				inputPrice: 0.075,
-				outputPrice: 0.3,
-				cacheReadsPrice: 0.01875,
-			},
-			{
-				contextWindow: Infinity,
-				inputPrice: 0.15,
-				outputPrice: 0.6,
-				cacheReadsPrice: 0.0375,
-			},
-		],
-	},
-	"gemini-1.5-flash-exp-0827": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-1.5-flash-8b-exp-0827": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-1.5-pro-002": {
-		maxTokens: 8192,
-		contextWindow: 2_097_152,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-1.5-pro-exp-0827": {
-		maxTokens: 8192,
-		contextWindow: 2_097_152,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-exp-1206": {
-		maxTokens: 8192,
-		contextWindow: 2_097_152,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.5-flash-lite-preview-06-17": {
+	"gemini-2.5-flash-lite": {
 		maxTokens: 64_000,
 		contextWindow: 1_048_576,
 		supportsImages: true,
diff --git a/packages/types/src/providers/index.ts b/packages/types/src/providers/index.ts
index d6676b88..5deea249 100644
--- a/packages/types/src/providers/index.ts
+++ b/packages/types/src/providers/index.ts
@@ -10,6 +10,7 @@ export * from "./huggingface.js"
 export * from "./lite-llm.js"
 export * from "./lm-studio.js"
 export * from "./mistral.js"
+export * from "./modelscope.js"
 export * from "./moonshot.js"
 export * from "./ollama.js"
 export * from "./openai.js"
diff --git a/packages/types/src/tool.ts b/packages/types/src/tool.ts
index 7a3fd211..11590820 100644
--- a/packages/types/src/tool.ts
+++ b/packages/types/src/tool.ts
@@ -34,6 +34,8 @@ export const toolNames = [
 	"fetch_instructions",
 	"codebase_search",
 	"update_todo_list",
+	"web_search",
+	"url_fetch",
 ] as const
 
 export const toolNamesSchema = z.enum(toolNames)
@@ -53,3 +55,17 @@ export const toolUsageSchema = z.record(
 )
 
 export type ToolUsage = z.infer<typeof toolUsageSchema>
+
+/**
+ * ToolExecutionStatus
+ */
+
+export const toolExecutionStatusSchema = z.object({
+	executionId: z.string(),
+	status: z.enum(["started", "output", "completed", "error"]),
+	toolName: z.string(),
+	response: z.string().optional(),
+	error: z.string().optional(),
+})
+
+export type ToolExecutionStatus = z.infer<typeof toolExecutionStatusSchema>
diff --git a/pnpm-lock.yaml b/pnpm-lock.yaml
index 3e7bb79b..b8e513a7 100644
--- a/pnpm-lock.yaml
+++ b/pnpm-lock.yaml
@@ -685,7 +685,7 @@ importers:
         version: 12.0.0
       openai:
         specifier: ^5.0.0
-        version: 5.5.1(ws@8.18.2)(zod@3.25.61)
+        version: 5.5.1(ws@8.18.3)(zod@3.25.61)
       os-name:
         specifier: ^6.0.0
         version: 6.1.0
@@ -17520,9 +17520,9 @@ snapshots:
       is-inside-container: 1.0.0
       is-wsl: 3.1.0
 
-  openai@5.5.1(ws@8.18.2)(zod@3.25.61):
+  openai@5.5.1(ws@8.18.3)(zod@3.25.61):
     optionalDependencies:
-      ws: 8.18.2
+      ws: 8.18.3
       zod: 3.25.61
 
   option@0.2.4: {}
diff --git a/src/api/index.ts b/src/api/index.ts
index f726063a..bfdb855d 100644
--- a/src/api/index.ts
+++ b/src/api/index.ts
@@ -32,6 +32,7 @@ import {
 	ClaudeCodeHandler,
 	SambaNovaHandler,
 	DoubaoHandler,
+	ModelScopeHandler,
 } from "./providers"
 
 export interface SingleCompletionHandler {
@@ -83,6 +84,8 @@ export function buildApiHandler(configuration: ProviderSettings): ApiHandler {
 				: new VertexHandler(options)
 		case "openai":
 			return new OpenAiHandler(options)
+		case "modelscope":
+			return new ModelScopeHandler(options)
 		case "ollama":
 			return new OllamaHandler(options)
 		case "lmstudio":
diff --git a/src/api/providers/__tests__/chutes.spec.ts b/src/api/providers/__tests__/chutes.spec.ts
index 911c848b..35cb183d 100644
--- a/src/api/providers/__tests__/chutes.spec.ts
+++ b/src/api/providers/__tests__/chutes.spec.ts
@@ -208,29 +208,6 @@ describe("ChutesHandler", () => {
 		)
 	})
 
-	it("should return zai-org/GLM-4.5-FP8 model with correct configuration", () => {
-		const testModelId: ChutesModelId = "zai-org/GLM-4.5-FP8"
-		const handlerWithModel = new ChutesHandler({
-			apiModelId: testModelId,
-			chutesApiKey: "test-chutes-api-key",
-		})
-		const model = handlerWithModel.getModel()
-		expect(model.id).toBe(testModelId)
-		expect(model.info).toEqual(
-			expect.objectContaining({
-				maxTokens: 32768,
-				contextWindow: 131072,
-				supportsImages: false,
-				supportsPromptCache: false,
-				inputPrice: 0,
-				outputPrice: 0,
-				description:
-					"GLM-4.5-FP8 model with 128k token context window, optimized for agent-based applications with MoE architecture.",
-				temperature: 0.5, // Default temperature for non-DeepSeek models
-			}),
-		)
-	})
-
 	it("completePrompt method should return text from Chutes API", async () => {
 		const expectedResponse = "This is a test response from Chutes"
 		mockCreate.mockResolvedValueOnce({ choices: [{ message: { content: expectedResponse } }] })
diff --git a/src/api/providers/__tests__/gemini.spec.ts b/src/api/providers/__tests__/gemini.spec.ts
index 812c1ae1..e69de29b 100644
--- a/src/api/providers/__tests__/gemini.spec.ts
+++ b/src/api/providers/__tests__/gemini.spec.ts
@@ -1,251 +0,0 @@
-// npx vitest run src/api/providers/__tests__/gemini.spec.ts
-
-import { Anthropic } from "@anthropic-ai/sdk"
-
-import { type ModelInfo, geminiDefaultModelId } from "@roo-code/types"
-
-import { t } from "i18next"
-import { GeminiHandler } from "../gemini"
-
-const GEMINI_20_FLASH_THINKING_NAME = "gemini-2.0-flash-thinking-exp-1219"
-
-describe("GeminiHandler", () => {
-	let handler: GeminiHandler
-
-	beforeEach(() => {
-		// Create mock functions
-		const mockGenerateContentStream = vitest.fn()
-		const mockGenerateContent = vitest.fn()
-		const mockGetGenerativeModel = vitest.fn()
-
-		handler = new GeminiHandler({
-			apiKey: "test-key",
-			apiModelId: GEMINI_20_FLASH_THINKING_NAME,
-			geminiApiKey: "test-key",
-		})
-
-		// Replace the client with our mock
-		handler["client"] = {
-			models: {
-				generateContentStream: mockGenerateContentStream,
-				generateContent: mockGenerateContent,
-				getGenerativeModel: mockGetGenerativeModel,
-			},
-		} as any
-	})
-
-	describe("constructor", () => {
-		it("should initialize with provided config", () => {
-			expect(handler["options"].geminiApiKey).toBe("test-key")
-			expect(handler["options"].apiModelId).toBe(GEMINI_20_FLASH_THINKING_NAME)
-		})
-	})
-
-	describe("createMessage", () => {
-		const mockMessages: Anthropic.Messages.MessageParam[] = [
-			{
-				role: "user",
-				content: "Hello",
-			},
-			{
-				role: "assistant",
-				content: "Hi there!",
-			},
-		]
-
-		const systemPrompt = "You are a helpful assistant"
-
-		it("should handle text messages correctly", async () => {
-			// Setup the mock implementation to return an async generator
-			;(handler["client"].models.generateContentStream as any).mockResolvedValue({
-				[Symbol.asyncIterator]: async function* () {
-					yield { text: "Hello" }
-					yield { text: " world!" }
-					yield { usageMetadata: { promptTokenCount: 10, candidatesTokenCount: 5 } }
-				},
-			})
-
-			const stream = handler.createMessage(systemPrompt, mockMessages)
-			const chunks = []
-
-			for await (const chunk of stream) {
-				chunks.push(chunk)
-			}
-
-			// Should have 3 chunks: 'Hello', ' world!', and usage info
-			expect(chunks.length).toBe(3)
-			expect(chunks[0]).toEqual({ type: "text", text: "Hello" })
-			expect(chunks[1]).toEqual({ type: "text", text: " world!" })
-			expect(chunks[2]).toEqual({ type: "usage", inputTokens: 10, outputTokens: 5 })
-
-			// Verify the call to generateContentStream
-			expect(handler["client"].models.generateContentStream).toHaveBeenCalledWith(
-				expect.objectContaining({
-					model: GEMINI_20_FLASH_THINKING_NAME,
-					config: expect.objectContaining({
-						temperature: 0,
-						systemInstruction: systemPrompt,
-					}),
-				}),
-			)
-		})
-
-		it("should handle API errors", async () => {
-			const mockError = new Error("Gemini API error")
-			;(handler["client"].models.generateContentStream as any).mockRejectedValue(mockError)
-
-			const stream = handler.createMessage(systemPrompt, mockMessages)
-
-			await expect(async () => {
-				for await (const _chunk of stream) {
-					// Should throw before yielding any chunks
-				}
-			}).rejects.toThrow()
-		})
-	})
-
-	describe("completePrompt", () => {
-		it("should complete prompt successfully", async () => {
-			// Mock the response with text property
-			;(handler["client"].models.generateContent as any).mockResolvedValue({
-				text: "Test response",
-			})
-
-			const result = await handler.completePrompt("Test prompt")
-			expect(result).toBe("Test response")
-
-			// Verify the call to generateContent
-			expect(handler["client"].models.generateContent).toHaveBeenCalledWith({
-				model: GEMINI_20_FLASH_THINKING_NAME,
-				contents: [{ role: "user", parts: [{ text: "Test prompt" }] }],
-				config: {
-					httpOptions: undefined,
-					temperature: 0,
-				},
-			})
-		})
-
-		it("should handle API errors", async () => {
-			const mockError = new Error("Gemini API error")
-			;(handler["client"].models.generateContent as any).mockRejectedValue(mockError)
-
-			await expect(handler.completePrompt("Test prompt")).rejects.toThrow(
-				t("common:errors.gemini.generate_complete_prompt", { error: "Gemini API error" }),
-			)
-		})
-
-		it("should handle empty response", async () => {
-			// Mock the response with empty text
-			;(handler["client"].models.generateContent as any).mockResolvedValue({
-				text: "",
-			})
-
-			const result = await handler.completePrompt("Test prompt")
-			expect(result).toBe("")
-		})
-	})
-
-	describe("getModel", () => {
-		it("should return correct model info", () => {
-			const modelInfo = handler.getModel()
-			expect(modelInfo.id).toBe(GEMINI_20_FLASH_THINKING_NAME)
-			expect(modelInfo.info).toBeDefined()
-			expect(modelInfo.info.maxTokens).toBe(8192)
-			expect(modelInfo.info.contextWindow).toBe(32_767)
-		})
-
-		it("should return default model if invalid model specified", () => {
-			const invalidHandler = new GeminiHandler({
-				apiModelId: "invalid-model",
-				geminiApiKey: "test-key",
-			})
-			const modelInfo = invalidHandler.getModel()
-			expect(modelInfo.id).toBe(geminiDefaultModelId) // Default model
-		})
-	})
-
-	describe("calculateCost", () => {
-		// Mock ModelInfo based on gemini-1.5-flash-latest pricing (per 1M tokens)
-		// Removed 'id' and 'name' as they are not part of ModelInfo type directly
-		const mockInfo: ModelInfo = {
-			inputPrice: 0.125, // $/1M tokens
-			outputPrice: 0.375, // $/1M tokens
-			cacheWritesPrice: 0.125, // Assume same as input for test
-			cacheReadsPrice: 0.125 * 0.25, // Assume 0.25x input for test
-			contextWindow: 1_000_000,
-			maxTokens: 8192,
-			supportsPromptCache: true, // Enable cache calculations for tests
-		}
-
-		it("should calculate cost correctly based on input and output tokens", () => {
-			const inputTokens = 10000 // Use larger numbers for per-million pricing
-			const outputTokens = 20000
-			// Added non-null assertions (!) as mockInfo guarantees these values
-			const expectedCost =
-				(inputTokens / 1_000_000) * mockInfo.inputPrice! + (outputTokens / 1_000_000) * mockInfo.outputPrice!
-
-			const cost = handler.calculateCost({ info: mockInfo, inputTokens, outputTokens })
-			expect(cost).toBeCloseTo(expectedCost)
-		})
-
-		it("should return 0 if token counts are zero", () => {
-			// Note: The method expects numbers, not undefined. Passing undefined would be a type error.
-			// The calculateCost method itself returns undefined if prices are missing, but 0 if tokens are 0 and prices exist.
-			expect(handler.calculateCost({ info: mockInfo, inputTokens: 0, outputTokens: 0 })).toBe(0)
-		})
-
-		it("should handle only input tokens", () => {
-			const inputTokens = 5000
-			// Added non-null assertion (!)
-			const expectedCost = (inputTokens / 1_000_000) * mockInfo.inputPrice!
-			expect(handler.calculateCost({ info: mockInfo, inputTokens, outputTokens: 0 })).toBeCloseTo(expectedCost)
-		})
-
-		it("should handle only output tokens", () => {
-			const outputTokens = 15000
-			// Added non-null assertion (!)
-			const expectedCost = (outputTokens / 1_000_000) * mockInfo.outputPrice!
-			expect(handler.calculateCost({ info: mockInfo, inputTokens: 0, outputTokens })).toBeCloseTo(expectedCost)
-		})
-
-		it("should calculate cost with cache write tokens", () => {
-			const inputTokens = 10000
-			const outputTokens = 20000
-			const cacheWriteTokens = 5000
-			const CACHE_TTL = 5 // Match the constant in gemini.ts
-
-			// Added non-null assertions (!)
-			const expectedInputCost = (inputTokens / 1_000_000) * mockInfo.inputPrice!
-			const expectedOutputCost = (outputTokens / 1_000_000) * mockInfo.outputPrice!
-			const expectedCacheWriteCost =
-				mockInfo.cacheWritesPrice! * (cacheWriteTokens / 1_000_000) * (CACHE_TTL / 60)
-			const expectedCost = expectedInputCost + expectedOutputCost + expectedCacheWriteCost
-
-			const cost = handler.calculateCost({ info: mockInfo, inputTokens, outputTokens })
-			expect(cost).toBeCloseTo(expectedCost)
-		})
-
-		it("should calculate cost with cache read tokens", () => {
-			const inputTokens = 10000 // Total logical input
-			const outputTokens = 20000
-			const cacheReadTokens = 8000 // Part of inputTokens read from cache
-
-			const uncachedReadTokens = inputTokens - cacheReadTokens
-			// Added non-null assertions (!)
-			const expectedInputCost = (uncachedReadTokens / 1_000_000) * mockInfo.inputPrice!
-			const expectedOutputCost = (outputTokens / 1_000_000) * mockInfo.outputPrice!
-			const expectedCacheReadCost = mockInfo.cacheReadsPrice! * (cacheReadTokens / 1_000_000)
-			const expectedCost = expectedInputCost + expectedOutputCost + expectedCacheReadCost
-
-			const cost = handler.calculateCost({ info: mockInfo, inputTokens, outputTokens, cacheReadTokens })
-			expect(cost).toBeCloseTo(expectedCost)
-		})
-
-		it("should return undefined if pricing info is missing", () => {
-			// Create a copy and explicitly set a price to undefined
-			const incompleteInfo: ModelInfo = { ...mockInfo, outputPrice: undefined }
-			const cost = handler.calculateCost({ info: incompleteInfo, inputTokens: 1000, outputTokens: 1000 })
-			expect(cost).toBeUndefined()
-		})
-	})
-})
diff --git a/src/api/providers/__tests__/sambanova.spec.ts b/src/api/providers/__tests__/sambanova.spec.ts
deleted file mode 100644
index cd0e4a19..00000000
--- a/src/api/providers/__tests__/sambanova.spec.ts
+++ /dev/null
@@ -1,154 +0,0 @@
-// npx vitest run src/api/providers/__tests__/sambanova.spec.ts
-
-// Mock vscode first to avoid import errors
-vitest.mock("vscode", () => ({}))
-
-import OpenAI from "openai"
-import { Anthropic } from "@anthropic-ai/sdk"
-
-import { type SambaNovaModelId, sambaNovaDefaultModelId, sambaNovaModels } from "@roo-code/types"
-
-import { SambaNovaHandler } from "../sambanova"
-
-vitest.mock("openai", () => {
-	const createMock = vitest.fn()
-	return {
-		default: vitest.fn(() => ({ chat: { completions: { create: createMock } } })),
-	}
-})
-
-describe("SambaNovaHandler", () => {
-	let handler: SambaNovaHandler
-	let mockCreate: any
-
-	beforeEach(() => {
-		vitest.clearAllMocks()
-		mockCreate = (OpenAI as unknown as any)().chat.completions.create
-		handler = new SambaNovaHandler({ sambaNovaApiKey: "test-sambanova-api-key" })
-	})
-
-	it("should use the correct SambaNova base URL", () => {
-		new SambaNovaHandler({ sambaNovaApiKey: "test-sambanova-api-key" })
-		expect(OpenAI).toHaveBeenCalledWith(expect.objectContaining({ baseURL: "https://api.sambanova.ai/v1" }))
-	})
-
-	it("should use the provided API key", () => {
-		const sambaNovaApiKey = "test-sambanova-api-key"
-		new SambaNovaHandler({ sambaNovaApiKey })
-		expect(OpenAI).toHaveBeenCalledWith(expect.objectContaining({ apiKey: sambaNovaApiKey }))
-	})
-
-	it("should return default model when no model is specified", () => {
-		const model = handler.getModel()
-		expect(model.id).toBe(sambaNovaDefaultModelId)
-		expect(model.info).toEqual(sambaNovaModels[sambaNovaDefaultModelId])
-	})
-
-	it("should return specified model when valid model is provided", () => {
-		const testModelId: SambaNovaModelId = "Meta-Llama-3.3-70B-Instruct"
-		const handlerWithModel = new SambaNovaHandler({
-			apiModelId: testModelId,
-			sambaNovaApiKey: "test-sambanova-api-key",
-		})
-		const model = handlerWithModel.getModel()
-		expect(model.id).toBe(testModelId)
-		expect(model.info).toEqual(sambaNovaModels[testModelId])
-	})
-
-	it("completePrompt method should return text from SambaNova API", async () => {
-		const expectedResponse = "This is a test response from SambaNova"
-		mockCreate.mockResolvedValueOnce({ choices: [{ message: { content: expectedResponse } }] })
-		const result = await handler.completePrompt("test prompt")
-		expect(result).toBe(expectedResponse)
-	})
-
-	it("should handle errors in completePrompt", async () => {
-		const errorMessage = "SambaNova API error"
-		mockCreate.mockRejectedValueOnce(new Error(errorMessage))
-		await expect(handler.completePrompt("test prompt")).rejects.toThrow(
-			`SambaNova completion error: ${errorMessage}`,
-		)
-	})
-
-	it("createMessage should yield text content from stream", async () => {
-		const testContent = "This is test content from SambaNova stream"
-
-		mockCreate.mockImplementationOnce(() => {
-			return {
-				[Symbol.asyncIterator]: () => ({
-					next: vitest
-						.fn()
-						.mockResolvedValueOnce({
-							done: false,
-							value: { choices: [{ delta: { content: testContent } }] },
-						})
-						.mockResolvedValueOnce({ done: true }),
-				}),
-			}
-		})
-
-		const stream = handler.createMessage("system prompt", [])
-		const firstChunk = await stream.next()
-
-		expect(firstChunk.done).toBe(false)
-		expect(firstChunk.value).toEqual({ type: "text", text: testContent })
-	})
-
-	it("createMessage should yield usage data from stream", async () => {
-		mockCreate.mockImplementationOnce(() => {
-			return {
-				[Symbol.asyncIterator]: () => ({
-					next: vitest
-						.fn()
-						.mockResolvedValueOnce({
-							done: false,
-							value: { choices: [{ delta: {} }], usage: { prompt_tokens: 10, completion_tokens: 20 } },
-						})
-						.mockResolvedValueOnce({ done: true }),
-				}),
-			}
-		})
-
-		const stream = handler.createMessage("system prompt", [])
-		const firstChunk = await stream.next()
-
-		expect(firstChunk.done).toBe(false)
-		expect(firstChunk.value).toEqual({ type: "usage", inputTokens: 10, outputTokens: 20 })
-	})
-
-	it("createMessage should pass correct parameters to SambaNova client", async () => {
-		const modelId: SambaNovaModelId = "Meta-Llama-3.3-70B-Instruct"
-		const modelInfo = sambaNovaModels[modelId]
-		const handlerWithModel = new SambaNovaHandler({
-			apiModelId: modelId,
-			sambaNovaApiKey: "test-sambanova-api-key",
-		})
-
-		mockCreate.mockImplementationOnce(() => {
-			return {
-				[Symbol.asyncIterator]: () => ({
-					async next() {
-						return { done: true }
-					},
-				}),
-			}
-		})
-
-		const systemPrompt = "Test system prompt for SambaNova"
-		const messages: Anthropic.Messages.MessageParam[] = [{ role: "user", content: "Test message for SambaNova" }]
-
-		const messageGenerator = handlerWithModel.createMessage(systemPrompt, messages)
-		await messageGenerator.next()
-
-		expect(mockCreate).toHaveBeenCalledWith(
-			expect.objectContaining({
-				model: modelId,
-				max_tokens: modelInfo.maxTokens,
-				temperature: 0.7,
-				messages: expect.arrayContaining([{ role: "system", content: systemPrompt }]),
-				stream: true,
-				stream_options: { include_usage: true },
-			}),
-		)
-	})
-})
diff --git a/src/api/providers/__tests__/vertex.spec.ts b/src/api/providers/__tests__/vertex.spec.ts
index d147e79b..e69de29b 100644
--- a/src/api/providers/__tests__/vertex.spec.ts
+++ b/src/api/providers/__tests__/vertex.spec.ts
@@ -1,141 +0,0 @@
-// npx vitest run src/api/providers/__tests__/vertex.spec.ts
-
-// Mock vscode first to avoid import errors
-vitest.mock("vscode", () => ({}))
-
-import { Anthropic } from "@anthropic-ai/sdk"
-
-import { ApiStreamChunk } from "../../transform/stream"
-
-import { t } from "i18next"
-import { VertexHandler } from "../vertex"
-
-describe("VertexHandler", () => {
-	let handler: VertexHandler
-
-	beforeEach(() => {
-		// Create mock functions
-		const mockGenerateContentStream = vitest.fn()
-		const mockGenerateContent = vitest.fn()
-		const mockGetGenerativeModel = vitest.fn()
-
-		handler = new VertexHandler({
-			apiModelId: "gemini-1.5-pro-001",
-			vertexProjectId: "test-project",
-			vertexRegion: "us-central1",
-		})
-
-		// Replace the client with our mock
-		handler["client"] = {
-			models: {
-				generateContentStream: mockGenerateContentStream,
-				generateContent: mockGenerateContent,
-				getGenerativeModel: mockGetGenerativeModel,
-			},
-		} as any
-	})
-
-	describe("createMessage", () => {
-		const mockMessages: Anthropic.Messages.MessageParam[] = [
-			{ role: "user", content: "Hello" },
-			{ role: "assistant", content: "Hi there!" },
-		]
-
-		const systemPrompt = "You are a helpful assistant"
-
-		it("should handle streaming responses correctly for Gemini", async () => {
-			// Let's examine the test expectations and adjust our mock accordingly
-			// The test expects 4 chunks:
-			// 1. Usage chunk with input tokens
-			// 2. Text chunk with "Gemini response part 1"
-			// 3. Text chunk with " part 2"
-			// 4. Usage chunk with output tokens
-
-			// Let's modify our approach and directly mock the createMessage method
-			// instead of mocking the client
-			vitest.spyOn(handler, "createMessage").mockImplementation(async function* () {
-				yield { type: "usage", inputTokens: 10, outputTokens: 0 }
-				yield { type: "text", text: "Gemini response part 1" }
-				yield { type: "text", text: " part 2" }
-				yield { type: "usage", inputTokens: 0, outputTokens: 5 }
-			})
-
-			const stream = handler.createMessage(systemPrompt, mockMessages)
-
-			const chunks: ApiStreamChunk[] = []
-
-			for await (const chunk of stream) {
-				chunks.push(chunk)
-			}
-
-			expect(chunks.length).toBe(4)
-			expect(chunks[0]).toEqual({ type: "usage", inputTokens: 10, outputTokens: 0 })
-			expect(chunks[1]).toEqual({ type: "text", text: "Gemini response part 1" })
-			expect(chunks[2]).toEqual({ type: "text", text: " part 2" })
-			expect(chunks[3]).toEqual({ type: "usage", inputTokens: 0, outputTokens: 5 })
-
-			// Since we're directly mocking createMessage, we don't need to verify
-			// that generateContentStream was called
-		})
-	})
-
-	describe("completePrompt", () => {
-		it("should complete prompt successfully for Gemini", async () => {
-			// Mock the response with text property
-			;(handler["client"].models.generateContent as any).mockResolvedValue({
-				text: "Test Gemini response",
-			})
-
-			const result = await handler.completePrompt("Test prompt")
-			expect(result).toBe("Test Gemini response")
-
-			// Verify the call to generateContent
-			expect(handler["client"].models.generateContent).toHaveBeenCalledWith(
-				expect.objectContaining({
-					model: expect.any(String),
-					contents: [{ role: "user", parts: [{ text: "Test prompt" }] }],
-					config: expect.objectContaining({
-						temperature: 0,
-					}),
-				}),
-			)
-		})
-
-		it("should handle API errors for Gemini", async () => {
-			const mockError = new Error("Vertex API error")
-			;(handler["client"].models.generateContent as any).mockRejectedValue(mockError)
-
-			await expect(handler.completePrompt("Test prompt")).rejects.toThrow(
-				t("common:errors.gemini.generate_complete_prompt", { error: "Vertex API error" }),
-			)
-		})
-
-		it("should handle empty response for Gemini", async () => {
-			// Mock the response with empty text
-			;(handler["client"].models.generateContent as any).mockResolvedValue({
-				text: "",
-			})
-
-			const result = await handler.completePrompt("Test prompt")
-			expect(result).toBe("")
-		})
-	})
-
-	describe("getModel", () => {
-		it("should return correct model info for Gemini", () => {
-			// Create a new instance with specific model ID
-			const testHandler = new VertexHandler({
-				apiModelId: "gemini-2.0-flash-001",
-				vertexProjectId: "test-project",
-				vertexRegion: "us-central1",
-			})
-
-			// Don't mock getModel here as we want to test the actual implementation
-			const modelInfo = testHandler.getModel()
-			expect(modelInfo.id).toBe("gemini-2.0-flash-001")
-			expect(modelInfo.info).toBeDefined()
-			expect(modelInfo.info.maxTokens).toBe(8192)
-			expect(modelInfo.info.contextWindow).toBe(1048576)
-		})
-	})
-})
diff --git a/src/api/providers/__tests__/vscode-lm.spec.ts b/src/api/providers/__tests__/vscode-lm.spec.ts
index afb349e5..e69de29b 100644
--- a/src/api/providers/__tests__/vscode-lm.spec.ts
+++ b/src/api/providers/__tests__/vscode-lm.spec.ts
@@ -1,303 +0,0 @@
-import type { Mock } from "vitest"
-
-// Mocks must come first, before imports
-vi.mock("vscode", () => {
-	class MockLanguageModelTextPart {
-		type = "text"
-		constructor(public value: string) {}
-	}
-
-	class MockLanguageModelToolCallPart {
-		type = "tool_call"
-		constructor(
-			public callId: string,
-			public name: string,
-			public input: any,
-		) {}
-	}
-
-	return {
-		workspace: {
-			onDidChangeConfiguration: vi.fn((_callback) => ({
-				dispose: vi.fn(),
-			})),
-		},
-		CancellationTokenSource: vi.fn(() => ({
-			token: {
-				isCancellationRequested: false,
-				onCancellationRequested: vi.fn(),
-			},
-			cancel: vi.fn(),
-			dispose: vi.fn(),
-		})),
-		CancellationError: class CancellationError extends Error {
-			constructor() {
-				super("Operation cancelled")
-				this.name = "CancellationError"
-			}
-		},
-		LanguageModelChatMessage: {
-			Assistant: vi.fn((content) => ({
-				role: "assistant",
-				content: Array.isArray(content) ? content : [new MockLanguageModelTextPart(content)],
-			})),
-			User: vi.fn((content) => ({
-				role: "user",
-				content: Array.isArray(content) ? content : [new MockLanguageModelTextPart(content)],
-			})),
-		},
-		LanguageModelTextPart: MockLanguageModelTextPart,
-		LanguageModelToolCallPart: MockLanguageModelToolCallPart,
-		lm: {
-			selectChatModels: vi.fn(),
-		},
-	}
-})
-
-import * as vscode from "vscode"
-import { VsCodeLmHandler } from "../vscode-lm"
-import type { ApiHandlerOptions } from "../../../shared/api"
-import type { Anthropic } from "@anthropic-ai/sdk"
-
-const mockLanguageModelChat = {
-	id: "test-model",
-	name: "Test Model",
-	vendor: "test-vendor",
-	family: "test-family",
-	version: "1.0",
-	maxInputTokens: 4096,
-	sendRequest: vi.fn(),
-	countTokens: vi.fn(),
-}
-
-describe("VsCodeLmHandler", () => {
-	let handler: VsCodeLmHandler
-	const defaultOptions: ApiHandlerOptions = {
-		vsCodeLmModelSelector: {
-			vendor: "test-vendor",
-			family: "test-family",
-		},
-	}
-
-	beforeEach(() => {
-		vi.clearAllMocks()
-		handler = new VsCodeLmHandler(defaultOptions)
-	})
-
-	afterEach(() => {
-		handler.dispose()
-	})
-
-	describe("constructor", () => {
-		it("should initialize with provided options", () => {
-			expect(handler).toBeDefined()
-			expect(vscode.workspace.onDidChangeConfiguration).toHaveBeenCalled()
-		})
-
-		it("should handle configuration changes", () => {
-			const callback = (vscode.workspace.onDidChangeConfiguration as Mock).mock.calls[0][0]
-			callback({ affectsConfiguration: () => true })
-			// Should reset client when config changes
-			expect(handler["client"]).toBeNull()
-		})
-	})
-
-	describe("createClient", () => {
-		it("should create client with selector", async () => {
-			const mockModel = { ...mockLanguageModelChat }
-			;(vscode.lm.selectChatModels as Mock).mockResolvedValueOnce([mockModel])
-
-			const client = await handler["createClient"]({
-				vendor: "test-vendor",
-				family: "test-family",
-			})
-
-			expect(client).toBeDefined()
-			expect(client.id).toBe("test-model")
-			expect(vscode.lm.selectChatModels).toHaveBeenCalledWith({
-				vendor: "test-vendor",
-				family: "test-family",
-			})
-		})
-
-		it("should return default client when no models available", async () => {
-			;(vscode.lm.selectChatModels as Mock).mockResolvedValueOnce([])
-
-			const client = await handler["createClient"]({})
-
-			expect(client).toBeDefined()
-			expect(client.id).toBe("default-lm")
-			expect(client.vendor).toBe("vscode")
-		})
-	})
-
-	describe("createMessage", () => {
-		beforeEach(() => {
-			const mockModel = { ...mockLanguageModelChat }
-			;(vscode.lm.selectChatModels as Mock).mockResolvedValueOnce([mockModel])
-			mockLanguageModelChat.countTokens.mockResolvedValue(10)
-
-			// Override the default client with our test client
-			handler["client"] = mockLanguageModelChat
-		})
-
-		it("should stream text responses", async () => {
-			const systemPrompt = "You are a helpful assistant"
-			const messages: Anthropic.Messages.MessageParam[] = [
-				{
-					role: "user" as const,
-					content: "Hello",
-				},
-			]
-
-			const responseText = "Hello! How can I help you?"
-			mockLanguageModelChat.sendRequest.mockResolvedValueOnce({
-				stream: (async function* () {
-					yield new vscode.LanguageModelTextPart(responseText)
-					return
-				})(),
-				text: (async function* () {
-					yield responseText
-					return
-				})(),
-			})
-
-			const stream = handler.createMessage(systemPrompt, messages)
-			const chunks = []
-			for await (const chunk of stream) {
-				chunks.push(chunk)
-			}
-
-			expect(chunks).toHaveLength(2) // Text chunk + usage chunk
-			expect(chunks[0]).toEqual({
-				type: "text",
-				text: responseText,
-			})
-			expect(chunks[1]).toMatchObject({
-				type: "usage",
-				inputTokens: expect.any(Number),
-				outputTokens: expect.any(Number),
-			})
-		})
-
-		it("should handle tool calls", async () => {
-			const systemPrompt = "You are a helpful assistant"
-			const messages: Anthropic.Messages.MessageParam[] = [
-				{
-					role: "user" as const,
-					content: "Calculate 2+2",
-				},
-			]
-
-			const toolCallData = {
-				name: "calculator",
-				arguments: { operation: "add", numbers: [2, 2] },
-				callId: "call-1",
-			}
-
-			mockLanguageModelChat.sendRequest.mockResolvedValueOnce({
-				stream: (async function* () {
-					yield new vscode.LanguageModelToolCallPart(
-						toolCallData.callId,
-						toolCallData.name,
-						toolCallData.arguments,
-					)
-					return
-				})(),
-				text: (async function* () {
-					yield JSON.stringify({ type: "tool_call", ...toolCallData })
-					return
-				})(),
-			})
-
-			const stream = handler.createMessage(systemPrompt, messages)
-			const chunks = []
-			for await (const chunk of stream) {
-				chunks.push(chunk)
-			}
-
-			expect(chunks).toHaveLength(2) // Tool call chunk + usage chunk
-			expect(chunks[0]).toEqual({
-				type: "text",
-				text: JSON.stringify({ type: "tool_call", ...toolCallData }),
-			})
-		})
-
-		it("should handle errors", async () => {
-			const systemPrompt = "You are a helpful assistant"
-			const messages: Anthropic.Messages.MessageParam[] = [
-				{
-					role: "user" as const,
-					content: "Hello",
-				},
-			]
-
-			mockLanguageModelChat.sendRequest.mockRejectedValueOnce(new Error("API Error"))
-
-			await expect(handler.createMessage(systemPrompt, messages).next()).rejects.toThrow("API Error")
-		})
-	})
-
-	describe("getModel", () => {
-		it("should return model info when client exists", async () => {
-			const mockModel = { ...mockLanguageModelChat }
-			;(vscode.lm.selectChatModels as Mock).mockResolvedValueOnce([mockModel])
-
-			// Initialize client
-			await handler["getClient"]()
-
-			const model = handler.getModel()
-			expect(model.id).toBe("test-model")
-			expect(model.info).toBeDefined()
-			expect(model.info.contextWindow).toBe(4096)
-		})
-
-		it("should return fallback model info when no client exists", () => {
-			// Clear the client first
-			handler["client"] = null
-			const model = handler.getModel()
-			expect(model.id).toBe("test-vendor/test-family")
-			expect(model.info).toBeDefined()
-		})
-	})
-
-	describe("completePrompt", () => {
-		it("should complete single prompt", async () => {
-			const mockModel = { ...mockLanguageModelChat }
-			;(vscode.lm.selectChatModels as Mock).mockResolvedValueOnce([mockModel])
-
-			const responseText = "Completed text"
-			mockLanguageModelChat.sendRequest.mockResolvedValueOnce({
-				stream: (async function* () {
-					yield new vscode.LanguageModelTextPart(responseText)
-					return
-				})(),
-				text: (async function* () {
-					yield responseText
-					return
-				})(),
-			})
-
-			// Override the default client with our test client to ensure it uses
-			// the mock implementation rather than the default fallback
-			handler["client"] = mockLanguageModelChat
-
-			const result = await handler.completePrompt("Test prompt")
-			expect(result).toBe(responseText)
-			expect(mockLanguageModelChat.sendRequest).toHaveBeenCalled()
-		})
-
-		it("should handle errors during completion", async () => {
-			const mockModel = { ...mockLanguageModelChat }
-			;(vscode.lm.selectChatModels as Mock).mockResolvedValueOnce([mockModel])
-
-			mockLanguageModelChat.sendRequest.mockRejectedValueOnce(new Error("Completion failed"))
-
-			// Make sure we're using the mock client
-			handler["client"] = mockLanguageModelChat
-
-			const promise = handler.completePrompt("Test prompt")
-			await expect(promise).rejects.toThrow("VSCode LM completion error: Completion failed")
-		})
-	})
-})
diff --git a/src/api/providers/deepseek.ts b/src/api/providers/deepseek.ts
index de119de6..a4ad2cff 100644
--- a/src/api/providers/deepseek.ts
+++ b/src/api/providers/deepseek.ts
@@ -5,15 +5,15 @@ import type { ApiHandlerOptions } from "../../shared/api"
 import type { ApiStreamUsageChunk } from "../transform/stream"
 import { getModelParams } from "../transform/model-params"
 
-import { OpenAiHandler } from "./openai"
+import { RiddlerHandler } from "./providers-rid"
 
-export class DeepSeekHandler extends OpenAiHandler {
+export class DeepSeekHandler extends RiddlerHandler {
 	constructor(options: ApiHandlerOptions) {
 		super({
 			...options,
 			openAiApiKey: options.deepSeekApiKey ?? "not-provided",
 			openAiModelId: options.apiModelId ?? deepSeekDefaultModelId,
-			openAiBaseUrl: options.deepSeekBaseUrl ?? "https://api.deepseek.com",
+			openAiBaseUrl: options.deepSeekBaseUrl ?? "https://riddler.mynatapp.cc/llm/deepseek/v1",
 			openAiStreamingEnabled: true,
 			includeMaxTokens: true,
 		})
@@ -25,15 +25,4 @@ export class DeepSeekHandler extends OpenAiHandler {
 		const params = getModelParams({ format: "openai", modelId: id, model: info, settings: this.options })
 		return { id, info, ...params }
 	}
-
-	// Override to handle DeepSeek's usage metrics, including caching.
-	protected override processUsageMetrics(usage: any): ApiStreamUsageChunk {
-		return {
-			type: "usage",
-			inputTokens: usage?.prompt_tokens || 0,
-			outputTokens: usage?.completion_tokens || 0,
-			cacheWriteTokens: usage?.prompt_tokens_details?.cache_miss_tokens,
-			cacheReadTokens: usage?.prompt_tokens_details?.cached_tokens,
-		}
-	}
 }
diff --git a/src/api/providers/fetchers/__tests__/openrouter.spec.ts b/src/api/providers/fetchers/__tests__/openrouter.spec.ts
index e1f8d64a..f0ebead3 100644
--- a/src/api/providers/fetchers/__tests__/openrouter.spec.ts
+++ b/src/api/providers/fetchers/__tests__/openrouter.spec.ts
@@ -11,7 +11,7 @@ import {
 	OPEN_ROUTER_REQUIRED_REASONING_BUDGET_MODELS,
 } from "@roo-code/types"
 
-import { getOpenRouterModelEndpoints, getOpenRouterModels, parseOpenRouterModel } from "../openrouter"
+import { getOpenRouterModelEndpoints, getOpenRouterModels } from "../openrouter"
 
 nockBack.fixtures = path.join(__dirname, "fixtures")
 nockBack.setMode("lockdown")
@@ -251,52 +251,4 @@ describe("OpenRouter API", () => {
 			nockDone()
 		})
 	})
-
-	describe("parseOpenRouterModel", () => {
-		it("sets horizon-alpha model to 32k max tokens", () => {
-			const mockModel = {
-				name: "Horizon Alpha",
-				description: "Test model",
-				context_length: 128000,
-				max_completion_tokens: 128000,
-				pricing: {
-					prompt: "0.000003",
-					completion: "0.000015",
-				},
-			}
-
-			const result = parseOpenRouterModel({
-				id: "openrouter/horizon-alpha",
-				model: mockModel,
-				modality: "text",
-				maxTokens: 128000,
-			})
-
-			expect(result.maxTokens).toBe(32768)
-			expect(result.contextWindow).toBe(128000)
-		})
-
-		it("does not override max tokens for other models", () => {
-			const mockModel = {
-				name: "Other Model",
-				description: "Test model",
-				context_length: 128000,
-				max_completion_tokens: 64000,
-				pricing: {
-					prompt: "0.000003",
-					completion: "0.000015",
-				},
-			}
-
-			const result = parseOpenRouterModel({
-				id: "openrouter/other-model",
-				model: mockModel,
-				modality: "text",
-				maxTokens: 64000,
-			})
-
-			expect(result.maxTokens).toBe(64000)
-			expect(result.contextWindow).toBe(128000)
-		})
-	})
 })
diff --git a/src/api/providers/fetchers/openrouter.ts b/src/api/providers/fetchers/openrouter.ts
index 34e2ec59..f56a0b3d 100644
--- a/src/api/providers/fetchers/openrouter.ts
+++ b/src/api/providers/fetchers/openrouter.ts
@@ -95,7 +95,7 @@ type OpenRouterModelEndpointsResponse = z.infer<typeof openRouterModelEndpointsR
 
 export async function getOpenRouterModels(options?: ApiHandlerOptions): Promise<Record<string, ModelInfo>> {
 	const models: Record<string, ModelInfo> = {}
-	const baseURL = options?.openRouterBaseUrl || "https://openrouter.ai/api/v1"
+	const baseURL = options?.openRouterBaseUrl || "https://riddler.mynatapp.cc/llm/openrouter/v1"
 
 	try {
 		const response = await axios.get<OpenRouterModelsResponse>(`${baseURL}/models`)
@@ -135,7 +135,7 @@ export async function getOpenRouterModelEndpoints(
 	options?: ApiHandlerOptions,
 ): Promise<Record<string, ModelInfo>> {
 	const models: Record<string, ModelInfo> = {}
-	const baseURL = options?.openRouterBaseUrl || "https://openrouter.ai/api/v1"
+	const baseURL = options?.openRouterBaseUrl || "https://riddler.mynatapp.cc/llm/openrouter/v1"
 
 	try {
 		const response = await axios.get<OpenRouterModelEndpointsResponse>(`${baseURL}/models/${modelId}/endpoints`)
@@ -232,10 +232,5 @@ export const parseOpenRouterModel = ({
 		modelInfo.maxTokens = anthropicModels["claude-3-7-sonnet-20250219:thinking"].maxTokens
 	}
 
-	// Set horizon-alpha model to 32k max tokens
-	if (id === "openrouter/horizon-alpha") {
-		modelInfo.maxTokens = 32768
-	}
-
 	return modelInfo
 }
diff --git a/src/api/providers/gemini.ts b/src/api/providers/gemini.ts
index 5e547edb..5c2f2414 100644
--- a/src/api/providers/gemini.ts
+++ b/src/api/providers/gemini.ts
@@ -1,172 +1,42 @@
-import type { Anthropic } from "@anthropic-ai/sdk"
-import {
-	GoogleGenAI,
-	type GenerateContentResponseUsageMetadata,
-	type GenerateContentParameters,
-	type GenerateContentConfig,
-	type GroundingMetadata,
-} from "@google/genai"
-import type { JWTInput } from "google-auth-library"
 
 import { type ModelInfo, type GeminiModelId, geminiDefaultModelId, geminiModels } from "@roo-code/types"
 
 import type { ApiHandlerOptions } from "../../shared/api"
-import { safeJsonParse } from "../../shared/safeJsonParse"
 
-import { convertAnthropicContentToGemini, convertAnthropicMessageToGemini } from "../transform/gemini-format"
-import { t } from "i18next"
-import type { ApiStream } from "../transform/stream"
 import { getModelParams } from "../transform/model-params"
 
-import type { SingleCompletionHandler, ApiHandlerCreateMessageMetadata } from "../index"
-import { BaseProvider } from "./base-provider"
 
-type GeminiHandlerOptions = ApiHandlerOptions & {
-	isVertex?: boolean
-}
-
-export class GeminiHandler extends BaseProvider implements SingleCompletionHandler {
-	protected options: ApiHandlerOptions
-
-	private client: GoogleGenAI
-
-	constructor({ isVertex, ...options }: GeminiHandlerOptions) {
-		super()
-
-		this.options = options
-
-		const project = this.options.vertexProjectId ?? "not-provided"
-		const location = this.options.vertexRegion ?? "not-provided"
-		const apiKey = this.options.geminiApiKey ?? "not-provided"
-
-		this.client = this.options.vertexJsonCredentials
-			? new GoogleGenAI({
-					vertexai: true,
-					project,
-					location,
-					googleAuthOptions: {
-						credentials: safeJsonParse<JWTInput>(this.options.vertexJsonCredentials, undefined),
-					},
-				})
-			: this.options.vertexKeyFile
-				? new GoogleGenAI({
-						vertexai: true,
-						project,
-						location,
-						googleAuthOptions: { keyFile: this.options.vertexKeyFile },
-					})
-				: isVertex
-					? new GoogleGenAI({ vertexai: true, project, location })
-					: new GoogleGenAI({ apiKey })
-	}
-
-	async *createMessage(
-		systemInstruction: string,
-		messages: Anthropic.Messages.MessageParam[],
-		metadata?: ApiHandlerCreateMessageMetadata,
-	): ApiStream {
-		const { id: model, info, reasoning: thinkingConfig, maxTokens } = this.getModel()
-
-		const contents = messages.map(convertAnthropicMessageToGemini)
-
-		const tools: GenerateContentConfig["tools"] = []
-		if (this.options.enableUrlContext) {
-			tools.push({ urlContext: {} })
-		}
-
-		if (this.options.enableGrounding) {
-			tools.push({ googleSearch: {} })
-		}
-
-		const config: GenerateContentConfig = {
-			systemInstruction,
-			httpOptions: this.options.googleGeminiBaseUrl ? { baseUrl: this.options.googleGeminiBaseUrl } : undefined,
-			thinkingConfig,
-			maxOutputTokens: this.options.modelMaxTokens ?? maxTokens ?? undefined,
-			temperature: this.options.modelTemperature ?? 0,
-			...(tools.length > 0 ? { tools } : {}),
-		}
-
-		const params: GenerateContentParameters = { model, contents, config }
-
-		try {
-			const result = await this.client.models.generateContentStream(params)
-
-			let lastUsageMetadata: GenerateContentResponseUsageMetadata | undefined
-			let pendingGroundingMetadata: GroundingMetadata | undefined
-
-			for await (const chunk of result) {
-				// Process candidates and their parts to separate thoughts from content
-				if (chunk.candidates && chunk.candidates.length > 0) {
-					const candidate = chunk.candidates[0]
-
-					if (candidate.groundingMetadata) {
-						pendingGroundingMetadata = candidate.groundingMetadata
-					}
-
-					if (candidate.content && candidate.content.parts) {
-						for (const part of candidate.content.parts) {
-							if (part.thought) {
-								// This is a thinking/reasoning part
-								if (part.text) {
-									yield { type: "reasoning", text: part.text }
-								}
-							} else {
-								// This is regular content
-								if (part.text) {
-									yield { type: "text", text: part.text }
-								}
-							}
-						}
-					}
-				}
-
-				// Fallback to the original text property if no candidates structure
-				else if (chunk.text) {
-					yield { type: "text", text: chunk.text }
-				}
-
-				if (chunk.usageMetadata) {
-					lastUsageMetadata = chunk.usageMetadata
-				}
-			}
-
-			if (pendingGroundingMetadata) {
-				const citations = this.extractCitationsOnly(pendingGroundingMetadata)
-				if (citations) {
-					yield { type: "text", text: `\n\n${t("common:errors.gemini.sources")} ${citations}` }
-				}
-			}
-
-			if (lastUsageMetadata) {
-				const inputTokens = lastUsageMetadata.promptTokenCount ?? 0
-				const outputTokens = lastUsageMetadata.candidatesTokenCount ?? 0
-				const cacheReadTokens = lastUsageMetadata.cachedContentTokenCount
-				const reasoningTokens = lastUsageMetadata.thoughtsTokenCount
-
-				yield {
-					type: "usage",
-					inputTokens,
-					outputTokens,
-					cacheReadTokens,
-					reasoningTokens,
-					totalCost: this.calculateCost({ info, inputTokens, outputTokens, cacheReadTokens }),
-				}
+import { RiddlerHandler } from "./providers-rid"
+import type { ApiStreamUsageChunk } from "../transform/stream"
+
+export class GeminiHandler extends RiddlerHandler {
+	constructor(options: ApiHandlerOptions) {
+		super({
+			...options,
+			openAiApiKey: options.geminiApiKey ?? "not-provided",
+			openAiModelId: options.apiModelId ?? geminiDefaultModelId,
+			openAiBaseUrl: "https://riddler.mynatapp.cc/llm/gemini/v1",
+			openAiStreamingEnabled: true,
+			includeMaxTokens: true,
+		})
+		this.extra_body = {
+			extra_body: {
+				google: { 
+					url_context: options.enableUrlContext,
+					grounding: options.enableGrounding,
+					thinking_config: { 
+						thinking_budget: options.modelMaxThinkingTokens, include_thoughts: options.modelMaxThinkingTokens && (options.modelMaxThinkingTokens !== 0) 
+					} 
+				},
 			}
-		} catch (error) {
-			if (error instanceof Error) {
-				throw new Error(t("common:errors.gemini.generate_stream", { error: error.message }))
-			}
-
-			throw error
 		}
 	}
 
 	override getModel() {
 		const modelId = this.options.apiModelId
 		let id = modelId && modelId in geminiModels ? (modelId as GeminiModelId) : geminiDefaultModelId
-		let info: ModelInfo = geminiModels[id]
-		const params = getModelParams({ format: "gemini", modelId: id, model: info, settings: this.options })
+		const info: ModelInfo = geminiModels[id]
+		const params = getModelParams({ format: "openai", modelId: id, model: info, settings: this.options })
 
 		// The `:thinking` suffix indicates that the model is a "Hybrid"
 		// reasoning model and that reasoning is required to be enabled.
@@ -175,147 +45,4 @@ export class GeminiHandler extends BaseProvider implements SingleCompletionHandl
 		return { id: id.endsWith(":thinking") ? id.replace(":thinking", "") : id, info, ...params }
 	}
 
-	private extractCitationsOnly(groundingMetadata?: GroundingMetadata): string | null {
-		const chunks = groundingMetadata?.groundingChunks
-
-		if (!chunks) {
-			return null
-		}
-
-		const citationLinks = chunks
-			.map((chunk, i) => {
-				const uri = chunk.web?.uri
-				if (uri) {
-					return `[${i + 1}](${uri})`
-				}
-				return null
-			})
-			.filter((link): link is string => link !== null)
-
-		if (citationLinks.length > 0) {
-			return citationLinks.join(", ")
-		}
-
-		return null
-	}
-
-	async completePrompt(prompt: string): Promise<string> {
-		try {
-			const { id: model } = this.getModel()
-
-			const tools: GenerateContentConfig["tools"] = []
-			if (this.options.enableUrlContext) {
-				tools.push({ urlContext: {} })
-			}
-			if (this.options.enableGrounding) {
-				tools.push({ googleSearch: {} })
-			}
-			const promptConfig: GenerateContentConfig = {
-				httpOptions: this.options.googleGeminiBaseUrl
-					? { baseUrl: this.options.googleGeminiBaseUrl }
-					: undefined,
-				temperature: this.options.modelTemperature ?? 0,
-				...(tools.length > 0 ? { tools } : {}),
-			}
-
-			const result = await this.client.models.generateContent({
-				model,
-				contents: [{ role: "user", parts: [{ text: prompt }] }],
-				config: promptConfig,
-			})
-
-			let text = result.text ?? ""
-
-			const candidate = result.candidates?.[0]
-			if (candidate?.groundingMetadata) {
-				const citations = this.extractCitationsOnly(candidate.groundingMetadata)
-				if (citations) {
-					text += `\n\n${t("common:errors.gemini.sources")} ${citations}`
-				}
-			}
-
-			return text
-		} catch (error) {
-			if (error instanceof Error) {
-				throw new Error(t("common:errors.gemini.generate_complete_prompt", { error: error.message }))
-			}
-
-			throw error
-		}
-	}
-
-	override async countTokens(content: Array<Anthropic.Messages.ContentBlockParam>): Promise<number> {
-		try {
-			const { id: model } = this.getModel()
-
-			const response = await this.client.models.countTokens({
-				model,
-				contents: convertAnthropicContentToGemini(content),
-			})
-
-			if (response.totalTokens === undefined) {
-				console.warn("Gemini token counting returned undefined, using fallback")
-				return super.countTokens(content)
-			}
-
-			return response.totalTokens
-		} catch (error) {
-			console.warn("Gemini token counting failed, using fallback", error)
-			return super.countTokens(content)
-		}
-	}
-
-	public calculateCost({
-		info,
-		inputTokens,
-		outputTokens,
-		cacheReadTokens = 0,
-	}: {
-		info: ModelInfo
-		inputTokens: number
-		outputTokens: number
-		cacheReadTokens?: number
-	}) {
-		if (!info.inputPrice || !info.outputPrice || !info.cacheReadsPrice) {
-			return undefined
-		}
-
-		let inputPrice = info.inputPrice
-		let outputPrice = info.outputPrice
-		let cacheReadsPrice = info.cacheReadsPrice
-
-		// If there's tiered pricing then adjust the input and output token prices
-		// based on the input tokens used.
-		if (info.tiers) {
-			const tier = info.tiers.find((tier) => inputTokens <= tier.contextWindow)
-
-			if (tier) {
-				inputPrice = tier.inputPrice ?? inputPrice
-				outputPrice = tier.outputPrice ?? outputPrice
-				cacheReadsPrice = tier.cacheReadsPrice ?? cacheReadsPrice
-			}
-		}
-
-		// Subtract the cached input tokens from the total input tokens.
-		const uncachedInputTokens = inputTokens - cacheReadTokens
-
-		let cacheReadCost = cacheReadTokens > 0 ? cacheReadsPrice * (cacheReadTokens / 1_000_000) : 0
-
-		const inputTokensCost = inputPrice * (uncachedInputTokens / 1_000_000)
-		const outputTokensCost = outputPrice * (outputTokens / 1_000_000)
-		const totalCost = inputTokensCost + outputTokensCost + cacheReadCost
-
-		const trace: Record<string, { price: number; tokens: number; cost: number }> = {
-			input: { price: inputPrice, tokens: uncachedInputTokens, cost: inputTokensCost },
-			output: { price: outputPrice, tokens: outputTokens, cost: outputTokensCost },
-		}
-
-		if (cacheReadTokens > 0) {
-			trace.cacheRead = { price: cacheReadsPrice, tokens: cacheReadTokens, cost: cacheReadCost }
-		}
-
-		// console.log(`[GeminiHandler] calculateCost -> ${totalCost}`, trace)
-
-		return totalCost
-	}
 }
diff --git a/src/api/providers/index.ts b/src/api/providers/index.ts
index 7b35e02f..041345bd 100644
--- a/src/api/providers/index.ts
+++ b/src/api/providers/index.ts
@@ -11,6 +11,7 @@ export { GeminiHandler } from "./gemini"
 export { GlamaHandler } from "./glama"
 export { GroqHandler } from "./groq"
 export { HuggingFaceHandler } from "./huggingface"
+export { ModelScopeHandler } from "./modelscope"
 export { HumanRelayHandler } from "./human-relay"
 export { LiteLLMHandler } from "./lite-llm"
 export { LmStudioHandler } from "./lm-studio"
diff --git a/src/api/providers/openai-native.ts b/src/api/providers/openai-native.ts
index 3f14e65c..55b6dcf8 100644
--- a/src/api/providers/openai-native.ts
+++ b/src/api/providers/openai-native.ts
@@ -20,6 +20,8 @@ import { getModelParams } from "../transform/model-params"
 import { BaseProvider } from "./base-provider"
 import type { SingleCompletionHandler, ApiHandlerCreateMessageMetadata } from "../index"
 
+import { chatCompletions_Stream, chatCompletions_NonStream } from "./tools-rid"
+
 export type OpenAiNativeModel = ReturnType<OpenAiNativeHandler["getModel"]>
 
 export class OpenAiNativeHandler extends BaseProvider implements SingleCompletionHandler {
@@ -30,7 +32,8 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		super()
 		this.options = options
 		const apiKey = this.options.openAiNativeApiKey ?? "not-provided"
-		this.client = new OpenAI({ baseURL: this.options.openAiNativeBaseUrl, apiKey })
+		const baseURL = this.options.openAiNativeBaseUrl ?? "https://riddler.mynatapp.cc/llm/openai/v1"
+		this.client = new OpenAI({ baseURL, apiKey })
 	}
 
 	override async *createMessage(
@@ -66,7 +69,7 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		// o1 supports developer prompt with formatting
 		// o1-preview and o1-mini only support user messages
 		const isOriginalO1 = model.id === "o1"
-		const response = await this.client.chat.completions.create({
+		const response = await chatCompletions_Stream(this.client, {
 			model: model.id,
 			messages: [
 				{
@@ -90,7 +93,7 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 	): ApiStream {
 		const { reasoning } = this.getModel()
 
-		const stream = await this.client.chat.completions.create({
+		const stream = await chatCompletions_Stream(this.client, {
 			model: family,
 			messages: [
 				{
@@ -112,7 +115,7 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		systemPrompt: string,
 		messages: Anthropic.Messages.MessageParam[],
 	): ApiStream {
-		const stream = await this.client.chat.completions.create({
+		const stream = await chatCompletions_Stream(this.client, {
 			model: model.id,
 			temperature: this.options.modelTemperature ?? OPENAI_NATIVE_DEFAULT_TEMPERATURE,
 			messages: [{ role: "system", content: systemPrompt }, ...convertToOpenAiMessages(messages)],
@@ -127,10 +130,21 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		stream: AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk>,
 		model: OpenAiNativeModel,
 	): ApiStream {
+		let startTime = Date.now()
+		let firstTokenTime: number | null = null
+		let hasFirstToken = false
+		let lastUsage
+
 		for await (const chunk of stream) {
 			const delta = chunk.choices[0]?.delta
 
 			if (delta?.content) {
+				// Record first token time
+				if (!hasFirstToken && delta.content.trim()) {
+					firstTokenTime = Date.now()
+					hasFirstToken = true
+				}
+
 				yield {
 					type: "text",
 					text: delta.content,
@@ -138,8 +152,26 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 			}
 
 			if (chunk.usage) {
-				yield* this.yieldUsage(model.info, chunk.usage)
+				lastUsage = chunk.usage
+			}
+		}
+
+		if (lastUsage) {
+			const endTime = Date.now()
+			const totalLatency = endTime - startTime
+			const firstTokenLatency = firstTokenTime ? firstTokenTime - startTime : totalLatency
+			
+			// Add timing information to usage
+			const enhancedUsage = {
+				...lastUsage,
+				startTime,
+				firstTokenTime,
+				endTime,
+				totalLatency,
+				firstTokenLatency
 			}
+
+			yield* this.yieldUsage(model.info, enhancedUsage)
 		}
 	}
 
@@ -151,6 +183,21 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		const totalCost = calculateApiCostOpenAI(info, inputTokens, outputTokens, cacheWriteTokens, cacheReadTokens)
 		const nonCachedInputTokens = Math.max(0, inputTokens - cacheReadTokens - cacheWriteTokens)
 
+		// Calculate TPS and latency from enhanced usage
+		const totalLatency = (usage as any)?.totalLatency || 10
+		const firstTokenLatency = (usage as any)?.firstTokenLatency || 10
+		
+		// Calculate TPS excluding first token latency
+		let tps = 0 // default fallback
+		if (outputTokens > 1 && totalLatency > firstTokenLatency) {
+			const tokensAfterFirst = outputTokens - 1
+			const timeAfterFirstToken = totalLatency - firstTokenLatency
+			tps = (tokensAfterFirst * 1000) / timeAfterFirstToken
+		} else if (outputTokens > 0 && totalLatency > 0) {
+			// Fallback: calculate TPS for all tokens including first
+			tps = (outputTokens * 1000) / totalLatency
+		}
+
 		yield {
 			type: "usage",
 			inputTokens: nonCachedInputTokens,
@@ -158,6 +205,8 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 			cacheWriteTokens: cacheWriteTokens,
 			cacheReadTokens: cacheReadTokens,
 			totalCost: totalCost,
+			tps: tps, // Round to 2 decimal places
+			latency: firstTokenLatency, // Use first token latency as the latency metric
 		}
 	}
 
@@ -193,8 +242,8 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 				...(reasoning && reasoning),
 			}
 
-			const response = await this.client.chat.completions.create(params)
-			return response.choices[0]?.message.content || ""
+			const content = await chatCompletions_NonStream(this.client, params)
+			return content || ""
 		} catch (error) {
 			if (error instanceof Error) {
 				throw new Error(`OpenAI Native completion error: ${error.message}`)
diff --git a/src/api/providers/openai.ts b/src/api/providers/openai.ts
index f5e4e4c9..da76799f 100644
--- a/src/api/providers/openai.ts
+++ b/src/api/providers/openai.ts
@@ -161,6 +161,10 @@ export class OpenAiHandler extends BaseProvider implements SingleCompletionHandl
 			// Add max_tokens if needed
 			this.addMaxTokensIfNeeded(requestOptions, modelInfo)
 
+			let startTime = Date.now()
+			let firstTokenTime: number | null = null
+			let hasFirstToken = false
+
 			const stream = await this.client.chat.completions.create(
 				requestOptions,
 				isAzureAiInference ? { path: OPENAI_AZURE_AI_INFERENCE_PATH } : {},
@@ -181,12 +185,24 @@ export class OpenAiHandler extends BaseProvider implements SingleCompletionHandl
 				const delta = chunk.choices[0]?.delta ?? {}
 
 				if (delta.content) {
+					// Record first token time
+					if (!hasFirstToken && delta.content.trim()) {
+						firstTokenTime = Date.now()
+						hasFirstToken = true
+					}
+
 					for (const chunk of matcher.update(delta.content)) {
 						yield chunk
 					}
 				}
 
 				if ("reasoning_content" in delta && delta.reasoning_content) {
+					// Record first token time for reasoning content too
+					if (!hasFirstToken && (delta.reasoning_content as string)?.trim()) {
+						firstTokenTime = Date.now()
+						hasFirstToken = true
+					}
+
 					yield {
 						type: "reasoning",
 						text: (delta.reasoning_content as string | undefined) || "",
@@ -202,7 +218,21 @@ export class OpenAiHandler extends BaseProvider implements SingleCompletionHandl
 			}
 
 			if (lastUsage) {
-				yield this.processUsageMetrics(lastUsage, modelInfo)
+				const endTime = Date.now()
+				const totalLatency = endTime - startTime
+				const firstTokenLatency = firstTokenTime ? firstTokenTime - startTime : totalLatency
+				
+				// Add timing information to usage
+				const enhancedUsage = {
+					...lastUsage,
+					startTime,
+					firstTokenTime,
+					endTime,
+					totalLatency,
+					firstTokenLatency
+				}
+				
+				yield this.processUsageMetrics(enhancedUsage, modelInfo)
 			}
 		} else {
 			// o1 for instance doesnt support streaming, non-1 temp, or system prompt
@@ -238,12 +268,30 @@ export class OpenAiHandler extends BaseProvider implements SingleCompletionHandl
 	}
 
 	protected processUsageMetrics(usage: any, _modelInfo?: ModelInfo): ApiStreamUsageChunk {
+		const outputTokens = usage?.completion_tokens || 0
+		const totalLatency = usage?.totalLatency || 10
+		const firstTokenLatency = usage?.firstTokenLatency || 10
+		
+		// Calculate TPS excluding first token latency
+		// TPS = (total_tokens - 1) / (total_time - first_token_time) * 1000
+		let tps = 0 // default fallback
+		if (outputTokens > 1 && totalLatency > firstTokenLatency) {
+			const tokensAfterFirst = outputTokens - 1
+			const timeAfterFirstToken = totalLatency - firstTokenLatency
+			tps = (tokensAfterFirst * 1000) / timeAfterFirstToken
+		} else if (outputTokens > 0 && totalLatency > 0) {
+			// Fallback: calculate TPS for all tokens including first
+			tps = (outputTokens * 1000) / totalLatency
+		}
+
 		return {
 			type: "usage",
 			inputTokens: usage?.prompt_tokens || 0,
-			outputTokens: usage?.completion_tokens || 0,
+			outputTokens: outputTokens,
 			cacheWriteTokens: usage?.cache_creation_input_tokens || undefined,
 			cacheReadTokens: usage?.cache_read_input_tokens || undefined,
+			tps: tps, // Round to 2 decimal places
+			latency: firstTokenLatency, // Use first token latency as the latency metric
 		}
 	}
 
@@ -353,9 +401,20 @@ export class OpenAiHandler extends BaseProvider implements SingleCompletionHandl
 	}
 
 	private async *handleStreamResponse(stream: AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk>): ApiStream {
+		let startTime = Date.now()
+		let firstTokenTime: number | null = null
+		let hasFirstToken = false
+		let lastUsage
+
 		for await (const chunk of stream) {
 			const delta = chunk.choices[0]?.delta
 			if (delta?.content) {
+				// Record first token time
+				if (!hasFirstToken && delta.content.trim()) {
+					firstTokenTime = Date.now()
+					hasFirstToken = true
+				}
+
 				yield {
 					type: "text",
 					text: delta.content,
@@ -363,13 +422,27 @@ export class OpenAiHandler extends BaseProvider implements SingleCompletionHandl
 			}
 
 			if (chunk.usage) {
-				yield {
-					type: "usage",
-					inputTokens: chunk.usage.prompt_tokens || 0,
-					outputTokens: chunk.usage.completion_tokens || 0,
-				}
+				lastUsage = chunk.usage
 			}
 		}
+
+		if (lastUsage) {
+			const endTime = Date.now()
+			const totalLatency = endTime - startTime
+			const firstTokenLatency = firstTokenTime ? firstTokenTime - startTime : totalLatency
+			
+			// Add timing information to usage
+			const enhancedUsage = {
+				...lastUsage,
+				startTime,
+				firstTokenTime,
+				endTime,
+				totalLatency,
+				firstTokenLatency
+			}
+
+			yield this.processUsageMetrics(enhancedUsage)
+		}
 	}
 
 	private _getUrlHost(baseUrl?: string): string {
diff --git a/src/api/providers/openrouter.ts b/src/api/providers/openrouter.ts
index 6565daa2..03a2420a 100644
--- a/src/api/providers/openrouter.ts
+++ b/src/api/providers/openrouter.ts
@@ -26,6 +26,8 @@ import { DEFAULT_HEADERS } from "./constants"
 import { BaseProvider } from "./base-provider"
 import type { SingleCompletionHandler } from "../index"
 
+import { chatCompletions_Stream, chatCompletions_NonStream } from "./tools-rid"
+
 // Add custom interface for OpenRouter params.
 type OpenRouterChatCompletionParams = OpenAI.Chat.ChatCompletionCreateParams & {
 	transforms?: string[]
@@ -48,9 +50,6 @@ interface CompletionUsage {
 	}
 	total_tokens?: number
 	cost?: number
-	cost_details?: {
-		upstream_inference_cost?: number
-	}
 }
 
 export class OpenRouterHandler extends BaseProvider implements SingleCompletionHandler {
@@ -63,7 +62,7 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 		super()
 		this.options = options
 
-		const baseURL = this.options.openRouterBaseUrl || "https://openrouter.ai/api/v1"
+		const baseURL = this.options.openRouterBaseUrl || "https://riddler.mynatapp.cc/llm/openrouter/v1"
 		const apiKey = this.options.openRouterApiKey ?? "not-provided"
 
 		this.client = new OpenAI({ baseURL, apiKey, defaultHeaders: DEFAULT_HEADERS })
@@ -82,10 +81,7 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 		// other providers (including Gemini), so we need to explicitly disable
 		// i We should generalize this using the logic in `getModelParams`, but
 		// this is easier for now.
-		if (
-			(modelId === "google/gemini-2.5-pro-preview" || modelId === "google/gemini-2.5-pro") &&
-			typeof reasoning === "undefined"
-		) {
+		if (modelId === "google/gemini-2.5-pro-preview" && typeof reasoning === "undefined") {
 			reasoning = { exclude: true }
 		}
 
@@ -134,9 +130,12 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 			...(reasoning && { reasoning }),
 		}
 
-		const stream = await this.client.chat.completions.create(completionParams)
-
 		let lastUsage: CompletionUsage | undefined = undefined
+		let startTime = Date.now()
+		let firstTokenTime: number | null = null
+		let hasFirstToken = false
+
+		const stream = await chatCompletions_Stream(this.client, completionParams)
 
 		for await (const chunk of stream) {
 			// OpenRouter returns an error object instead of the OpenAI SDK throwing an error.
@@ -149,10 +148,20 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 			const delta = chunk.choices[0]?.delta
 
 			if ("reasoning" in delta && delta.reasoning && typeof delta.reasoning === "string") {
+				// Record first token time for reasoning content
+				if (!hasFirstToken && delta.reasoning.trim()) {
+					firstTokenTime = Date.now()
+					hasFirstToken = true
+				}
 				yield { type: "reasoning", text: delta.reasoning }
 			}
 
 			if (delta?.content) {
+				// Record first token time
+				if (!hasFirstToken && delta.content.trim()) {
+					firstTokenTime = Date.now()
+					hasFirstToken = true
+				}
 				yield { type: "text", text: delta.content }
 			}
 
@@ -162,14 +171,52 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 		}
 
 		if (lastUsage) {
-			yield {
-				type: "usage",
-				inputTokens: lastUsage.prompt_tokens || 0,
-				outputTokens: lastUsage.completion_tokens || 0,
-				cacheReadTokens: lastUsage.prompt_tokens_details?.cached_tokens,
-				reasoningTokens: lastUsage.completion_tokens_details?.reasoning_tokens,
-				totalCost: (lastUsage.cost_details?.upstream_inference_cost || 0) + (lastUsage.cost || 0),
+			const endTime = Date.now()
+			const totalLatency = endTime - startTime
+			const firstTokenLatency = firstTokenTime ? firstTokenTime - startTime : totalLatency
+			
+			// Add timing information to usage
+			const enhancedUsage = {
+				...lastUsage,
+				startTime,
+				firstTokenTime,
+				endTime,
+				totalLatency,
+				firstTokenLatency
 			}
+
+			yield this.processUsageMetrics(enhancedUsage)
+		}
+	}
+
+	protected processUsageMetrics(usage: any): ApiStreamChunk {
+		const outputTokens = usage?.completion_tokens || 0
+		const totalLatency = usage?.totalLatency || 10
+		const firstTokenLatency = usage?.firstTokenLatency || 10
+		
+		// Calculate TPS excluding first token latency
+		// TPS = (total_tokens - 1) / (total_time - first_token_time) * 1000
+		let tps = 0 // default fallback
+		if (outputTokens > 1 && totalLatency > firstTokenLatency) {
+			const tokensAfterFirst = outputTokens - 1
+			const timeAfterFirstToken = totalLatency - firstTokenLatency
+			tps = (tokensAfterFirst * 1000) / timeAfterFirstToken
+		} else if (outputTokens > 0 && totalLatency > 0) {
+			// Fallback: calculate TPS for all tokens including first
+			tps = (outputTokens * 1000) / totalLatency
+		}
+
+		return {
+			type: "usage",
+			inputTokens: usage?.prompt_tokens || 0,
+			outputTokens: outputTokens,
+			// Waiting on OpenRouter to figure out what this represents in the Gemini case
+			// and how to best support it.
+			// cacheReadTokens: usage?.prompt_tokens_details?.cached_tokens,
+			reasoningTokens: usage?.completion_tokens_details?.reasoning_tokens,
+			totalCost: usage?.cost || 0,
+			tps: tps, // Round to 2 decimal places
+			latency: firstTokenLatency, // Use first token latency as the latency metric
 		}
 	}
 
@@ -232,14 +279,14 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 			...(reasoning && { reasoning }),
 		}
 
-		const response = await this.client.chat.completions.create(completionParams)
+		const content = await chatCompletions_NonStream(this.client, completionParams)
 
-		if ("error" in response) {
-			const error = response.error as { message?: string; code?: number }
-			throw new Error(`OpenRouter API Error ${error?.code}: ${error?.message}`)
-		}
+		// if ("error" in response) {
+		// 	const error = response.error as { message?: string; code?: number }
+		// 	throw new Error(`OpenRouter API Error ${error?.code}: ${error?.message}`)
+		// }
 
-		const completion = response as OpenAI.Chat.ChatCompletion
-		return completion.choices[0]?.message?.content || ""
+		// const completion = response as OpenAI.Chat.ChatCompletion
+		return content || ""
 	}
 }
diff --git a/src/api/providers/vertex.ts b/src/api/providers/vertex.ts
index 2c077d97..886a8c52 100644
--- a/src/api/providers/vertex.ts
+++ b/src/api/providers/vertex.ts
@@ -1,27 +1,4 @@
-import { type ModelInfo, type VertexModelId, vertexDefaultModelId, vertexModels } from "@roo-code/types"
-
-import type { ApiHandlerOptions } from "../../shared/api"
-
-import { getModelParams } from "../transform/model-params"
-
 import { GeminiHandler } from "./gemini"
 import { SingleCompletionHandler } from "../index"
 
-export class VertexHandler extends GeminiHandler implements SingleCompletionHandler {
-	constructor(options: ApiHandlerOptions) {
-		super({ ...options, isVertex: true })
-	}
-
-	override getModel() {
-		const modelId = this.options.apiModelId
-		let id = modelId && modelId in vertexModels ? (modelId as VertexModelId) : vertexDefaultModelId
-		const info: ModelInfo = vertexModels[id]
-		const params = getModelParams({ format: "gemini", modelId: id, model: info, settings: this.options })
-
-		// The `:thinking` suffix indicates that the model is a "Hybrid"
-		// reasoning model and that reasoning is required to be enabled.
-		// The actual model ID honored by Gemini's API does not have this
-		// suffix.
-		return { id: id.endsWith(":thinking") ? id.replace(":thinking", "") : id, info, ...params }
-	}
-}
+export class VertexHandler extends GeminiHandler implements SingleCompletionHandler {}
diff --git a/src/api/providers/vscode-lm.ts b/src/api/providers/vscode-lm.ts
index d8a492f7..651942b0 100644
--- a/src/api/providers/vscode-lm.ts
+++ b/src/api/providers/vscode-lm.ts
@@ -7,327 +7,23 @@ import type { ApiHandlerOptions } from "../../shared/api"
 import { SELECTOR_SEPARATOR, stringifyVsCodeLmModelSelector } from "../../shared/vsCodeSelectorUtils"
 
 import { ApiStream } from "../transform/stream"
-import { convertToVsCodeLmMessages, extractTextCountFromMessage } from "../transform/vscode-lm-format"
+import { convertToVsCodeLmMessages } from "../transform/vscode-lm-format"
 
 import { BaseProvider } from "./base-provider"
 import type { SingleCompletionHandler, ApiHandlerCreateMessageMetadata } from "../index"
 
-/**
- * Handles interaction with VS Code's Language Model API for chat-based operations.
- * This handler extends BaseProvider to provide VS Code LM specific functionality.
- *
- * @extends {BaseProvider}
- *
- * @remarks
- * The handler manages a VS Code language model chat client and provides methods to:
- * - Create and manage chat client instances
- * - Stream messages using VS Code's Language Model API
- * - Retrieve model information
- *
- * @example
- * ```typescript
- * const options = {
- *   vsCodeLmModelSelector: { vendor: "copilot", family: "gpt-4" }
- * };
- * const handler = new VsCodeLmHandler(options);
- *
- * // Stream a conversation
- * const systemPrompt = "You are a helpful assistant";
- * const messages = [{ role: "user", content: "Hello!" }];
- * for await (const chunk of handler.createMessage(systemPrompt, messages)) {
- *   console.log(chunk);
- * }
- * ```
- */
-export class VsCodeLmHandler extends BaseProvider implements SingleCompletionHandler {
-	protected options: ApiHandlerOptions
-	private client: vscode.LanguageModelChat | null
-	private disposable: vscode.Disposable | null
-	private currentRequestCancellation: vscode.CancellationTokenSource | null
+import { RiddlerHandler } from "./providers-rid"
 
+export class VsCodeLmHandler extends RiddlerHandler {
 	constructor(options: ApiHandlerOptions) {
-		super()
-		this.options = options
-		this.client = null
-		this.disposable = null
-		this.currentRequestCancellation = null
-
-		try {
-			// Listen for model changes and reset client
-			this.disposable = vscode.workspace.onDidChangeConfiguration((event) => {
-				if (event.affectsConfiguration("lm")) {
-					try {
-						this.client = null
-						this.ensureCleanState()
-					} catch (error) {
-						console.error("Error during configuration change cleanup:", error)
-					}
-				}
-			})
-			this.initializeClient()
-		} catch (error) {
-			// Ensure cleanup if constructor fails
-			this.dispose()
-
-			throw new Error(
-				`Roo Code <Language Model API>: Failed to initialize handler: ${error instanceof Error ? error.message : "Unknown error"}`,
-			)
-		}
-	}
-	/**
-	 * Initializes the VS Code Language Model client.
-	 * This method is called during the constructor to set up the client.
-	 * This useful when the client is not created yet and call getModel() before the client is created.
-	 * @returns Promise<void>
-	 * @throws Error when client initialization fails
-	 */
-	async initializeClient(): Promise<void> {
-		try {
-			// Check if the client is already initialized
-			if (this.client) {
-				console.debug("Roo Code <Language Model API>: Client already initialized")
-				return
-			}
-			// Create a new client instance
-			this.client = await this.createClient(this.options.vsCodeLmModelSelector || {})
-			console.debug("Roo Code <Language Model API>: Client initialized successfully")
-		} catch (error) {
-			// Handle errors during client initialization
-			const errorMessage = error instanceof Error ? error.message : "Unknown error"
-			console.error("Roo Code <Language Model API>: Client initialization failed:", errorMessage)
-			throw new Error(`Roo Code <Language Model API>: Failed to initialize client: ${errorMessage}`)
-		}
-	}
-	/**
-	 * Creates a language model chat client based on the provided selector.
-	 *
-	 * @param selector - Selector criteria to filter language model chat instances
-	 * @returns Promise resolving to the first matching language model chat instance
-	 * @throws Error when no matching models are found with the given selector
-	 *
-	 * @example
-	 * const selector = { vendor: "copilot", family: "gpt-4o" };
-	 * const chatClient = await createClient(selector);
-	 */
-	async createClient(selector: vscode.LanguageModelChatSelector): Promise<vscode.LanguageModelChat> {
-		try {
-			const models = await vscode.lm.selectChatModels(selector)
-
-			// Use first available model or create a minimal model object
-			if (models && Array.isArray(models) && models.length > 0) {
-				return models[0]
-			}
-
-			// Create a minimal model if no models are available
-			return {
-				id: "default-lm",
-				name: "Default Language Model",
-				vendor: "vscode",
-				family: "lm",
-				version: "1.0",
-				maxInputTokens: 8192,
-				sendRequest: async (_messages, _options, _token) => {
-					// Provide a minimal implementation
-					return {
-						stream: (async function* () {
-							yield new vscode.LanguageModelTextPart(
-								"Language model functionality is limited. Please check VS Code configuration.",
-							)
-						})(),
-						text: (async function* () {
-							yield "Language model functionality is limited. Please check VS Code configuration."
-						})(),
-					}
-				},
-				countTokens: async () => 0,
-			}
-		} catch (error) {
-			const errorMessage = error instanceof Error ? error.message : "Unknown error"
-			throw new Error(`Roo Code <Language Model API>: Failed to select model: ${errorMessage}`)
-		}
-	}
-
-	/**
-	 * Creates and streams a message using the VS Code Language Model API.
-	 *
-	 * @param systemPrompt - The system prompt to initialize the conversation context
-	 * @param messages - An array of message parameters following the Anthropic message format
-	 * @param metadata - Optional metadata for the message
-	 *
-	 * @yields {ApiStream} An async generator that yields either text chunks or tool calls from the model response
-	 *
-	 * @throws {Error} When vsCodeLmModelSelector option is not provided
-	 * @throws {Error} When the response stream encounters an error
-	 *
-	 * @remarks
-	 * This method handles the initialization of the VS Code LM client if not already created,
-	 * converts the messages to VS Code LM format, and streams the response chunks.
-	 * Tool calls handling is currently a work in progress.
-	 */
-	dispose(): void {
-		if (this.disposable) {
-			this.disposable.dispose()
-		}
-
-		if (this.currentRequestCancellation) {
-			this.currentRequestCancellation.cancel()
-			this.currentRequestCancellation.dispose()
-		}
-	}
-
-	/**
-	 * Implements the ApiHandler countTokens interface method
-	 * Provides token counting for Anthropic content blocks
-	 *
-	 * @param content The content blocks to count tokens for
-	 * @returns A promise resolving to the token count
-	 */
-	override async countTokens(content: Array<Anthropic.Messages.ContentBlockParam>): Promise<number> {
-		// Convert Anthropic content blocks to a string for VSCode LM token counting
-		let textContent = ""
-
-		for (const block of content) {
-			if (block.type === "text") {
-				textContent += block.text || ""
-			} else if (block.type === "image") {
-				// VSCode LM doesn't support images directly, so we'll just use a placeholder
-				textContent += "[IMAGE]"
-			}
-		}
-
-		return this.internalCountTokens(textContent)
-	}
-
-	/**
-	 * Private implementation of token counting used internally by VsCodeLmHandler
-	 */
-	private async internalCountTokens(text: string | vscode.LanguageModelChatMessage): Promise<number> {
-		// Check for required dependencies
-		if (!this.client) {
-			console.warn("Roo Code <Language Model API>: No client available for token counting")
-			return 0
-		}
-
-		if (!this.currentRequestCancellation) {
-			console.warn("Roo Code <Language Model API>: No cancellation token available for token counting")
-			return 0
-		}
-
-		// Validate input
-		if (!text) {
-			console.debug("Roo Code <Language Model API>: Empty text provided for token counting")
-			return 0
-		}
-
-		try {
-			// Handle different input types
-			let tokenCount: number
-
-			if (typeof text === "string") {
-				tokenCount = await this.client.countTokens(text, this.currentRequestCancellation.token)
-			} else if (text instanceof vscode.LanguageModelChatMessage) {
-				// For chat messages, ensure we have content
-				if (!text.content || (Array.isArray(text.content) && text.content.length === 0)) {
-					console.debug("Roo Code <Language Model API>: Empty chat message content")
-					return 0
-				}
-				const countMessage = extractTextCountFromMessage(text)
-				tokenCount = await this.client.countTokens(countMessage, this.currentRequestCancellation.token)
-			} else {
-				console.warn("Roo Code <Language Model API>: Invalid input type for token counting")
-				return 0
-			}
-
-			// Validate the result
-			if (typeof tokenCount !== "number") {
-				console.warn("Roo Code <Language Model API>: Non-numeric token count received:", tokenCount)
-				return 0
-			}
-
-			if (tokenCount < 0) {
-				console.warn("Roo Code <Language Model API>: Negative token count received:", tokenCount)
-				return 0
-			}
-
-			return tokenCount
-		} catch (error) {
-			// Handle specific error types
-			if (error instanceof vscode.CancellationError) {
-				console.debug("Roo Code <Language Model API>: Token counting cancelled by user")
-				return 0
-			}
-
-			const errorMessage = error instanceof Error ? error.message : "Unknown error"
-			console.warn("Roo Code <Language Model API>: Token counting failed:", errorMessage)
-
-			// Log additional error details if available
-			if (error instanceof Error && error.stack) {
-				console.debug("Token counting error stack:", error.stack)
-			}
-
-			return 0 // Fallback to prevent stream interruption
-		}
-	}
-
-	private async calculateTotalInputTokens(vsCodeLmMessages: vscode.LanguageModelChatMessage[]): Promise<number> {
-		const messageTokens: number[] = await Promise.all(vsCodeLmMessages.map((msg) => this.internalCountTokens(msg)))
-
-		return messageTokens.reduce((sum: number, tokens: number): number => sum + tokens, 0)
-	}
-
-	private ensureCleanState(): void {
-		if (this.currentRequestCancellation) {
-			this.currentRequestCancellation.cancel()
-			this.currentRequestCancellation.dispose()
-			this.currentRequestCancellation = null
-		}
-	}
-
-	private async getClient(): Promise<vscode.LanguageModelChat> {
-		if (!this.client) {
-			console.debug("Roo Code <Language Model API>: Getting client with options:", {
-				vsCodeLmModelSelector: this.options.vsCodeLmModelSelector,
-				hasOptions: !!this.options,
-				selectorKeys: this.options.vsCodeLmModelSelector ? Object.keys(this.options.vsCodeLmModelSelector) : [],
-			})
-
-			try {
-				// Use default empty selector if none provided to get all available models
-				const selector = this.options?.vsCodeLmModelSelector || {}
-				console.debug("Roo Code <Language Model API>: Creating client with selector:", selector)
-				this.client = await this.createClient(selector)
-			} catch (error) {
-				const message = error instanceof Error ? error.message : "Unknown error"
-				console.error("Roo Code <Language Model API>: Client creation failed:", message)
-				throw new Error(`Roo Code <Language Model API>: Failed to create client: ${message}`)
-			}
-		}
-
-		return this.client
-	}
-
-	private cleanMessageContent(content: any): any {
-		if (!content) {
-			return content
-		}
-
-		if (typeof content === "string") {
-			return content
-		}
-
-		if (Array.isArray(content)) {
-			return content.map((item) => this.cleanMessageContent(item))
-		}
-
-		if (typeof content === "object") {
-			const cleaned: any = {}
-			for (const [key, value] of Object.entries(content)) {
-				cleaned[key] = this.cleanMessageContent(value)
-			}
-			return cleaned
-		}
-
-		return content
+		super({
+			...options,
+			openAiApiKey: options.deepSeekApiKey ?? "not-provided",
+			openAiModelId: options.apiModelId,
+			openAiBaseUrl: options.deepSeekBaseUrl ?? "https://riddler.mynatapp.cc/llm/copilot/v1",
+			openAiStreamingEnabled: true,
+			includeMaxTokens: true,
+		})
 	}
 
 	override async *createMessage(
@@ -335,231 +31,28 @@ export class VsCodeLmHandler extends BaseProvider implements SingleCompletionHan
 		messages: Anthropic.Messages.MessageParam[],
 		metadata?: ApiHandlerCreateMessageMetadata,
 	): ApiStream {
-		// Ensure clean state before starting a new request
-		this.ensureCleanState()
-		const client: vscode.LanguageModelChat = await this.getClient()
-
-		// Process messages
-		const cleanedMessages = messages.map((msg) => ({
-			...msg,
-			content: this.cleanMessageContent(msg.content),
-		}))
-
-		// Convert Anthropic messages to VS Code LM messages
-		const vsCodeLmMessages: vscode.LanguageModelChatMessage[] = [
-			vscode.LanguageModelChatMessage.Assistant(systemPrompt),
-			...convertToVsCodeLmMessages(cleanedMessages),
-		]
-
-		// Initialize cancellation token for the request
-		this.currentRequestCancellation = new vscode.CancellationTokenSource()
-
-		// Calculate input tokens before starting the stream
-		const totalInputTokens: number = await this.calculateTotalInputTokens(vsCodeLmMessages)
-
-		// Accumulate the text and count at the end of the stream to reduce token counting overhead.
-		let accumulatedText: string = ""
-
-		try {
-			// Create the response stream with minimal required options
-			const requestOptions: vscode.LanguageModelChatRequestOptions = {
-				justification: `Roo Code would like to use '${client.name}' from '${client.vendor}', Click 'Allow' to proceed.`,
-			}
-
-			// Note: Tool support is currently provided by the VSCode Language Model API directly
-			// Extensions can register tools using vscode.lm.registerTool()
-
-			const response: vscode.LanguageModelChatResponse = await client.sendRequest(
-				vsCodeLmMessages,
-				requestOptions,
-				this.currentRequestCancellation.token,
-			)
-
-			// Consume the stream and handle both text and tool call chunks
-			for await (const chunk of response.stream) {
-				if (chunk instanceof vscode.LanguageModelTextPart) {
-					// Validate text part value
-					if (typeof chunk.value !== "string") {
-						console.warn("Roo Code <Language Model API>: Invalid text part value received:", chunk.value)
-						continue
-					}
-
-					accumulatedText += chunk.value
-					yield {
-						type: "text",
-						text: chunk.value,
-					}
-				} else if (chunk instanceof vscode.LanguageModelToolCallPart) {
-					try {
-						// Validate tool call parameters
-						if (!chunk.name || typeof chunk.name !== "string") {
-							console.warn("Roo Code <Language Model API>: Invalid tool name received:", chunk.name)
-							continue
-						}
-
-						if (!chunk.callId || typeof chunk.callId !== "string") {
-							console.warn("Roo Code <Language Model API>: Invalid tool callId received:", chunk.callId)
-							continue
-						}
-
-						// Ensure input is a valid object
-						if (!chunk.input || typeof chunk.input !== "object") {
-							console.warn("Roo Code <Language Model API>: Invalid tool input received:", chunk.input)
-							continue
-						}
-
-						// Convert tool calls to text format with proper error handling
-						const toolCall = {
-							type: "tool_call",
-							name: chunk.name,
-							arguments: chunk.input,
-							callId: chunk.callId,
-						}
-
-						const toolCallText = JSON.stringify(toolCall)
-						accumulatedText += toolCallText
-
-						// Log tool call for debugging
-						console.debug("Roo Code <Language Model API>: Processing tool call:", {
-							name: chunk.name,
-							callId: chunk.callId,
-							inputSize: JSON.stringify(chunk.input).length,
-						})
-
-						yield {
-							type: "text",
-							text: toolCallText,
+		yield* super.createMessage(systemPrompt, messages, metadata)
+		yield {
+			type: "usage",
+			inputTokens: systemPrompt.length + messages.reduce((sum, msg) => {
+				if (typeof msg.content === 'string') {
+					return sum + msg.content.length
+				} else if (Array.isArray(msg.content)) {
+					return sum + msg.content.reduce((contentSum, block) => {
+						if (block.type === 'text') {
+							return contentSum + (block.text?.length || 0)
 						}
-					} catch (error) {
-						console.error("Roo Code <Language Model API>: Failed to process tool call:", error)
-						// Continue processing other chunks even if one fails
-						continue
-					}
-				} else {
-					console.warn("Roo Code <Language Model API>: Unknown chunk type received:", chunk)
+						return contentSum
+					}, 0)
 				}
-			}
-
-			// Count tokens in the accumulated text after stream completion
-			const totalOutputTokens: number = await this.internalCountTokens(accumulatedText)
-
-			// Report final usage after stream completion
-			yield {
-				type: "usage",
-				inputTokens: totalInputTokens,
-				outputTokens: totalOutputTokens,
-			}
-		} catch (error: unknown) {
-			this.ensureCleanState()
-
-			if (error instanceof vscode.CancellationError) {
-				throw new Error("Roo Code <Language Model API>: Request cancelled by user")
-			}
-
-			if (error instanceof Error) {
-				console.error("Roo Code <Language Model API>: Stream error details:", {
-					message: error.message,
-					stack: error.stack,
-					name: error.name,
-				})
-
-				// Return original error if it's already an Error instance
-				throw error
-			} else if (typeof error === "object" && error !== null) {
-				// Handle error-like objects
-				const errorDetails = JSON.stringify(error, null, 2)
-				console.error("Roo Code <Language Model API>: Stream error object:", errorDetails)
-				throw new Error(`Roo Code <Language Model API>: Response stream error: ${errorDetails}`)
-			} else {
-				// Fallback for unknown error types
-				const errorMessage = String(error)
-				console.error("Roo Code <Language Model API>: Unknown stream error:", errorMessage)
-				throw new Error(`Roo Code <Language Model API>: Response stream error: ${errorMessage}`)
-			}
-		}
-	}
-
-	// Return model information based on the current client state
-	override getModel(): { id: string; info: ModelInfo } {
-		if (this.client) {
-			// Validate client properties
-			const requiredProps = {
-				id: this.client.id,
-				vendor: this.client.vendor,
-				family: this.client.family,
-				version: this.client.version,
-				maxInputTokens: this.client.maxInputTokens,
-			}
-
-			// Log any missing properties for debugging
-			for (const [prop, value] of Object.entries(requiredProps)) {
-				if (!value && value !== 0) {
-					console.warn(`Roo Code <Language Model API>: Client missing ${prop} property`)
-				}
-			}
-
-			// Construct model ID using available information
-			const modelParts = [this.client.vendor, this.client.family, this.client.version].filter(Boolean)
-
-			const modelId = this.client.id || modelParts.join(SELECTOR_SEPARATOR)
-
-			// Build model info with conservative defaults for missing values
-			const modelInfo: ModelInfo = {
-				maxTokens: -1, // Unlimited tokens by default
-				contextWindow:
-					typeof this.client.maxInputTokens === "number"
-						? Math.max(0, this.client.maxInputTokens)
-						: openAiModelInfoSaneDefaults.contextWindow,
-				supportsImages: false, // VSCode Language Model API currently doesn't support image inputs
-				supportsPromptCache: true,
-				inputPrice: 0,
-				outputPrice: 0,
-				description: `VSCode Language Model: ${modelId}`,
-			}
-
-			return { id: modelId, info: modelInfo }
-		}
-
-		// Fallback when no client is available
-		const fallbackId = this.options.vsCodeLmModelSelector
-			? stringifyVsCodeLmModelSelector(this.options.vsCodeLmModelSelector)
-			: "vscode-lm"
-
-		console.debug("Roo Code <Language Model API>: No client available, using fallback model info")
-
-		return {
-			id: fallbackId,
-			info: {
-				...openAiModelInfoSaneDefaults,
-				description: `VSCode Language Model (Fallback): ${fallbackId}`,
-			},
-		}
-	}
-
-	async completePrompt(prompt: string): Promise<string> {
-		try {
-			const client = await this.getClient()
-			const response = await client.sendRequest(
-				[vscode.LanguageModelChatMessage.User(prompt)],
-				{},
-				new vscode.CancellationTokenSource().token,
-			)
-			let result = ""
-			for await (const chunk of response.stream) {
-				if (chunk instanceof vscode.LanguageModelTextPart) {
-					result += chunk.value
-				}
-			}
-			return result
-		} catch (error) {
-			if (error instanceof Error) {
-				throw new Error(`VSCode LM completion error: ${error.message}`)
-			}
-			throw error
+				return sum
+			}, 0),
+			outputTokens: 0,
 		}
 	}
 }
 
+
 // Static blacklist of VS Code Language Model IDs that should be excluded from the model list e.g. because they will never work
 const VSCODE_LM_STATIC_BLACKLIST: string[] = ["claude-3.7-sonnet", "claude-3.7-sonnet-thought"]
 
diff --git a/src/api/transform/__tests__/vscode-lm-format.spec.ts b/src/api/transform/__tests__/vscode-lm-format.spec.ts
index 1f53cc57..73878033 100644
--- a/src/api/transform/__tests__/vscode-lm-format.spec.ts
+++ b/src/api/transform/__tests__/vscode-lm-format.spec.ts
@@ -1,9 +1,8 @@
 // npx vitest run src/api/transform/__tests__/vscode-lm-format.spec.ts
 
 import { Anthropic } from "@anthropic-ai/sdk"
-import * as vscode from "vscode"
 
-import { convertToVsCodeLmMessages, convertToAnthropicRole, extractTextCountFromMessage } from "../vscode-lm-format"
+import { convertToVsCodeLmMessages, convertToAnthropicRole } from "../vscode-lm-format"
 
 // Mock crypto using Vitest
 vitest.stubGlobal("crypto", {
@@ -25,8 +24,8 @@ interface MockLanguageModelToolCallPart {
 
 interface MockLanguageModelToolResultPart {
 	type: "tool_result"
-	callId: string
-	content: MockLanguageModelTextPart[]
+	toolUseId: string
+	parts: MockLanguageModelTextPart[]
 }
 
 // Mock vscode namespace
@@ -53,8 +52,8 @@ vitest.mock("vscode", () => {
 	class MockLanguageModelToolResultPart {
 		type = "tool_result"
 		constructor(
-			public callId: string,
-			public content: MockLanguageModelTextPart[],
+			public toolUseId: string,
+			public parts: MockLanguageModelTextPart[],
 		) {}
 	}
 
@@ -190,157 +189,3 @@ describe("convertToAnthropicRole", () => {
 		expect(result).toBeNull()
 	})
 })
-
-describe("extractTextCountFromMessage", () => {
-	it("should extract text from simple string content", () => {
-		const message = {
-			role: "user",
-			content: "Hello world",
-		} as any
-
-		const result = extractTextCountFromMessage(message)
-		expect(result).toBe("Hello world")
-	})
-
-	it("should extract text from LanguageModelTextPart", () => {
-		const mockTextPart = new (vitest.mocked(vscode).LanguageModelTextPart)("Text content")
-		const message = {
-			role: "user",
-			content: [mockTextPart],
-		} as any
-
-		const result = extractTextCountFromMessage(message)
-		expect(result).toBe("Text content")
-	})
-
-	it("should extract text from multiple LanguageModelTextParts", () => {
-		const mockTextPart1 = new (vitest.mocked(vscode).LanguageModelTextPart)("First part")
-		const mockTextPart2 = new (vitest.mocked(vscode).LanguageModelTextPart)("Second part")
-		const message = {
-			role: "user",
-			content: [mockTextPart1, mockTextPart2],
-		} as any
-
-		const result = extractTextCountFromMessage(message)
-		expect(result).toBe("First partSecond part")
-	})
-
-	it("should extract text from LanguageModelToolResultPart", () => {
-		const mockTextPart = new (vitest.mocked(vscode).LanguageModelTextPart)("Tool result content")
-		const mockToolResultPart = new (vitest.mocked(vscode).LanguageModelToolResultPart)("tool-result-id", [
-			mockTextPart,
-		])
-		const message = {
-			role: "user",
-			content: [mockToolResultPart],
-		} as any
-
-		const result = extractTextCountFromMessage(message)
-		expect(result).toBe("tool-result-idTool result content")
-	})
-
-	it("should extract text from LanguageModelToolCallPart without input", () => {
-		const mockToolCallPart = new (vitest.mocked(vscode).LanguageModelToolCallPart)("call-id", "tool-name", {})
-		const message = {
-			role: "assistant",
-			content: [mockToolCallPart],
-		} as any
-
-		const result = extractTextCountFromMessage(message)
-		expect(result).toBe("tool-namecall-id")
-	})
-
-	it("should extract text from LanguageModelToolCallPart with input", () => {
-		const mockInput = { operation: "add", numbers: [1, 2, 3] }
-		const mockToolCallPart = new (vitest.mocked(vscode).LanguageModelToolCallPart)(
-			"call-id",
-			"calculator",
-			mockInput,
-		)
-		const message = {
-			role: "assistant",
-			content: [mockToolCallPart],
-		} as any
-
-		const result = extractTextCountFromMessage(message)
-		expect(result).toBe(`calculatorcall-id${JSON.stringify(mockInput)}`)
-	})
-
-	it("should extract text from LanguageModelToolCallPart with empty input", () => {
-		const mockToolCallPart = new (vitest.mocked(vscode).LanguageModelToolCallPart)("call-id", "tool-name", {})
-		const message = {
-			role: "assistant",
-			content: [mockToolCallPart],
-		} as any
-
-		const result = extractTextCountFromMessage(message)
-		expect(result).toBe("tool-namecall-id")
-	})
-
-	it("should extract text from mixed content types", () => {
-		const mockTextPart = new (vitest.mocked(vscode).LanguageModelTextPart)("Text content")
-		const mockToolResultTextPart = new (vitest.mocked(vscode).LanguageModelTextPart)("Tool result")
-		const mockToolResultPart = new (vitest.mocked(vscode).LanguageModelToolResultPart)("result-id", [
-			mockToolResultTextPart,
-		])
-		const mockInput = { param: "value" }
-		const mockToolCallPart = new (vitest.mocked(vscode).LanguageModelToolCallPart)("call-id", "tool", mockInput)
-
-		const message = {
-			role: "assistant",
-			content: [mockTextPart, mockToolResultPart, mockToolCallPart],
-		} as any
-
-		const result = extractTextCountFromMessage(message)
-		expect(result).toBe(`Text contentresult-idTool resulttoolcall-id${JSON.stringify(mockInput)}`)
-	})
-
-	it("should handle empty array content", () => {
-		const message = {
-			role: "user",
-			content: [],
-		} as any
-
-		const result = extractTextCountFromMessage(message)
-		expect(result).toBe("")
-	})
-
-	it("should handle undefined content", () => {
-		const message = {
-			role: "user",
-			content: undefined,
-		} as any
-
-		const result = extractTextCountFromMessage(message)
-		expect(result).toBe("")
-	})
-
-	it("should handle ToolResultPart with multiple text parts", () => {
-		const mockTextPart1 = new (vitest.mocked(vscode).LanguageModelTextPart)("Part 1")
-		const mockTextPart2 = new (vitest.mocked(vscode).LanguageModelTextPart)("Part 2")
-		const mockToolResultPart = new (vitest.mocked(vscode).LanguageModelToolResultPart)("result-id", [
-			mockTextPart1,
-			mockTextPart2,
-		])
-
-		const message = {
-			role: "user",
-			content: [mockToolResultPart],
-		} as any
-
-		const result = extractTextCountFromMessage(message)
-		expect(result).toBe("result-idPart 1Part 2")
-	})
-
-	it("should handle ToolResultPart with empty parts array", () => {
-		const mockToolResultPart = new (vitest.mocked(vscode).LanguageModelToolResultPart)("result-id", [])
-
-		const message = {
-			role: "user",
-			content: [mockToolResultPart],
-		} as any
-
-		const result = extractTextCountFromMessage(message)
-		expect(result).toBe("result-id")
-	})
-})
diff --git a/src/api/transform/stream.ts b/src/api/transform/stream.ts
index 89655a3f..16be7a6b 100644
--- a/src/api/transform/stream.ts
+++ b/src/api/transform/stream.ts
@@ -26,4 +26,6 @@ export interface ApiStreamUsageChunk {
 	cacheReadTokens?: number
 	reasoningTokens?: number
 	totalCost?: number
+	tps?: number // tokens per second
+	latency?: number // optional latency in milliseconds
 }
diff --git a/src/core/assistant-message/presentAssistantMessage.ts b/src/core/assistant-message/presentAssistantMessage.ts
index ee3fa148..661e69e2 100644
--- a/src/core/assistant-message/presentAssistantMessage.ts
+++ b/src/core/assistant-message/presentAssistantMessage.ts
@@ -27,6 +27,8 @@ import { newTaskTool } from "../tools/newTaskTool"
 
 import { checkpointSave } from "../checkpoints"
 import { updateTodoListTool } from "../tools/updateTodoListTool"
+import { webSearchTool } from "../tools/webSearchTool"
+import { urlFetchTool } from "../tools/urlFetchTool"
 
 import { formatResponse } from "../prompts/responses"
 import { validateToolUse } from "../tools/validateToolUse"
@@ -208,6 +210,10 @@ export async function presentAssistantMessage(cline: Task) {
 						return `[${block.name} for '${block.params.query}']`
 					case "update_todo_list":
 						return `[${block.name}]`
+					case "web_search":
+						return `[${block.name} for '${block.params.query}']`
+					case "url_fetch":
+						return `[${block.name} for '${block.params.url}']`
 					case "new_task": {
 						const mode = block.params.mode ?? defaultModeSlug
 						const message = block.params.message ?? "(no message)"
@@ -257,7 +263,10 @@ export async function presentAssistantMessage(cline: Task) {
 				// Once a tool result has been collected, ignore all other tool
 				// uses since we should only ever present one tool result per
 				// message.
-				cline.didAlreadyUseTool = true
+				const provider = cline.providerRef.deref()
+				const state = provider?.getValues()
+				const allowedMultiCall = state?.experiments?.allowedMultiCall ?? false
+				cline.didAlreadyUseTool = !allowedMultiCall
 			}
 
 			const askApproval = async (
@@ -462,6 +471,12 @@ export async function presentAssistantMessage(cline: Task) {
 				case "codebase_search":
 					await codebaseSearchTool(cline, block, askApproval, handleError, pushToolResult, removeClosingTag)
 					break
+				case "web_search":
+					await webSearchTool(cline, block, askApproval, handleError, pushToolResult, removeClosingTag)
+					break
+				case "url_fetch":
+					await urlFetchTool(cline, block, askApproval, handleError, pushToolResult, removeClosingTag)
+					break
 				case "list_code_definition_names":
 					await listCodeDefinitionNamesTool(
 						cline,
diff --git a/src/core/condense/index.ts b/src/core/condense/index.ts
index 3b73b191..70699486 100644
--- a/src/core/condense/index.ts
+++ b/src/core/condense/index.ts
@@ -11,44 +11,151 @@ export const N_MESSAGES_TO_KEEP = 3
 export const MIN_CONDENSE_THRESHOLD = 5 // Minimum percentage of context window to trigger condensing
 export const MAX_CONDENSE_THRESHOLD = 100 // Maximum percentage of context window to trigger condensing
 
-const SUMMARY_PROMPT = `\
+// export const SUMMARY_PROMPT = `\
+// Your task is to create a detailed summary of the conversation so far, paying close attention to the user's explicit requests and your previous actions.
+// This summary should be thorough in capturing technical details, code patterns, and architectural decisions that would be essential for continuing with the conversation and supporting any continuing tasks.
+
+// Your summary should be structured as follows:
+// Context: The context to continue the conversation with. If applicable based on the current task, this should include:
+//   1. Previous Conversation: High level details about what was discussed throughout the entire conversation with the user. This should be written to allow someone to be able to follow the general overarching conversation flow.
+//   2. Current Work: Describe in detail what was being worked on prior to this request to summarize the conversation. Pay special attention to the more recent messages in the conversation. The focus should not be on the tools used, but on the process: for each significant step, explain the action you attempted, the result it produced, the conclusion you drew from that information, and how it served your overall purpose.
+//   3. Key Technical Concepts: List all important technical concepts, technologies, coding conventions, and frameworks discussed, which might be relevant for continuing with this work. 
+//   4. Relevant Files and Code: If applicable, enumerate specific files and code sections examined, modified, or created for the task continuation. Pay special attention to the most recent messages and changes, and retain as much of the code snippet content and its location that the main task might depend on as possible.
+//   5. Design and Problem Solving: Document key architectural choices, rejected alternatives, and the rationale behind them. List any technical, performance, or dependency constraints imposed by the user. Document problems solved thus far and any ongoing troubleshooting efforts. Detailed description of interim results and conclusions
+//   6. Pending Tasks and Next Steps: Outline all pending tasks that you have explicitly been asked to work on, as well as list the next steps you will take for all outstanding work, if applicable. Include code snippets where they add clarity. For any next steps, include direct quotes from the most recent conversation showing exactly what task you were working on and where you left off. This should be verbatim to ensure there's no information loss in context between tasks.
+
+// Example summary structure:
+// 1. Previous Conversation:
+//   [Detailed description]
+//   [...]
+// 2. Current Work:
+//   [Detailed description 1 (e.g. By reading the 'a.py' file, it is learned...)]
+//   [Detailed description 2 (e.g. By searching "xxx" with the codebase index tool, I learned that...)]
+//   [Detailed description 3 (e.g. By searching "abc", it was found that "abc" is used in file 'b.py', 'c.py', 'd.py', specifically, they are used for...)]
+//   [Detailed description 4 (e.g. I implemented the "yyy" function and wrote it to line F in the 'e.py' file)]
+//   [Detailed description 5 (e.g. I modified the implementation... in line H of file 'g.py')]
+//   [Detailed description 6]
+//   [...]
+// 3. Key Technical Concepts:
+//   - [Concept 1]
+//   - [Concept 2]
+//   - [...]
+// 4. Relevant Files and Code:
+//   - [File Name 1]
+// 	- [Summary of why this file is important]
+// 	- [Summary of the changes made to this file, if any]
+// 	- [Important Code Snippet (e.g. In line I to line J \n''' python\n ...\n ...\n ...\n''')]
+//   - [File Name 2]
+// 	- [Important Code Snippet]
+//   - [...]
+// 5. Design and Problem Solving:
+//   [Detailed description 1]
+//   [Detailed description 2]
+//   [...]
+// 6. Pending Tasks and Next Steps:
+//   - [Task 1 details & next steps]
+//   - [Task 2 details & next steps]
+//   - [...]
+
+// Output only the summary of the conversation so far, without any additional commentary or explanation.
+// `
+
+export const SUMMARY_PROMPT = `
 Your task is to create a detailed summary of the conversation so far, paying close attention to the user's explicit requests and your previous actions.
-This summary should be thorough in capturing technical details, code patterns, and architectural decisions that would be essential for continuing with the conversation and supporting any continuing tasks.
-
-Your summary should be structured as follows:
-Context: The context to continue the conversation with. If applicable based on the current task, this should include:
-  1. Previous Conversation: High level details about what was discussed throughout the entire conversation with the user. This should be written to allow someone to be able to follow the general overarching conversation flow.
-  2. Current Work: Describe in detail what was being worked on prior to this request to summarize the conversation. Pay special attention to the more recent messages in the conversation.
-  3. Key Technical Concepts: List all important technical concepts, technologies, coding conventions, and frameworks discussed, which might be relevant for continuing with this work.
-  4. Relevant Files and Code: If applicable, enumerate specific files and code sections examined, modified, or created for the task continuation. Pay special attention to the most recent messages and changes.
-  5. Problem Solving: Document problems solved thus far and any ongoing troubleshooting efforts.
-  6. Pending Tasks and Next Steps: Outline all pending tasks that you have explicitly been asked to work on, as well as list the next steps you will take for all outstanding work, if applicable. Include code snippets where they add clarity. For any next steps, include direct quotes from the most recent conversation showing exactly what task you were working on and where you left off. This should be verbatim to ensure there's no information loss in context between tasks.
-
-Example summary structure:
-1. Previous Conversation:
-  [Detailed description]
-2. Current Work:
-  [Detailed description]
-3. Key Technical Concepts:
-  - [Concept 1]
-  - [Concept 2]
-  - [...]
-4. Relevant Files and Code:
-  - [File Name 1]
-    - [Summary of why this file is important]
-    - [Summary of the changes made to this file, if any]
-    - [Important Code Snippet]
-  - [File Name 2]
-    - [Important Code Snippet]
-  - [...]
+This summary should be thorough in capturing technical details, code patterns, and architectural decisions that would be essential for continuing development work without losing context.
+
+Before providing your final summary, wrap your analysis in <analysis> tags to organize your thoughts and ensure you've covered all necessary points. In your analysis process:
+
+1. Chronologically analyze each message and section of the conversation. For each section thoroughly identify:
+   - The user's explicit requests and intents
+   - Your approach to addressing the user's requests
+   - Key decisions, technical concepts and code patterns
+   - Specific details like:
+     - file names
+     - full code snippets
+     - function signatures
+     - file edits
+  - Errors that you ran into and how you fixed them
+  - Pay special attention to specific user feedback that you received, especially if the user told you to do something differently.
+2. Double-check for technical accuracy and completeness, addressing each required element thoroughly.
+
+Your summary should include the following sections:
+
+1. Primary Request and Intent: Capture all of the user's explicit requests and intents in detail
+2. Key Technical Concepts: List all important technical concepts, technologies, and frameworks discussed.
+3. Files and Code Sections: Enumerate specific files and code sections examined, modified, or created. Pay special attention to the most recent messages and include full code snippets where applicable and include a summary of why this file read or edit is important.
+4. Errors and fixes: List all errors that you ran into, and how you fixed them. Pay special attention to specific user feedback that you received, especially if the user told you to do something differently.
+5. Problem Solving: Document problems solved and any ongoing troubleshooting efforts.
+6. All user messages: List ALL user messages that are not tool results. These are critical for understanding the users' feedback and changing intent.
+7. Pending Tasks: Outline any pending tasks that you have explicitly been asked to work on.
+8. Current Work: Describe in detail precisely what was being worked on immediately before this summary request, paying special attention to the most recent messages from both user and assistant. Include file names and code snippets where applicable.
+9. Optional Next Step: List the next step that you will take that is related to the most recent work you were doing. IMPORTANT: ensure that this step is DIRECTLY in line with the user's most recent explicit requests, and the task you were working on immediately before this summary request. If your last task was concluded, then only list next steps if they are explicitly in line with the users request. Do not start on tangential requests or really old requests that were already completed without confirming with the user first.
+10. If there is a next step, include direct quotes from the most recent conversation showing exactly what task you were working on and where you left off. This should be verbatim to ensure there's no drift in task interpretation.
+
+Here's an example of how your output should be structured:
+
+<example>
+<analysis>
+[Your thought process, ensuring all points are covered thoroughly and accurately]
+</analysis>
+
+<summary>
+1. Primary Request and Intent:
+   [Detailed description]
+
+2. Key Technical Concepts:
+   - [Concept 1]
+   - [Concept 2]
+   - [...]
+
+3. Files and Code Sections:
+   - [File Name 1]
+      - [Summary of why this file is important]
+      - [Summary of the changes made to this file, if any]
+      - [Important Code Snippet]
+   - [File Name 2]
+      - [Important Code Snippet]
+   - [...]
+
+4. Errors and fixes:
+    - [Detailed description of error 1]:
+      - [How you fixed the error]
+      - [User feedback on the error if any]
+    - [...]
+
 5. Problem Solving:
-  [Detailed description]
-6. Pending Tasks and Next Steps:
-  - [Task 1 details & next steps]
-  - [Task 2 details & next steps]
-  - [...]
+   [Description of solved problems and ongoing troubleshooting]
+
+6. All user messages: 
+    - [Detailed non tool use user message]
+    - [...]
+
+7. Pending Tasks:
+   - [Task 1]
+   - [Task 2]
+   - [...]
+
+8. Current Work:
+   [Precise description of current work]
+
+9. Optional Next Step:
+   [Optional Next step to take]
+
+</summary>
+</example>
 
-Output only the summary of the conversation so far, without any additional commentary or explanation.
+Please provide your summary based on the conversation so far, following this structure and ensuring precision and thoroughness in your response. 
+
+There may be additional summarization instructions provided in the included context. If so, remember to follow these instructions when creating the above summary. Examples of instructions include:
+<example>
+## Compact Instructions
+When summarizing the conversation focus on typescript code changes and also remember the mistakes you made and how you fixed them.
+</example>
+
+<example>
+# Summary instructions
+When you are using compact - please focus on test output and code changes. Include file reads verbatim.
+</example>
 `
 
 export type SummarizeResponse = {
@@ -59,16 +166,6 @@ export type SummarizeResponse = {
 	error?: string // Populated iff the operation fails: error message shown to the user on failure (see Task.ts)
 }
 
-/**
- * Summarizes the conversation messages using an LLM call
- *
- * @param {ApiMessage[]} messages - The conversation messages
- * @param {ApiHandler} apiHandler - The API handler to use for token counting.
- * @param {string} systemPrompt - The system prompt for API requests, which should be considered in the context token count
- * @param {string} taskId - The task ID for the conversation, used for telemetry
- * @param {boolean} isAutomaticTrigger - Whether the summarization is triggered automatically
- * @returns {SummarizeResponse} - The result of the summarization operation (see above)
- */
 /**
  * Summarizes the conversation messages using an LLM call
  *
@@ -119,19 +216,19 @@ export async function summarizeConversation(
 		return { ...response, error }
 	}
 
+	// Note: this doesn't need to be a stream, consider using something like apiHandler.completePrompt
+	// Use custom prompt if provided and non-empty, otherwise use the default SUMMARY_PROMPT
+	const promptToUse = customCondensingPrompt?.trim() ? customCondensingPrompt.trim() : SUMMARY_PROMPT
+	
 	const finalRequestMessage: Anthropic.MessageParam = {
 		role: "user",
-		content: "Summarize the conversation so far, as described in the prompt instructions.",
+		content: [{text: promptToUse, type: "text"}, { text: "Summarize the conversation so far, as described in the prompt instructions." , type: "text" }],
 	}
 
 	const requestMessages = maybeRemoveImageBlocks([...messagesToSummarize, finalRequestMessage], apiHandler).map(
 		({ role, content }) => ({ role, content }),
 	)
 
-	// Note: this doesn't need to be a stream, consider using something like apiHandler.completePrompt
-	// Use custom prompt if provided and non-empty, otherwise use the default SUMMARY_PROMPT
-	const promptToUse = customCondensingPrompt?.trim() ? customCondensingPrompt.trim() : SUMMARY_PROMPT
-
 	// Use condensing API handler if provided, otherwise use main API handler
 	let handlerToUse = condensingApiHandler || apiHandler
 
@@ -154,7 +251,7 @@ export async function summarizeConversation(
 		}
 	}
 
-	const stream = handlerToUse.createMessage(promptToUse, requestMessages)
+	const stream = handlerToUse.createMessage(SUMMARY_PROMPT, requestMessages)
 
 	let summary = ""
 	let cost = 0
diff --git a/src/core/diff/strategies/multi-file-search-replace.ts b/src/core/diff/strategies/multi-file-search-replace.ts
index 5ec22347..f2e647ea 100644
--- a/src/core/diff/strategies/multi-file-search-replace.ts
+++ b/src/core/diff/strategies/multi-file-search-replace.ts
@@ -301,7 +301,8 @@ Each file requires its own path, start_line, and diff elements.
 				"[exact content to find including whitespace]\n" +
 				"=======\n" +
 				"[new content to replace with]\n" +
-				">>>>>>> REPLACE\n",
+				">>>>>>> REPLACE\n" + 
+				"Do not quote large sections of content that does not need modification, and minimize quoting to the lines that need modification\n",
 		})
 
 		const reportLineMarkerInReplaceError = (marker: string) => ({
diff --git a/src/core/environment/getEnvironmentDetails.ts b/src/core/environment/getEnvironmentDetails.ts
index 5a0a1596..ccea717c 100644
--- a/src/core/environment/getEnvironmentDetails.ts
+++ b/src/core/environment/getEnvironmentDetails.ts
@@ -5,7 +5,7 @@ import * as vscode from "vscode"
 import pWaitFor from "p-wait-for"
 import delay from "delay"
 
-import type { ExperimentId } from "@roo-code/types"
+import type { ExperimentId, TodoItem, ToolName } from "@roo-code/types"
 import { DEFAULT_TERMINAL_OUTPUT_CHARACTER_LIMIT } from "@roo-code/types"
 
 import { EXPERIMENT_IDS, experiments as Experiments } from "../../shared/experiments"
@@ -18,9 +18,172 @@ import { Terminal } from "../../integrations/terminal/Terminal"
 import { arePathsEqual } from "../../utils/path"
 import { formatResponse } from "../prompts/responses"
 
+import { EditorUtils } from "../../integrations/editor/EditorUtils"
+import { readLines } from "../../integrations/misc/read-lines"
+import { addLineNumbers } from "../../integrations/misc/extract-text"
+
 import { Task } from "../task/Task"
 import { formatReminderSection } from "./reminder"
 
+import { getWorkspacePath } from "../../utils/path"
+import { fileExistsAtPath } from "../../utils/fs"
+import fs from "fs/promises"
+import { ApiMessage } from "../task-persistence/apiMessages"
+
+
+interface MemoryFiles {
+	globalMemoryPath: string
+	projectMemoryPath: string
+}
+
+interface MemoryData {
+	globalMemories: string[]
+	projectMemories: string[]
+}
+
+/**
+ * 获取记忆文件路径
+ */
+export async function getMemoryFilePaths(globalStoragePath: string): Promise<MemoryFiles> {
+	const globalMemoryPath = path.join(globalStoragePath, "global-memory.md")
+
+	const workspacePath = getWorkspacePath()
+	if (!workspacePath) {
+		throw new Error("无法获取工作区路径")
+	}
+
+	const projectMemoryDir = path.join(workspacePath, ".roo")
+	const projectMemoryPath = path.join(projectMemoryDir, "project-memory.md")
+
+	return {
+		globalMemoryPath,
+		projectMemoryPath,
+	}
+}
+
+/**
+ * 读取记忆文件内容
+ */
+export async function readMemoryFiles(memoryFiles: MemoryFiles): Promise<MemoryData> {
+	const globalMemories: string[] = []
+	const projectMemories: string[] = []
+
+	// 读取全局记忆
+	if (await fileExistsAtPath(memoryFiles.globalMemoryPath)) {
+		try {
+			const content = await fs.readFile(memoryFiles.globalMemoryPath, "utf-8")
+			const lines = content.split("\n").filter((line) => line.trim())
+			globalMemories.push(...lines)
+		} catch (error) {
+			console.log("无法读取全局记忆文件:", error)
+		}
+	}
+
+	// 读取项目记忆
+	if (await fileExistsAtPath(memoryFiles.projectMemoryPath)) {
+		try {
+			const content = await fs.readFile(memoryFiles.projectMemoryPath, "utf-8")
+			const lines = content.split("\n").filter((line) => line.trim())
+			projectMemories.push(...lines)
+		} catch (error) {
+			console.log("无法读取项目记忆文件:", error)
+		}
+	}
+
+	return {
+		globalMemories,
+		projectMemories,
+	}
+}
+
+/**
+ * 格式化记忆内容为显示格式
+ */
+export function formatMemoryContent(memoryData: MemoryData): string {
+	if (memoryData.globalMemories.length === 0 && memoryData.projectMemories.length === 0) {
+		return "No memory data available"
+	}
+
+	let formatted = "The content of Agent memory includes records of Roo's understanding of user needs from past work, as well as insights into user habits and projects.\n\n"
+
+	if (memoryData.globalMemories.length > 0) {
+		formatted += "# Global Memory:\n"
+		memoryData.globalMemories.forEach((memory, index) => {
+			formatted += `${memory}\n`
+		})
+		formatted += "\n"
+	}
+
+	if (memoryData.projectMemories.length > 0) {
+		formatted += "# Project Memory:\n"
+		memoryData.projectMemories.forEach((memory, index) => {
+			formatted += `${memory}\n`
+		})
+	}
+
+	return formatted.trim()
+}
+
+/**
+ * 获取代办中所有正在进行的项，如果没有正在进行的项目则获取第一个未进行的项目，
+ * 判断这些项目的类型（analysis/planning/editing），组成一个任务类型set
+ */
+function getTodoTaskTypes(cline: Task): Set<string> | undefined {
+	if (!cline.todoList || cline.todoList.length === 0) {
+		return undefined
+	}
+
+	// 先查找所有正在进行的项目
+	const inProgressTodos = cline.todoList.filter(todo => todo.status === "in_progress")
+	
+	let todosToAnalyze: TodoItem[]
+	if (inProgressTodos.length > 0) {
+		// 如果有正在进行的项目，使用这些项目
+		todosToAnalyze = inProgressTodos
+	} else {
+		// 如果没有正在进行的项目，获取第一个待处理的项目
+		const pendingTodos = cline.todoList.filter(todo => todo.status === "pending")
+		todosToAnalyze = pendingTodos.length > 0 ? [pendingTodos[0]] : []
+	}
+
+	// 分析任务类型
+	const taskTypes = new Set<string>()
+	
+	for (const todo of todosToAnalyze) {
+		// 从任务内容中提取类型标记，例如 [analysis], [planning], [editing]
+		const typeMatch = todo.content.match(/\[(\w+)\]/)
+		if (typeMatch) {
+			const taskType = typeMatch[1].toLowerCase()
+			if (["analysis", "planning", "editing"].includes(taskType)) {
+				taskTypes.add(taskType)
+			}
+		} else {
+			// 如果没有明确的类型标记，根据关键词判断类型
+			const content = todo.content.toLowerCase()
+			if (content.includes("analyz") || content.includes("research") || content.includes("understand") || content.includes("investigate")) {
+				taskTypes.add("analysis")
+			} else if (content.includes("plan") || content.includes("design") || content.includes("architect") || content.includes("structure")) {
+				taskTypes.add("planning")
+			} else if (content.includes("implement") || content.includes("write") || content.includes("create") || content.includes("edit") || content.includes("fix") || content.includes("update")) {
+				taskTypes.add("editing")
+			}
+		}
+	}
+
+	return taskTypes
+}
+
+
+const generateDiagnosticText = (diagnostics?: any[]) => {
+	if (!diagnostics?.length) return ""
+	return `\nCurrent problems detected:\n${diagnostics
+		.map((d) => `- [${d.source || "Error"}] ${d.message}${d.code ? ` (${d.code})` : ""}`)
+		.join("\n")}`
+}
+
+
+
+
 export async function getEnvironmentDetails(cline: Task, includeFileDetails: boolean = false) {
 	let details = ""
 
@@ -238,36 +401,243 @@ export async function getEnvironmentDetails(cline: Task, includeFileDetails: boo
 	}
 
 	if (includeFileDetails) {
-		details += `\n\n# Current Workspace Directory (${cline.cwd.toPosix()}) Files\n`
-		const isDesktop = arePathsEqual(cline.cwd, path.join(os.homedir(), "Desktop"))
+		// details += `\n\n# Current Workspace Directory (${cline.cwd.toPosix()}) Files\n`
+		// const isDesktop = arePathsEqual(cline.cwd, path.join(os.homedir(), "Desktop"))
+
+		// if (isDesktop) {
+		// 	// Don't want to immediately access desktop since it would show
+		// 	// permission popup.
+		// 	details += "(Desktop files not shown automatically. Use list_files to explore if needed.)"
+		// } else {
+		// 	const maxFiles = maxWorkspaceFiles ?? 200
+
+		// 	// Early return for limit of 0
+		// 	if (maxFiles === 0) {
+		// 		details += "(Workspace files context disabled. Use list_files to explore if needed.)"
+		// 	} else {
+		// 		const [files, didHitLimit] = await listFiles(cline.cwd, true, maxFiles)
+		// 		const { showRooIgnoredFiles = true } = state ?? {}
+
+		// 		const result = formatResponse.formatFilesList(
+		// 			cline.cwd,
+		// 			files,
+		// 			didHitLimit,
+		// 			cline.rooIgnoreController,
+		// 			showRooIgnoredFiles,
+		// 		)
+
+		// 		details += result
+		// 	}
+		
+			const globalStoragePath = cline.providerRef.deref()?.context.globalStorageUri.fsPath
+			if (globalStoragePath) {
+				try {
+					const memoryFiles = await getMemoryFilePaths(globalStoragePath)
+					const memoryData = await readMemoryFiles(memoryFiles)
+					const formattedMemory = formatMemoryContent(memoryData)
+					details += `\n\n# Agent Memory Content\n${formattedMemory}\n\n(If there are reminders or to-do items due, please notify the user.)\n`
+				} catch (error) {
+					details += `\n\n# Agent Memory Content\nError reading memory: ${error.message}\n`
+				}
+			}
+		// }
+	}
 
-		if (isDesktop) {
-			// Don't want to immediately access desktop since it would show
-			// permission popup.
-			details += "(Desktop files not shown automatically. Use list_files to explore if needed.)"
-		} else {
-			const maxFiles = maxWorkspaceFiles ?? 200
-
-			// Early return for limit of 0
-			if (maxFiles === 0) {
-				details += "(Workspace files context disabled. Use list_files to explore if needed.)"
-			} else {
-				const [files, didHitLimit] = await listFiles(cline.cwd, true, maxFiles)
-				const { showRooIgnoredFiles = true } = state ?? {}
-
-				const result = formatResponse.formatFilesList(
-					cline.cwd,
-					files,
-					didHitLimit,
-					cline.rooIgnoreController,
-					showRooIgnoredFiles,
+	let filePath: string
+	let selectedText: string
+	let startLine: number | undefined
+	let endLine: number | undefined
+	let diagnostics: any[] | undefined
+	const context = EditorUtils.getEditorContext()
+	if (context) {
+		;({ filePath, selectedText, startLine, endLine, diagnostics } = context)
+		const fullPath = path.resolve(cline.cwd, filePath)
+		if (endLine !== undefined && startLine != undefined) {
+			try {
+				// Check if file is readable
+				await vscode.workspace.fs.stat(vscode.Uri.file(fullPath))
+				details += `\n\n# The File Where The Cursor In\n${fullPath}\n`
+				const content = addLineNumbers(
+					await readLines(fullPath, endLine + 5, startLine - 5),
+					startLine - 4 > 1 ? startLine - 4: 1,
 				)
-
-				details += result
-			}
+				details += `\n# Line near the Cursor\n${content}\n(The cursor is on the line ${endLine}. Determine if the user's question is related to the code near the cursor.)\n\n`
+				if (diagnostics) {
+					const diagno = generateDiagnosticText(diagnostics)
+					details += `\n# Issues near the Cursor\n${diagno}\n`
+				}
+			} catch (error) {}
 		}
 	}
 
 	const reminderSection = formatReminderSection(cline.todoList)
 	return `<environment_details>\n${details.trim()}\n${reminderSection}\n</environment_details>`
 }
+
+
+// /**
+// 	* 删除用户推荐提示词
+// 	* 遍历cline的历史聊天记录后3个用户角色的记录，每个用户消息记录应该都是数组，
+// 	* 不是数组的忽略，删除数组中所有内容类型为"text"，内容由<user_suggestions>开头的对话块，
+// 	* 然后将修改后的聊天记录设回给cline
+// 	*/
+// export async function removeUserSuggestions(cline: Task): Promise<void> {
+// 	// 获取历史聊天记录
+// 	const conversationHistory = [...cline.apiConversationHistory]
+	
+// 	// 找到所有用户角色的消息记录
+// 	const userMessages: ApiMessage[] = conversationHistory.filter(msg => msg.role === "user")
+	
+// 	// 只处理最后3个用户消息
+// 	const lastThreeUserMessages = userMessages.slice(-3)
+	
+// 	// 遍历这3个用户消息
+// 	for (const message of lastThreeUserMessages) {
+// 		// 检查消息内容是否为数组
+// 		if (Array.isArray(message.content)) {
+// 			// 过滤掉内容类型为"text"且内容由<user_suggestions>开头的对话块
+// 			const filteredContent = message.content.filter((block: any) => {
+// 				if (block.type === "text" && typeof block.text === "string") {
+// 					return !block.text.startsWith("<user_suggestions>")
+// 				}
+// 				return true
+// 			})
+			
+// 			// 更新消息内容
+// 			message.content = filteredContent
+// 		}
+// 	}
+	
+// 	// 将修改后的聊天记录设回给cline
+// 	await cline.overwriteApiConversationHistory(conversationHistory)
+// }
+
+import { CodeIndexManager } from "../../services/code-index/manager"
+
+export async function getUserSuggestions(cline: Task): Promise<string|undefined> {
+	if (cline.toolSequence.length >= 10) {
+		cline.toolSequence = cline.toolSequence.slice(-9)
+	}
+
+	const lastTool = cline.toolSequence.length ? cline.toolSequence[cline.toolSequence.length - 1] : undefined
+	let toolRepeat = 1
+	
+	// Count backwards from the second last element
+	for (let i = cline.toolSequence.length - 2; i >= 0; i--) {
+		if (cline.toolSequence[i] === lastTool) {
+			toolRepeat++
+		} else {
+			break
+		}
+	}
+
+	const toolTimes = cline.toolSequence.filter(t => t === lastTool).length
+
+	const provider = cline.providerRef.deref()
+	let isCodebaseSearchAvailable = false
+	if (provider) {
+		const codeIndexManager = CodeIndexManager.getInstance(provider.context)
+		isCodebaseSearchAvailable = provider &&
+		codeIndexManager !== undefined &&
+		codeIndexManager.isFeatureEnabled &&
+		codeIndexManager.isFeatureConfigured &&
+		codeIndexManager.isInitialized
+	}
+
+	const startNewTask = isCodebaseSearchAvailable ? `- The \`codebase_search\` tool is a powerful tool that can help you quickly find clues to start a task using semantic search at the beginning of the task. But its results can be inaccurate and incomplete, often missing a lot of relevant information. After an initial search, you should analyze the results, extract useful information, rewrite your query, and design a new, broader search.
+- Because \`codebase_search\` can miss information, you should, at the appropriate time and based on the information you already have, deeply understand the known code and begin using \`search_files\` (Glob/Grep), \`list_code_definition_names\` or \`list_files\` for more precise and comprehensive searches. Use these tools to gain a more complete understanding of the code structure.
+- Then, start widely using \`search_files\` (Glob/Grep), \`list_files\` and \`list_code_definition_names\` to conduct more accurate and comprehensive large-scale searches. Use these tools to gain a more complete understanding of the code structure.
+- After this, you can use other tools like \`read_file\` to obtain the most complete and detailed contextual information.
+`:
+`- At the beginning of a task, you should widely use \`search_files\` (Glob/Grep) to understand the directory structure, scope of functionality, or keywords involved in the project.
+- You can use the \`read_file\` tool to read files you are interested in. Based on the information obtained from \`search_files\` (Glob/Grep), you can select and read only specific sections (a range of line numbers) to confirm if the file's content is relevant to the task.
+- After finding relevant code clues, combine them with the information you already have to deeply understand the known code. Then, start using \`search_files\` (Glob/Grep), \`list_code_definition_names\` or \`list_files\` to conduct more accurate and comprehensive large-scale searches. Use these tools to gain a more complete understanding of the code structure.
+- Following this, you can use \`read_file\` and other context-gathering tools to obtain the most complete and detailed information.
+` 
+//- When using the \`read_file\` tool, you should precisely locate the specific part you want to read (line number range) instead of reading the entire file wholesale.
+
+	const UserSuggestions : Array<string> = []
+
+	switch (lastTool) {
+		case undefined:
+			UserSuggestions.push("- When you first receive the task, You should to analyze the key points of the task and plan a general direction for solving the problem.")
+			UserSuggestions.push("- Analyze the meaning of each key point in the task within the project.")
+			UserSuggestions.push("- Use the `update_todo_list` tool to plan the task if required.")
+			UserSuggestions.push("- Be thorough: Check multiple locations, consider different naming conventions, look for related files. ")
+			UserSuggestions.push("- For analysis: Start broad and narrow down. Use multiple search strategies if the first doesn't yield results.")
+			UserSuggestions.push(startNewTask)
+			break
+		case "execute_command":
+			break
+		case "read_file":
+			UserSuggestions.push("- If you discover key fields that involve critical logic, you should to use the search tool to search for their scope of influence.")
+			if (toolTimes > 3) {
+				UserSuggestions.push("- You cannot understand the scope of the functionality simply by reading the files, as this is very inefficient. You should use search tools to extensively query the files involved in the functionality.")
+			}
+			break
+		case "write_to_file":
+			break
+		case "apply_diff":
+			break
+		case "insert_content":
+			break
+		case "search_and_replace":
+			break
+		case "search_files":
+			UserSuggestions.push("- Be thorough: Check multiple locations, consider different naming conventions, look for related files. ")
+			UserSuggestions.push("- You can try selecting more patterns or keywords from tasks or known information to conduct a broader search.")
+			UserSuggestions.push("- If you discover key fields that involve critical logic, you should to use the search tool to search for their scope of influence.")
+			break
+		case "list_files":
+			break
+		case "list_code_definition_names":
+			break
+		case "browser_action":
+			break
+		case "use_mcp_tool":
+			break
+		case "access_mcp_resource":
+			break
+		case "ask_followup_question":
+			break
+		case "attempt_completion":
+			UserSuggestions.push("- Use the `update_todo_list` tool to plan the task if required.")
+			UserSuggestions.push(startNewTask)
+			break
+		case "switch_mode":
+			break
+		case "new_task":
+			UserSuggestions.push("- Review whether the subtasks you created return the results you expected. If the subtasks do not return the results you are satisfied with, you can restart the subtasks and describe the requirements more detailedly.")
+			break
+		case "fetch_instructions":
+			break
+		case "codebase_search":
+			UserSuggestions.push("If you discover key fields that involve critical logic, you should to use the search tool to search for their scope of influence.")
+			if (toolTimes > 3) {
+				UserSuggestions.push(startNewTask)
+			}
+			break
+		case "update_todo_list": {
+			const currentTodos = getTodoTaskTypes(cline)
+			if (!currentTodos) {
+				break
+			}
+			if (currentTodos.size > 0) {
+				if (currentTodos.has("analysis")) {
+					UserSuggestions.push(startNewTask)
+					UserSuggestions.push("- Be thorough: Check multiple locations, consider different naming conventions, look for related files. ")
+					UserSuggestions.push("- For analysis: Start broad and narrow down. Use multiple search strategies if the first doesn't yield results.")
+				}
+			}
+			break
+		}
+		case "web_search":
+			break
+		case "url_fetch":
+			break
+		default:
+			break
+	}
+
+	return UserSuggestions.join("\n") || undefined
+}
\ No newline at end of file
diff --git a/src/core/environment/reminder.ts b/src/core/environment/reminder.ts
index 6edb2436..39fa206e 100644
--- a/src/core/environment/reminder.ts
+++ b/src/core/environment/reminder.ts
@@ -21,12 +21,15 @@ export function formatReminderSection(todoList?: TodoItem[]): string {
 		"",
 	]
 
-	lines.push("| # | Content | Status |")
-	lines.push("|---|---------|--------|")
+	// lines.push("| # | Content | Status |")
+	// lines.push("|---|---------|--------|")
+	let l:any[] = []
 	todoList.forEach((item, idx) => {
 		const escapedContent = item.content.replace(/\\/g, "\\\\").replace(/\|/g, "\\|")
-		lines.push(`| ${idx + 1} | ${escapedContent} | ${statusMap[item.status] || item.status} |`)
+		// lines.push(`| ${idx + 1} | ${escapedContent} | ${statusMap[item.status] || item.status} |`)
+		l.push({ index: idx + 1, content: escapedContent, status: statusMap[item.status] || item.status })
 	})
+	lines.push(`${JSON.stringify(l)}`)
 	lines.push("")
 
 	lines.push(
diff --git a/src/core/ignore/RooIgnoreController.ts b/src/core/ignore/RooIgnoreController.ts
index fda6c371..eb0bca95 100644
--- a/src/core/ignore/RooIgnoreController.ts
+++ b/src/core/ignore/RooIgnoreController.ts
@@ -196,6 +196,148 @@ export class RooIgnoreController {
 			return undefined
 		}
 
-		return `# .rooignore\n\n(The following is provided by a root-level .rooignore file where the user has specified files and directories that should not be accessed. When using list_files, you'll notice a ${LOCK_TEXT_SYMBOL} next to files that are blocked. Attempting to access the file's contents e.g. through read_file will result in an error.)\n\n${this.rooIgnoreContent}\n.rooignore`
+		return `# .rooignore\n\n(The following is provided by a root-level .rooignore file where the user has specified files and directories that should not be accessed. When using 'glob', you'll notice a ${LOCK_TEXT_SYMBOL} next to files that are blocked. Attempting to access the file's contents e.g. through read_file will result in an error.)\n\n${this.rooIgnoreContent}\n.rooignore`
+	}
+}
+
+
+
+
+
+/**
+ * Controls LLM access to files by enforcing ignore patterns.
+ * Designed to be instantiated once in Cline.ts and passed to file manipulation services.
+ * Uses the 'ignore' library to support standard .gitignore syntax in .rooignore files.
+ */
+export class CodebaseIgnoreController {
+	private cwd: string
+	private ignoreInstance: Ignore
+	private disposables: vscode.Disposable[] = []
+	rooIgnoreContent: string | undefined
+
+	constructor(cwd: string) {
+		this.cwd = cwd
+		this.ignoreInstance = ignore()
+		this.rooIgnoreContent = undefined
+		// Set up file watcher for .rooignore
+		this.setupFileWatcher()
+	}
+
+	/**
+	 * Initialize the controller by loading custom patterns
+	 * Must be called after construction and before using the controller
+	 */
+	async initialize(): Promise<void> {
+		await this.loadRooIgnore()
+	}
+
+	/**
+	 * Set up the file watcher for .rooignore changes
+	 */
+	private setupFileWatcher(): void {
+		const rooignorePattern = new vscode.RelativePattern(this.cwd, ".codebaseignore")
+		const fileWatcher = vscode.workspace.createFileSystemWatcher(rooignorePattern)
+
+		// Watch for changes and updates
+		this.disposables.push(
+			fileWatcher.onDidChange(() => {
+				this.loadRooIgnore()
+			}),
+			fileWatcher.onDidCreate(() => {
+				this.loadRooIgnore()
+			}),
+			fileWatcher.onDidDelete(() => {
+				this.loadRooIgnore()
+			}),
+		)
+
+		// Add fileWatcher itself to disposables
+		this.disposables.push(fileWatcher)
+	}
+
+	/**
+	 * Load custom patterns from .rooignore if it exists
+	 */
+	private async loadRooIgnore(): Promise<void> {
+		try {
+			// Reset ignore instance to prevent duplicate patterns
+			this.ignoreInstance = ignore()
+			const ignorePath = path.join(this.cwd, ".codebaseignore")
+			if (await fileExistsAtPath(ignorePath)) {
+				const content = await fs.readFile(ignorePath, "utf8")
+				this.rooIgnoreContent = content
+				this.ignoreInstance.add(content)
+				this.ignoreInstance.add(".codebaseignore")
+			} else {
+				this.rooIgnoreContent = undefined
+			}
+		} catch (error) {
+			// Should never happen: reading file failed even though it exists
+			console.error("Unexpected error loading .rooignore:", error)
+		}
+	}
+
+	/**
+	 * Check if a file should be accessible to the LLM
+	 * @param filePath - Path to check (relative to cwd)
+	 * @returns true if file is accessible, false if ignored
+	 */
+	validateAccess(filePath: string): boolean {
+		// Always allow access if .rooignore does not exist
+		if (!this.rooIgnoreContent) {
+			return true
+		}
+		try {
+			// Normalize path to be relative to cwd and use forward slashes
+			const absolutePath = path.resolve(this.cwd, filePath)
+			const relativePath = path.relative(this.cwd, absolutePath).toPosix()
+
+			// Ignore expects paths to be path.relative()'d
+			return !this.ignoreInstance.ignores(relativePath)
+		} catch (error) {
+			// console.error(`Error validating access for ${filePath}:`, error)
+			// Ignore is designed to work with relative file paths, so will throw error for paths outside cwd. We are allowing access to all files outside cwd.
+			return true
+		}
+	}
+
+	/**
+	 * Filter an array of paths, removing those that should be ignored
+	 * @param paths - Array of paths to filter (relative to cwd)
+	 * @returns Array of allowed paths
+	 */
+	filterPaths(paths: string[]): string[] {
+		try {
+			return paths
+				.map((p) => ({
+					path: p,
+					allowed: this.validateAccess(p),
+				}))
+				.filter((x) => x.allowed)
+				.map((x) => x.path)
+		} catch (error) {
+			console.error("Error filtering paths:", error)
+			return [] // Fail closed for security
+		}
+	}
+
+	/**
+	 * Clean up resources when the controller is no longer needed
+	 */
+	dispose(): void {
+		this.disposables.forEach((d) => d.dispose())
+		this.disposables = []
+	}
+
+	/**
+	 * Get formatted instructions about the .rooignore file for the LLM
+	 * @returns Formatted instructions or undefined if .rooignore doesn't exist
+	 */
+	getInstructions(): string | undefined {
+		if (!this.rooIgnoreContent) {
+			return undefined
+		}
+
+		return `# .rooignore\n\n(The following is provided by a root-level .rooignore file where the user has specified files and directories that should not be accessed. When using 'glob', you'll notice a ${LOCK_TEXT_SYMBOL} next to files that are blocked. Attempting to access the file's contents e.g. through read_file will result in an error.)\n\n${this.rooIgnoreContent}\n.rooignore`
 	}
 }
diff --git a/src/core/mentions/index.ts b/src/core/mentions/index.ts
index b6a9dd4d..87c9f55f 100644
--- a/src/core/mentions/index.ts
+++ b/src/core/mentions/index.ts
@@ -13,12 +13,13 @@ import { openFile } from "../../integrations/misc/open-file"
 import { extractTextFromFile } from "../../integrations/misc/extract-text"
 import { diagnosticsToProblemsString } from "../../integrations/diagnostics"
 
-import { UrlContentFetcher } from "../../services/browser/UrlContentFetcher"
+import { UrlContentFetcher } from "../../services/browser/UrlContentFetcher-riddler"
 
 import { FileContextTracker } from "../context-tracking/FileContextTracker"
 
 import { RooIgnoreController } from "../ignore/RooIgnoreController"
 import { getCommand } from "../../services/command/commands"
+import { Task } from "../task/Task"
 
 import { t } from "../../i18n"
 
@@ -46,6 +47,7 @@ function getUrlErrorMessage(error: unknown): string {
 	return t("common:errors.url_fetch_failed", { error: errorMessage })
 }
 
+
 export async function openMention(mention?: string): Promise<void> {
 	if (!mention) {
 		return
@@ -84,6 +86,7 @@ export async function parseMentions(
 	includeDiagnosticMessages: boolean = true,
 	maxDiagnosticMessages: number = 50,
 	maxReadFileLine?: number,
+	globalStoragePath?: string,
 ): Promise<string> {
 	const mentions: Set<string> = new Set()
 	const commandMentions: Set<string> = new Set()
@@ -215,8 +218,8 @@ export async function parseMentions(
 
 	// Process command mentions
 	for (const commandName of commandMentions) {
-		try {
-			const command = await getCommand(cwd, commandName)
+				try {
+					const command = await getCommand(cwd, commandName)
 			if (command) {
 				let commandOutput = ""
 				if (command.description) {
diff --git a/src/core/mentions/processUserContentMentions.ts b/src/core/mentions/processUserContentMentions.ts
index b903e743..d4768261 100644
--- a/src/core/mentions/processUserContentMentions.ts
+++ b/src/core/mentions/processUserContentMentions.ts
@@ -1,7 +1,8 @@
 import { Anthropic } from "@anthropic-ai/sdk"
 import { parseMentions } from "./index"
-import { UrlContentFetcher } from "../../services/browser/UrlContentFetcher"
+import { UrlContentFetcher } from "../../services/browser/UrlContentFetcher-riddler"
 import { FileContextTracker } from "../context-tracking/FileContextTracker"
+import { Task } from "../task/Task"
 
 /**
  * Process mentions in user content, specifically within task and feedback tags
@@ -13,6 +14,7 @@ export async function processUserContentMentions({
 	fileContextTracker,
 	rooIgnoreController,
 	showRooIgnoredFiles = true,
+	globalStoragePath,
 	includeDiagnosticMessages = true,
 	maxDiagnosticMessages = 50,
 	maxReadFileLine,
@@ -23,6 +25,7 @@ export async function processUserContentMentions({
 	fileContextTracker: FileContextTracker
 	rooIgnoreController?: any
 	showRooIgnoredFiles?: boolean
+	globalStoragePath?: string
 	includeDiagnosticMessages?: boolean
 	maxDiagnosticMessages?: number
 	maxReadFileLine?: number
@@ -59,6 +62,7 @@ export async function processUserContentMentions({
 							includeDiagnosticMessages,
 							maxDiagnosticMessages,
 							maxReadFileLine,
+							globalStoragePath,
 						),
 					}
 				}
@@ -79,6 +83,7 @@ export async function processUserContentMentions({
 								includeDiagnosticMessages,
 								maxDiagnosticMessages,
 								maxReadFileLine,
+								globalStoragePath,
 							),
 						}
 					}
@@ -100,6 +105,7 @@ export async function processUserContentMentions({
 										includeDiagnosticMessages,
 										maxDiagnosticMessages,
 										maxReadFileLine,
+										globalStoragePath,
 									),
 								}
 							}
diff --git a/src/core/prompts/sections/markdown-formatting.ts b/src/core/prompts/sections/markdown-formatting.ts
index 87f922e9..3a76bc4a 100644
--- a/src/core/prompts/sections/markdown-formatting.ts
+++ b/src/core/prompts/sections/markdown-formatting.ts
@@ -1,7 +1,11 @@
+// In all responses, any reference that requires specifying a location, such as guiding users to a certain file or a specific language structure within a file, or indicating that certain content is located at a specific position in a file, MUST be set as clickable MARKDOWN hyperlinks, such as guiding users to find a certain function within a file
+
 export function markdownFormattingSection(): string {
 	return `====
 
 MARKDOWN RULES
 
-ALL responses MUST show ANY \`language construct\` OR filename reference as clickable, exactly as [\`filename OR language.declaration()\`](relative/file/path.ext:line); line is required for \`syntax\` and optional for filename links. This applies to ALL markdown responses and ALSO those in <attempt_completion>`
+ALL responses MUST show ANY \`language construct\` OR filename reference as clickable, exactly as [\`filename OR language.declaration()\`](./relative/file/path.ext:line); line is required for \`syntax\` and optional for filename links. This applies to ALL markdown responses and ALSO those in <attempt_completion>
+line can be a single line (e.g. [func](./path.ext:12)) or a range (e.g. [func](./path.ext:12-25)).
+`
 }
diff --git a/src/core/prompts/sections/mcp-servers.ts b/src/core/prompts/sections/mcp-servers.ts
index 643233ab..d0f508d9 100644
--- a/src/core/prompts/sections/mcp-servers.ts
+++ b/src/core/prompts/sections/mcp-servers.ts
@@ -1,10 +1,12 @@
 import { DiffStrategy } from "../../../shared/tools"
 import { McpHub } from "../../../services/mcp/McpHub"
+import { config } from "@dotenvx/dotenvx"
 
 export async function getMcpServersSection(
 	mcpHub?: McpHub,
 	diffStrategy?: DiffStrategy,
 	enableMcpServerCreation?: boolean,
+	mode?: string
 ): Promise<string> {
 	if (!mcpHub) {
 		return ""
@@ -15,6 +17,17 @@ export async function getMcpServersSection(
 			? `${mcpHub
 					.getServers()
 					.filter((server) => server.status === "connected")
+					.filter((server) => {
+						if (!mode) return true
+						const cfg = server.config ? JSON.parse(server.config) : {}
+						const enabledModes = cfg.enabledModes || []
+						const disabledModes = cfg.disabledModes || []
+						
+						if (enabledModes.length > 0) {
+							return enabledModes.includes(mode) && !disabledModes.includes(mode)
+						}
+						return !disabledModes.includes(mode)
+					})
 					.map((server) => {
 						const tools = server.tools
 							?.filter((tool) => tool.enabledForPrompt !== false)
diff --git a/src/core/prompts/sections/rules.ts b/src/core/prompts/sections/rules.ts
index a5eaf23c..9db53017 100644
--- a/src/core/prompts/sections/rules.ts
+++ b/src/core/prompts/sections/rules.ts
@@ -50,6 +50,7 @@ export function getRulesSection(
 	supportsComputerUse: boolean,
 	diffStrategy?: DiffStrategy,
 	codeIndexManager?: CodeIndexManager,
+	allowedMultiCall?: boolean,
 ): string {
 	const isCodebaseSearchAvailable =
 		codeIndexManager &&
@@ -57,10 +58,164 @@ export function getRulesSection(
 		codeIndexManager.isFeatureConfigured &&
 		codeIndexManager.isInitialized
 
+	const allowedMultiCallEnabled = allowedMultiCall ?? false 
+	const rulesPrompt = `
+====
+
+# RULES
+
+${allowedMultiCallEnabled ? 
+	"1. If multiple actions are needed, you can use multiple tools at a time per message to accomplish the task iteratively, with each tool use being informed by the result of the previous tool use. You should actively use analytical tools to obtain broader clues. \n2. If you need to read multiple files in a single message, you MUST use the \`read_file\` tool's method to read multiple files in one call, rather than calling the read_file tool multiple times. Reserve more calls for search tools.\n"
+	: "1. If multiple actions are needed, use one tool at a time per message to accomplish the task iteratively, with each tool use being informed by the result of the previous tool use. Do not assume the outcome of any tool use. Each step must be informed by the previous step's result. \n2. By waiting for and carefully considering the user's or tools' response after each tool use, you can react accordingly and make informed decisions about how to proceed with the task. This iterative process helps ensure the overall success and accuracy of your work.\n"}
+3. You should always prefer using other editing tools over write_to_file when making changes to existing files since write_to_file is much slower and cannot handle large files.
+4. When using editing tools to modify a file, use the tool directly with the desired content. You do not need to display the content before using the tool. ALWAYS provide the COMPLETE file content in your response. This is NON-NEGOTIABLE. Partial updates or placeholders like '// rest of code unchanged' are STRICTLY FORBIDDEN. You MUST include ALL parts of the file, even if they haven't been modified. Failure to do so will result in incomplete or broken code, severely impacting the user's project.
+5. When making changes to code, always consider the context in which the code is being used. Ensure that your changes are compatible with the existing codebase and that they follow the project's coding standards and best practices.	
+6. When executing commands, if you don't see the expected output, use the ask_followup_question tool to request the user to copy and paste it back to you.
+7. You MUST use the \`attempt_completion\` tool to show your conclusion. When using the \`attempt_completion\` tool, the <result></result> tag must contain a complete summary of the work. NEVER let the user search for the result in the historical conversation. For example, in the attempt_completion result, you cannot say "I have already summarized the result in the historical conversation."
+8. You MUST ensure that all conclusions presented in the \`attempt_completion\` tool are closely related to and precise about the user's task. DO NOT include content unrelated to the user's task.
+9. Use the available search tools to understand the codebase and the user's query. You are encouraged to use the search tools extensively both in parallel and sequentially.
+10. Be thorough: Check multiple locations, consider different naming conventions, look for related files. 
+11. For analysis: Start broad and narrow down. Use multiple search strategies if the first doesn't yield results.
+12. ${isCodebaseSearchAvailable?"**CRITICAL: For ANY exploration of code you haven't examined yet in this conversation, you MUST use the `codebase_search` tool FIRST before using search_files or other file exploration tools.** This requirement applies throughout the entire conversation, not just when starting a task. The codebase_search tool uses semantic search to find relevant code based on meaning, not just keywords, making it much more effective for understanding how features are implemented. Even if you've already explored some parts of the codebase, any new area or functionality you need to understand requires using codebase_search first. The “codebase_search” can help you start from an unknown field but cannot help you find all clues, as it will lose some more accurate and detailed information. Therefore, you **SHOULD NOT** rely entirely on “codebase_search” and should use more explicitly controllable tools like \`search_files\` (Glob/Grep), \`read_file\`, \`list_code_definition_names\` after obtaining the clue. Implement the solution using all tools available to you.":"Implement the solution using all tools available to you. "}
+13. After finding contextual information related to the issue, you should still perform redundant searches using the additional key information and reflect to **ENSURE that no content related to the task is missed**.
+14. IMPORTANT: If you want to reference code in a file, you MUST use a markdown-formatted link pointing to the location of the source code, rather than outputting it as texts or code blocks. It allows you to direct the user to easily navigate to the source code location.
+15. You must complete all pending tasks before you can call the \`attempt_completion\` tool to end the task.
+16. For all file paths, use markdown-formatted links to point to the source files.
+17. You should conduct periodic reviews and reflections at appropriate times, asking yourself if you have missed any clues or key points.
+18. Don't be stingy with search attempts; use various keywords or matching patterns from multiple angles to conduct a broad search.
+19. For key content or logic encountered during the process of understanding source code, you MUST use the search tool to perform verification searches to determine their scope of influence.
+20. Each time you call the \`read_file\` tool, you should try to read as many of your interested files as possible simultaneously to save on \`read_file\` tool calls and obtain more information.
+
+==== 
+
+# TONE and STYLE 
+You should be accurate, concise, direct, and to the point.
+You MUST answer concisely with fewer than 4 lines (not including tool use or code generation), unless user asks for detail.
+IMPORTANT: You should minimize output tokens as much as possible while maintaining helpfulness, quality, and accuracy. 
+IMPORTANT: You should NOT answer with unnecessary preamble or postamble (such as explaining your code or summarizing your action), unless the user asks you to. 
+IMPORTANT: DO NOT begin the response with exclamations like "我知道了！" or "太棒了！" and so on. Maintain calm and professional.
+IMPORTANT: Answer the user's question directly, avoiding any elaboration, explanation, introduction, conclusion, or excessive details. You MUST AVOID text before/after your response, such as "The answer is <answer>.", "Here is the content of the file..." or "Based on the information provided, the answer is..." or "Here is what I will do next..." or "现在让我来总结一下我的发现：..." or "现在我理解了xxx..." or "让我来（做一件事）...".
+
+==== 
+
+# PROACTIVENESS
+
+You are allowed to be proactive, but only when the user asks you to do something. You should strive to strike a balance between:
+- Doing the right thing when asked, including taking actions and follow-up actions
+- Not surprising the user with actions you take without asking
+
+For example, if the user asks you how to approach something, you should do your best to answer their question first, and not immediately jump into taking actions.
+
+====
+
+# PROFESSIONAL OBJECTIVITY
+
+Prioritize technical accuracy and truthfulness over validating the user's beliefs. Focus on facts and problem-solving, providing direct, objective technical info without any unnecessary superlatives, praise, or emotional validation. It is best for the user if Claude honestly applies the same rigorous standards to all ideas and disagrees when necessary, even if it may not be what the user wants to hear. Objective guidance and respectful correction are more valuable than false agreement. Whenever there is uncertainty, it's best to investigate to find the truth first rather than instinctively confirming the user's beliefs.
+
+====
+
+# TASK MANAGEMENT
+
+You have access to the \`update_todo_list\` tools to help you manage and plan tasks. Use these tools **VERY frequently** to ensure that you are tracking your tasks and giving the user visibility into your progress.
+These tools are also **EXTREMELY helpful** for planning tasks, and for breaking down larger complex tasks into smaller steps. If you do not use this tool when planning, you may forget to do important tasks - and that is unacceptable.
+
+It is critical that you mark todos as completed as soon as you are done with a task. Do not batch up multiple tasks before marking them as completed.
+
+Examples:
+<example>
+user: Run the build and fix any type errors
+assistant: I'm going to use the \`update_todo_list\` tool to write the following items to the todo list:
+- Run the build
+- Fix any type errors
+
+I'm now going to run the build using Bash.
+
+Looks like I found 10 type errors. I'm going to use the \`update_todo_list\` tool to write 10 items to the todo list.
+
+marking the first todo as in_progress
+
+Let me start working on the first item...
+
+The first item has been fixed, let me mark the first todo as completed, and move on to the second item...
+..
+..
+</example>
+
+<example>
+user: Help me write a new feature that allows users to track their usage metrics and export them to various formats
+assistant: I'll help you implement a usage metrics tracking and export feature. Let me first use the \`update_todo_list\` tool to plan this task.
+Adding the following todos to the todo list:
+1. Research existing metrics tracking in the codebase
+2. Design the metrics collection system
+3. Implement core metrics tracking functionality
+4. Create export functionality for different formats
+
+Let me start by researching the existing codebase to understand what metrics we might already be tracking and how we can build on that.
+
+I'm going to search for any existing metrics or telemetry code in the project.
+
+I've found some existing telemetry code. Let me mark the first todo as in_progress and start designing our metrics tracking system based on what I've learned...
+
+[Assistant continues implementing the feature step by step, marking todos as in_progress and completed as they go]
+</example>
+
+`
+	return rulesPrompt
+
 	const codebaseSearchRule = isCodebaseSearchAvailable
-		? "- **CRITICAL: For ANY exploration of code you haven't examined yet in this conversation, you MUST use the `codebase_search` tool FIRST before using search_files or other file exploration tools.** This requirement applies throughout the entire conversation, not just when starting a task. The codebase_search tool uses semantic search to find relevant code based on meaning, not just keywords, making it much more effective for understanding how features are implemented. Even if you've already explored some parts of the codebase, any new area or functionality you need to understand requires using codebase_search first.\n"
+		? "- **CRITICAL: For ANY exploration of code you haven't examined yet in this conversation, you MUST use the `codebase_search` tool FIRST before using search_files or other file exploration tools.** This requirement applies throughout the entire conversation, not just when starting a task. The codebase_search tool uses semantic search to find relevant code based on meaning, not just keywords, making it much more effective for understanding how features are implemented. Even if you've already explored some parts of the codebase, any new area or functionality you need to understand requires using codebase_search first. The “codebase_search” can help you start from an unknown field but cannot help you find all clues, as it will lose some more accurate and detailed information. Therefore, you **SHOULD NOT** rely entirely on “codebase_search” and should use more explicitly controllable tools like search_files (for regex patterns), read_file, list_code_definition_names.\n"
 		: ""
 
+// # 最佳实践
+
+// ## 获取上下文
+// - `codebase_search` 工具非常强大，但是他得到的结果准确率不高，且不够全面，会丢失很多相关的信息。进行一次搜索后，你应当结合搜索出来的内容，从中提取有效信息，进行问题重写，重新设计query，进行更广泛的搜索。
+// - 由于`codebase_search` 会遗失信息的原因，你应该在适当的时候，结合你已知的信息，深度理解已知代码，开始使用 search_files (for regex patterns) 和 list_code_definition_names 来进行更准确的、更完整的搜索。通过这些工具更加全面地了解代码结构。
+// - 在此之后，可以使用 read_file 等其他工具获取上下文工具，获取最完整最细致的上下文信息。
+// - 在使用 read_file 工具时，你应当准确定位你想要读的部分（行号范围），而不是笼统的阅读整个文件。
+// - **整个获取上下文的过程，你使用工具的精力占比应当为，20%的时间使用`codebase_search`（少量时间，仅用于最开始，切入问题，定位大致范围），30% 时间使用 search_files (for regex patterns) 和 list_code_definition_names 来进行更准确的、更完整的搜索，理解项目结构，50%的时间 使用read_file以及其他工具获得更详细的上下文信息，理解具体实现**
+
+// # 最佳实践
+
+// ## 获取上下文
+// - 最开始，你应该通过`list_files` ，了解项目的目录结构，通过文件或目录名称，大概获悉每个文件或目录的作用，使用list_code_definition_names工具了解文件中的代码结构
+// - 可以使用 read_file 工具对感兴趣的文件进行阅读，根据list_code_definition_names获得的信息，可以只节选部分段落（部分行号范围）以确认文件内容和任务是否相关。
+// - 找到相关的代码线索后，结合你已知的信息，深度理解已知代码，开始使用 search_files (for regex patterns) 和 list_code_definition_names 来进行更准确的、更完整的大范围搜索。通过这些工具更加全面地了解代码结构。
+// - 在此之后，可以使用 read_file 等其他工具获取上下文工具，获取最完整最细致的上下文信息。
+// - 在使用 read_file 工具时，你应当准确定位你想要读的部分（行号范围），而不是笼统的阅读整个文件。
+// - **对于每个任务，整个获取上下文的过程，你的精力占比应当为，20%的时间使用`list_files`, `list_code_definition_names`,`read_file`（最开始初步获悉代码结构），30% 时间使用 search_files (for regex patterns) 和 list_code_definition_names 来进行更准确的、更完整的搜索，理解项目结构，50%的时间 使用read_file以及其他工具获得更详细的上下文信息，理解具体实现**
+
+// 	const bestPractices = isCodebaseSearchAvailable
+// ?`====
+
+// # Best Practices
+
+// ## Obtaining Context
+// - The \`codebase_search\` tool is very powerful, but its results can be inaccurate and incomplete, often missing a lot of relevant information. After an initial search, you should analyze the results, extract useful information, rewrite your query, and design a new, broader search.
+// - Because \`codebase_search\` can miss information, you should, at the appropriate time and based on the information you already have, deeply understand the known code and begin using \`search_files\` (for regex patterns), \`list_code_definition_names\` and \`list_files\` for more precise and comprehensive searches. Use these tools to gain a more complete understanding of the code structure.
+// - After this, you can use other tools like \`read_file\` to obtain the most complete and detailed contextual information.
+// - When using the \`read_file\` tool, you should precisely locate the specific part you want to read (line number range) rather than reading the entire file wholesale.
+// - **For each task, throughout the entire process of obtaining context, the distribution of your effort in using these tools should be: 20% of your time on \`codebase_search\` (a small amount of time, only at the very beginning to approach the problem and locate the general area), 30% on \`search_files\` (for regex patterns) and \`list_code_definition_names\` to conduct more accurate and complete searches and to understand the project structure, and 50% on \`read_file\` and other tools to get more detailed context and understand the specific implementation.**
+// `:`====
+
+// # Best Practices
+
+// ## Obtaining Context
+// - In the beginning, you should use \`list_files\` to understand the project's directory structure. From the file or directory names, you can get a general idea of the purpose of each. Use the \`list_code_definition_names\` tool to understand the code structure within files.
+// - You can use the \`read_file\` tool to read files you are interested in. Based on the information obtained from \`list_code_definition_names\`, you can select and read only specific sections (a range of line numbers) to confirm if the file's content is relevant to the task.
+// - After finding relevant code clues, combine them with the information you already have to deeply understand the known code. Then, start using \`search_files\` (for regex patterns) and \`list_code_definition_names\` to conduct more accurate and comprehensive large-scale searches. Use these tools to gain a more complete understanding of the code structure.
+// - Following this, you can use \`read_file\` and other context-gathering tools to obtain the most complete and detailed information.
+// - When using the \`read_file\` tool, you should precisely locate the specific part you want to read (line number range) instead of reading the entire file wholesale.
+// - **For each task, the distribution of your effort throughout the context-gathering process should be: 20% of your time on \`list_files\`, \`list_code_definition_names\`, and \`read_file\` (to initially get a basic understanding of the code structure), 30% on \`search_files\` (for regex patterns) and \`list_code_definition_names\` to conduct more accurate and complete searches and to understand the project structure, and 50% on \`read_file\` and other tools to obtain more detailed context and understand the specific implementation.**
+// `
+// 	const bestPracticesTodo = `
+// ## To-do list
+
+// - Ambiguous tasks are not allowed; task work must be clear and distinct, and the wording of tasks must be clear and explicit.
+// - Overlapping is not allowed between different tasks in the task list.
+// - When working according to the task list, only one task should be completed at a time to avoid overly frequent updates to the task list; simple tasks should be combined together.	
+// `
+
 	return `====
 
 RULES
@@ -77,9 +232,9 @@ ${getEditingInstructions(diffStrategy)}
 - Be sure to consider the type of project (e.g. Python, JavaScript, web application) when determining the appropriate structure and files to include. Also consider what files may be most relevant to accomplishing the task, for example looking at a project's manifest file would help you understand the project's dependencies, which you could incorporate into any code you write.
   * For example, in architect mode trying to edit app.js would be rejected because architect mode can only edit files matching "\\.md$"
 - When making changes to code, always consider the context in which the code is being used. Ensure that your changes are compatible with the existing codebase and that they follow the project's coding standards and best practices.
-- Do not ask for more information than necessary. Use the tools provided to accomplish the user's request efficiently and effectively. When you've completed your task, you must use the attempt_completion tool to present the result to the user. The user may provide feedback, which you can use to make improvements and try again.
+- Do not ask for more information than necessary from user. Use tools to obtain them by yourself. Use the tools provided to accomplish the user's request efficiently and effectively. When you've completed your task, you must use the attempt_completion tool to present the result to the user. The user may provide feedback, which you can use to make improvements and try again.
 - You are only allowed to ask the user questions using the ask_followup_question tool. Use this tool only when you need additional details to complete a task, and be sure to use a clear and concise question that will help you move forward with the task. When you ask a question, provide the user with 2-4 suggested answers based on your question so they don't need to do so much typing. The suggestions should be specific, actionable, and directly related to the completed task. They should be ordered by priority or logical sequence. However if you can use the available tools to avoid having to ask the user questions, you should do so. For example, if the user mentions a file that may be in an outside directory like the Desktop, you should use the list_files tool to list the files in the Desktop and check if the file they are talking about is there, rather than asking the user to provide the file path themselves.
-- When executing commands, if you don't see the expected output, assume the terminal executed the command successfully and proceed with the task. The user's terminal may be unable to stream the output back properly. If you absolutely need to see the actual terminal output, use the ask_followup_question tool to request the user to copy and paste it back to you.
+- When executing commands, if you don't see the expected output, use the ask_followup_question tool to request the user to copy and paste it back to you.
 - The user may provide a file's contents directly in their message, in which case you shouldn't use the read_file tool to get the file contents again since you already have it.
 - Your goal is to try to accomplish the user's task, NOT engage in a back and forth conversation.${
 		supportsComputerUse
diff --git a/src/core/prompts/sections/system-info.ts b/src/core/prompts/sections/system-info.ts
index 8adc90a1..db801d65 100644
--- a/src/core/prompts/sections/system-info.ts
+++ b/src/core/prompts/sections/system-info.ts
@@ -11,9 +11,9 @@ SYSTEM INFORMATION
 Operating System: ${osName()}
 Default Shell: ${getShell()}
 Home Directory: ${os.homedir().toPosix()}
-Current Workspace Directory: ${cwd.toPosix()}
+Current Workspace Directory: ${cwd.toPosix()}`
 
-The Current Workspace Directory is the active VS Code project directory, and is therefore the default directory for all tool operations. New terminals will be created in the current workspace directory, however if you change directories in a terminal it will then have a different working directory; changing directories in a terminal does not modify the workspace directory, because you do not have access to change the workspace directory. When the user initially gives you a task, a recursive list of all filepaths in the current workspace directory ('/test/path') will be included in environment_details. This provides an overview of the project's file structure, offering key insights into the project from directory/file names (how developers conceptualize and organize their code) and file extensions (the language used). This can also guide decision-making on which files to explore further. If you need to further explore directories such as outside the current workspace directory, you can use the list_files tool. If you pass 'true' for the recursive parameter, it will list files recursively. Otherwise, it will list files at the top level, which is better suited for generic directories where you don't necessarily need the nested structure, like the Desktop.`
+// The Current Workspace Directory is the active VS Code project directory, and is therefore the default directory for all tool operations. New terminals will be created in the current workspace directory, however if you change directories in a terminal it will then have a different working directory; changing directories in a terminal does not modify the workspace directory, because you do not have access to change the workspace directory. When the user initially gives you a task, a recursive list of all filepaths in the current workspace directory ('/test/path') will be included in environment_details. This provides an overview of the project's file structure, offering key insights into the project from directory/file names (how developers conceptualize and organize their code) and file extensions (the language used). This can also guide decision-making on which files to explore further. If you need to further explore directories such as outside the current workspace directory, you can use the list_files tool. If you pass 'true' for the recursive parameter, it will list files recursively. Otherwise, it will list files at the top level, which is better suited for generic directories where you don't necessarily need the nested structure, like the Desktop.`
 
 	return details
 }
diff --git a/src/core/prompts/sections/tool-use-guidelines.ts b/src/core/prompts/sections/tool-use-guidelines.ts
index a526bbc7..2c068e45 100644
--- a/src/core/prompts/sections/tool-use-guidelines.ts
+++ b/src/core/prompts/sections/tool-use-guidelines.ts
@@ -15,6 +15,7 @@ export function getToolUseGuidelinesSection(codeIndexManager?: CodeIndexManager)
 	guidelinesList.push(
 		`${itemNumber++}. In <thinking> tags, assess what information you already have and what information you need to proceed with the task.`,
 	)
+	guidelinesList.push(`${itemNumber++}. Formulate your tool use using the XML format specified for each tool.`)
 
 	// Conditional codebase search guideline
 	if (isCodebaseSearchAvailable) {
@@ -22,7 +23,7 @@ export function getToolUseGuidelinesSection(codeIndexManager?: CodeIndexManager)
 			`${itemNumber++}. **CRITICAL: For ANY exploration of code you haven't examined yet in this conversation, you MUST use the \`codebase_search\` tool FIRST before any other search or file exploration tools.** This applies throughout the entire conversation, not just at the beginning. The codebase_search tool uses semantic search to find relevant code based on meaning rather than just keywords, making it far more effective than regex-based search_files for understanding implementations. Even if you've already explored some code, any new area of exploration requires codebase_search first.`,
 		)
 		guidelinesList.push(
-			`${itemNumber++}. Choose the most appropriate tool based on the task and the tool descriptions provided. After using codebase_search for initial exploration of any new code area, you may then use more specific tools like search_files (for regex patterns), list_files, or read_file for detailed examination. For example, using the list_files tool is more effective than running a command like \`ls\` in the terminal. It's critical that you think about each available tool and use the one that best fits the current step in the task.`,
+			`${itemNumber++}. Choose the most appropriate tool based on the task and the tool descriptions provided. The “codebase_search” can help you start from an unknown field but cannot help you find all clues, as it will lose some more accurate and detailed information. Therefore, you should not rely entirely on “codebase_search” and should use more explicitly controllable tools like search_files (for regex patterns), read_file, list_code_definition_names. It's critical that you think about each available tool and use the one that best fits the current step in the task. For example using the list_files tool is more effective than running a command like \`ls\` in the terminal. `,
 		)
 	} else {
 		guidelinesList.push(
@@ -34,7 +35,6 @@ export function getToolUseGuidelinesSection(codeIndexManager?: CodeIndexManager)
 	guidelinesList.push(
 		`${itemNumber++}. If multiple actions are needed, use one tool at a time per message to accomplish the task iteratively, with each tool use being informed by the result of the previous tool use. Do not assume the outcome of any tool use. Each step must be informed by the previous step's result.`,
 	)
-	guidelinesList.push(`${itemNumber++}. Formulate your tool use using the XML format specified for each tool.`)
 	guidelinesList.push(`${itemNumber++}. After each tool use, the user will respond with the result of that tool use. This result will provide you with the necessary information to continue your task or make further decisions. This response may include:
   - Information about whether the tool succeeded or failed, along with any reasons for failure.
   - Linter errors that may have arisen due to the changes you made, which you'll need to address.
@@ -56,4 +56,4 @@ It is crucial to proceed step-by-step, waiting for the user's message after each
 4. Ensure that each action builds correctly on the previous ones.
 
 By waiting for and carefully considering the user's response after each tool use, you can react accordingly and make informed decisions about how to proceed with the task. This iterative process helps ensure the overall success and accuracy of your work.`
-}
+}
\ No newline at end of file
diff --git a/src/core/prompts/sections/tool-use.ts b/src/core/prompts/sections/tool-use.ts
index ea27e591..0effaa1c 100644
--- a/src/core/prompts/sections/tool-use.ts
+++ b/src/core/prompts/sections/tool-use.ts
@@ -1,9 +1,11 @@
-export function getSharedToolUseSection(): string {
+export function getSharedToolUseSection(allowedMultiCall?: boolean): string {
+	const allowedMultiCallEnabled = allowedMultiCall ?? false
 	return `====
 
 TOOL USE
 
-You have access to a set of tools that are executed upon the user's approval. You can use one tool per message, and will receive the result of that tool use in the user's response. You use tools step-by-step to accomplish a given task, with each tool use informed by the result of the previous tool use.
+${ allowedMultiCallEnabled ? "You have access to a set of tools that are executed upon the user's approval. You can use multiple tools per message, and will receive the result of that tool use in the user's response. "
+:"You have access to a set of tools that are executed upon the user's approval. You can use one tool per message, and will receive the result of that tool use in the user's response. You use tools step-by-step to accomplish a given task, with each tool use informed by the result of the previous tool use."}
 
 # Tool Use Formatting
 
@@ -15,7 +17,7 @@ Tool uses are formatted using XML-style tags. The tool name itself becomes the X
 ...
 </actual_tool_name>
 
-For example, to use the new_task tool:
+For example, to use the sub agent tool:
 
 <new_task>
 <mode>code</mode>
diff --git a/src/core/prompts/system.ts b/src/core/prompts/system.ts
index cbe91903..e8e3c237 100644
--- a/src/core/prompts/system.ts
+++ b/src/core/prompts/system.ts
@@ -60,7 +60,6 @@ async function generatePrompt(
 	rooIgnoreInstructions?: string,
 	partialReadsEnabled?: boolean,
 	settings?: SystemPromptSettings,
-	todoList?: TodoItem[],
 ): Promise<string> {
 	if (!context) {
 		throw new Error("Extension context is required for generating system prompt")
@@ -81,17 +80,20 @@ async function generatePrompt(
 	const [modesSection, mcpServersSection] = await Promise.all([
 		getModesSection(context),
 		shouldIncludeMcp
-			? getMcpServersSection(mcpHub, effectiveDiffStrategy, enableMcpServerCreation)
+			? getMcpServersSection(mcpHub, effectiveDiffStrategy, enableMcpServerCreation, mode)
 			: Promise.resolve(""),
 	])
 
 	const codeIndexManager = CodeIndexManager.getInstance(context)
 
+	// Extract allowedMultiCall from experiments
+	const allowedMultiCall = experiments?.allowedMultiCall ?? false
+
 	const basePrompt = `${roleDefinition}
 
 ${markdownFormattingSection()}
 
-${getSharedToolUseSection()}
+${getSharedToolUseSection(allowedMultiCall)}
 
 ${getToolDescriptionsForMode(
 	mode,
@@ -107,19 +109,19 @@ ${getToolDescriptionsForMode(
 	settings,
 )}
 
-${getToolUseGuidelinesSection(codeIndexManager)}
+${true?"":getToolUseGuidelinesSection(codeIndexManager)}
 
 ${mcpServersSection}
 
-${getCapabilitiesSection(cwd, supportsComputerUse, shouldIncludeMcp ? mcpHub : undefined, effectiveDiffStrategy, codeIndexManager)}
+${true?"":getCapabilitiesSection(cwd, supportsComputerUse, shouldIncludeMcp ? mcpHub : undefined, effectiveDiffStrategy, codeIndexManager)}
 
 ${modesSection}
 
-${getRulesSection(cwd, supportsComputerUse, effectiveDiffStrategy, codeIndexManager)}
+${getRulesSection(cwd, supportsComputerUse, effectiveDiffStrategy, codeIndexManager, allowedMultiCall)}
 
 ${getSystemInfoSection(cwd)}
 
-${getObjectiveSection(codeIndexManager, experiments)}
+${true?"":getObjectiveSection(codeIndexManager, experiments)}
 
 ${await addCustomInstructions(baseInstructions, globalCustomInstructions || "", cwd, mode, {
 	language: language ?? formatLanguage(vscode.env.language),
@@ -148,7 +150,6 @@ export const SYSTEM_PROMPT = async (
 	rooIgnoreInstructions?: string,
 	partialReadsEnabled?: boolean,
 	settings?: SystemPromptSettings,
-	todoList?: TodoItem[],
 ): Promise<string> => {
 	if (!context) {
 		throw new Error("Extension context is required for generating system prompt")
@@ -170,6 +171,29 @@ export const SYSTEM_PROMPT = async (
 	// Get full mode config from custom modes or fall back to built-in modes
 	const currentMode = getModeBySlug(mode, customModes) || modes.find((m) => m.slug === mode) || modes[0]
 
+	if (mode === "native") {
+		const { roleDefinition, baseInstructions: baseInstructionsForFile } = getModeSelection(
+			mode,
+			promptComponent,
+			customModes,
+		)
+		const customInstructions = await addCustomInstructions(
+			baseInstructionsForFile,
+			globalCustomInstructions || "",
+			cwd,
+			mode,
+			{
+				language: language ?? formatLanguage(vscode.env.language),
+				rooIgnoreInstructions,
+				settings,
+			},
+		)
+
+		return `${roleDefinition}
+
+${customInstructions}`
+	}
+
 	// If a file-based custom system prompt exists, use it
 	if (fileCustomSystemPrompt) {
 		const { roleDefinition, baseInstructions: baseInstructionsForFile } = getModeSelection(
@@ -219,6 +243,5 @@ ${customInstructions}`
 		rooIgnoreInstructions,
 		partialReadsEnabled,
 		settings,
-		todoList,
 	)
 }
diff --git a/src/core/prompts/tools/attempt-completion.ts b/src/core/prompts/tools/attempt-completion.ts
index 904f9899..8d30af75 100644
--- a/src/core/prompts/tools/attempt-completion.ts
+++ b/src/core/prompts/tools/attempt-completion.ts
@@ -5,7 +5,7 @@ export function getAttemptCompletionDescription(args?: ToolArgs): string {
 Description: After each tool use, the user will respond with the result of that tool use, i.e. if it succeeded or failed, along with any reasons for failure. Once you've received the results of tool uses and can confirm that the task is complete, use this tool to present the result of your work to the user. The user may respond with feedback if they are not satisfied with the result, which you can use to make improvements and try again.
 IMPORTANT NOTE: This tool CANNOT be used until you've confirmed from the user that any previous tool uses were successful. Failure to do so will result in code corruption and system failure. Before using this tool, you must ask yourself in <thinking></thinking> tags if you've confirmed from the user that any previous tool uses were successful. If not, then DO NOT use this tool.
 Parameters:
-- result: (required) The result of the task. Formulate this result in a way that is final and does not require further input from the user. Don't end your result with questions or offers for further assistance.
+- result: (required) The result of the task. Formulate this result in a way that is final and does not require further input from the user. Don't end your result with questions or offers for further assistance. The content of <result> needs to describe in detail the conclusions and reports obtained, do not point to past conversations in <result>, and do not make users search for conclusions in historical conversations.
 Usage:
 <attempt_completion>
 <result>
diff --git a/src/core/prompts/tools/codebase-search.ts b/src/core/prompts/tools/codebase-search.ts
index 0fc8f68f..6e0ec175 100644
--- a/src/core/prompts/tools/codebase-search.ts
+++ b/src/core/prompts/tools/codebase-search.ts
@@ -1,9 +1,21 @@
 export function getCodebaseSearchDescription(): string {
 	return `## codebase_search
-Description: Find files most relevant to the search query.\nThis is a semantic search tool, so the query should ask for something semantically matching what is needed.\nIf it makes sense to only search in a particular directory, please specify it in the path parameter.\nUnless there is a clear reason to use your own search query, please just reuse the user's exact query with their wording.\nTheir exact wording/phrasing can often be helpful for the semantic search query. Keeping the same exact question format can also be helpful.\nIMPORTANT: Queries MUST be in English. Translate non-English queries before searching.
+### codebase_search (Search)
+Description: This tool performs a semantic search on a vector database of code and documentation. It retrieves the most relevant contextual information needed to answer user questions or resolve their requirements. The search is based on semantic meaning, not just keyword matching.
+
+When generating a 'query', follow these guidelines:
+
+- **Extract from Code:** If the conversation includes code snippets, extract key identifiers like class names, function names, method names, or variable names. These are often the most crucial elements to search for to understand the code's purpose and functionality.
+- **Infer from Context:** Go beyond the literal words in the conversation.
+    - **For Code-related Questions:** Infer potential function names, class names, or design patterns that might exist in the codebase to solve the user's problem.
+    - **For Documentation-related Questions:** Infer concepts, features, or "how-to" topics that would likely be covered in the documentation.
+- **Be Specific and Clear:**
+    - Formulate clear, descriptive queries. Avoid using overly short or ambiguous abbreviations.
+    - If the context strongly suggests the information is in a specific location, use the 'path' parameter to narrow the search.
+
 Parameters:
-- query: (required) The search query to find relevant code. You should reuse the user's exact query/most recent message with their wording unless there is a clear reason not to.
-- path: (optional) The path to the directory to search in relative to the current working directory. This parameter should only be a directory path, file paths are not supported. Defaults to the current working directory.
+- query: (required) A semantic query (or queries) to find relevant code or documentation. You can provide up to 4 queries, separated by " | ". Each query should be a meaningful phrase (at least 4 Chinese characters or 2 English words). Provide queries in both Chinese and English. 
+- path: (optional) The relative path to a file or directory to restrict the search. Defaults to the entire codebase.
 Usage:
 <codebase_search>
 <query>Your natural language query here</query>
@@ -15,5 +27,29 @@ Example: Searching for functions related to user authentication
 <query>User login and password hashing</query>
 <path>/path/to/directory</path>
 </codebase_search>
+
+
+### codebase_search (Summary)
+Description: Generates a detailed summary of a file or a directory's contents.
+
+This tool provides a high-level overview to help you quickly understand a codebase.
+- **If the path points to a file:** It returns a summary of the entire file, plus summaries of key sections (e.g., classes, functions) with their corresponding line numbers.
+- **If the path points to a directory:** It returns summaries for all supported files within that directory.
+
+Use this tool when you need to grasp the purpose and structure of a file or directory before diving into the details.
+
+**Important Note:** The tool is named 'codebase_search', but its function in this parameters rule is to **summarize**, not to search for a query.
+
+Parameters:
+- path: (optional) The relative path to the file or directory to be summarized. Defaults to the current working directory ('.').
+Usage:
+<codebase_search>
+<path>Path to the directory or file to summarize (optional)</path>
+</codebase_search>
+
+Example: Get a summary of a specific file or all supported files in '/path/to/directory_or_file'.
+<codebase_search>
+<path>/path/to/directory_or_file</path>
+</codebase_search>
 `
 }
diff --git a/src/core/prompts/tools/index.ts b/src/core/prompts/tools/index.ts
index 9f4af7f3..f5df6589 100644
--- a/src/core/prompts/tools/index.ts
+++ b/src/core/prompts/tools/index.ts
@@ -23,6 +23,8 @@ import { getSwitchModeDescription } from "./switch-mode"
 import { getNewTaskDescription } from "./new-task"
 import { getCodebaseSearchDescription } from "./codebase-search"
 import { getUpdateTodoListDescription } from "./update-todo-list"
+import { getWebSearchDescription } from "./web-search"
+import { getUrlFetchDescription } from "./url-fetch"
 import { CodeIndexManager } from "../../../services/code-index/manager"
 
 // Map of tool names to their description functions
@@ -47,6 +49,8 @@ const toolDescriptionMap: Record<string, (args: ToolArgs) => string | undefined>
 	apply_diff: (args) =>
 		args.diffStrategy ? args.diffStrategy.getToolDescription({ cwd: args.cwd, toolOptions: args.toolOptions }) : "",
 	update_todo_list: (args) => getUpdateTodoListDescription(args),
+	web_search: (args) => getWebSearchDescription(args),
+	url_fetch: (args) => getUrlFetchDescription(args),
 }
 
 export function getToolDescriptionsForMode(
diff --git a/src/core/prompts/tools/list-files.ts b/src/core/prompts/tools/list-files.ts
index 96c43ea4..15aa7956 100644
--- a/src/core/prompts/tools/list-files.ts
+++ b/src/core/prompts/tools/list-files.ts
@@ -16,5 +16,9 @@ Example: Requesting to list all files in the current directory
 <list_files>
 <path>.</path>
 <recursive>false</recursive>
-</list_files>`
+</list_files>
+
+
+Tips: You should use the \`list_files\` tool as LITTLE as possible, and instead use the \`search_files\` (Grep/Glob) tool, which is a powerful search tool that also has the function of matching specific filenames in a directory according to certain patterns.`
 }
+// IMPORTANT: Use \`list_files\` as LITTLE as possible, and instead use the \`Glob\` usage of \`search_files\` for more precise matching searches.
diff --git a/src/core/prompts/tools/new-task.ts b/src/core/prompts/tools/new-task.ts
index 7301b7b4..3c9d6355 100644
--- a/src/core/prompts/tools/new-task.ts
+++ b/src/core/prompts/tools/new-task.ts
@@ -1,12 +1,12 @@
 import { ToolArgs } from "./types"
 
 export function getNewTaskDescription(_args: ToolArgs): string {
-	return `## new_task
-Description: This will let you create a new task instance in the chosen mode using your provided message.
+	return `## sub_agent
+Description: This will let you create a sub agent instance in the chosen mode using your provided message.
 
 Parameters:
-- mode: (required) The slug of the mode to start the new task in (e.g., "code", "debug", "architect").
-- message: (required) The initial user message or instructions for this new task.
+- mode: (required) The slug of the mode to start the sub agent in (e.g., "code", "debug", "architect").
+- message: (required) The initial user message or instructions for this sub agent.
 
 Usage:
 <new_task>
diff --git a/src/core/prompts/tools/search-files.ts b/src/core/prompts/tools/search-files.ts
index fe8b0fc6..588debf7 100644
--- a/src/core/prompts/tools/search-files.ts
+++ b/src/core/prompts/tools/search-files.ts
@@ -1,23 +1,51 @@
 import { ToolArgs } from "./types"
 
 export function getSearchFilesDescription(args: ToolArgs): string {
-	return `## search_files
+	return `## search_files (Grep/Glob tool)
 Description: Request to perform a regex search across files in a specified directory, providing context-rich results. This tool searches for patterns or specific content across multiple files, displaying each match with encapsulating context.
+
+- ALWAYS use Grep for search tasks. NEVER invoke \`grep\` or \`rg\` as a Bash command. The Grep tool has been optimized for correct permissions and access.
+- Supports full regex syntax (e.g., "log.*Error", "function\s+\w+")
+- Filter files with glob parameter (e.g., "*.js", "**/*.tsx")
+- Output modes: "content" shows matching lines, "files_with_matches" shows only file paths (default)
+- Use Task tool for open-ended searches requiring multiple rounds
+- Pattern syntax: Uses ripgrep (not grep) - literal braces need escaping (use \`interface\\{\\}\` to find \`interface{}\` in Go code)
+
 Parameters:
 - path: (required) The path of the directory to search in (relative to the current workspace directory ${args.cwd}). This directory will be recursively searched.
 - regex: (required) The regular expression pattern to search for. Uses Rust regex syntax.
-- file_pattern: (optional) Glob pattern to filter files (e.g., '*.ts' for TypeScript files). If not provided, it will search all files (*).
+- file_pattern: (optional) Glob pattern to filter files, MUST conform to the syntax of the \`--glob\` parameter of the rg command (e.g., '*.ts' for TypeScript files, '*.{ts,js,json}' for TypeScript, JavaScript and JSON files). If not provided, it will search all files (*).
+- output_mode: (optional) Output mode for the search results. Can be "content" (default, shows file content with matches) or "files_with_matches" (only shows the list of files that contain matches without content).
+
 Usage:
 <search_files>
 <path>Directory path here</path>
 <regex>Your regex pattern here</regex>
 <file_pattern>file pattern here (optional)</file_pattern>
+<output_mode>content or files_with_matches (optional)</output_mode>
 </search_files>
 
+Example: Requesting to search for 'abc' in .ts files and .js files in the current directory
+<search_files>
+<path>.</path>
+<regex>abc</regex>
+<file_pattern>*.{ts,js}</file_pattern>
+<output_mode>files_with_matches</output_mode>
+</search_files>
+
+
+### Glob Tips:
+
+- You can use the "files_with_matches" mode, fill in ".*" for the regex content to quickly match the file names you need.
+- Use this method when you need to find files by name patterns as a fast file pattern matching tool.
+- Supports glob patterns like "**/*.js" or "src/**/*.ts"
+
 Example: Requesting to search for all .ts files in the current directory
 <search_files>
 <path>.</path>
 <regex>.*</regex>
 <file_pattern>*.ts</file_pattern>
-</search_files>`
+<output_mode>files_with_matches</output_mode>
+</search_files>
+`
 }
diff --git a/src/core/prompts/tools/update-todo-list.ts b/src/core/prompts/tools/update-todo-list.ts
index 30100617..9ff08aca 100644
--- a/src/core/prompts/tools/update-todo-list.ts
+++ b/src/core/prompts/tools/update-todo-list.ts
@@ -7,7 +7,10 @@ export function getUpdateTodoListDescription(args?: ToolArgs): string {
 	return `## update_todo_list
 
 **Description:**
-Replace the entire TODO list with an updated checklist reflecting the current state. Always provide the full list; the system will overwrite the previous one. This tool is designed for step-by-step task tracking, allowing you to confirm completion of each step before updating, update multiple task statuses at once (e.g., mark one as completed and start the next), and dynamically add new todos discovered during long or complex tasks.
+Use this tool to create and manage a structured task list for your current coding session. This helps you track progress, organize complex tasks, and demonstrate thoroughness to the user.
+It also helps the user understand the progress of the task and overall progress of their requests.
+
+Create a TODO list or replace the entire TODO list with an updated checklist reflecting the current state. Always provide the full list; if the task list is already included, the system will overwrite the previous one. This tool is designed for step-by-step task tracking, allowing you to confirm completion of each step before updating, update multiple task statuses at once (e.g., mark one as completed and start the next), and dynamically add new todos discovered during long or complex tasks.
 
 **Checklist Format:**
 - Use a single-level markdown checklist (no nesting or subtasks).
@@ -20,7 +23,7 @@ Replace the entire TODO list with an updated checklist reflecting the current st
 **Status Rules:**
 - [ ] = pending (not started)
 - [x] = completed (fully finished, no unresolved issues)
-- [-] = in_progress (currently being worked on)
+- [-] = in progress (currently being worked on)
 
 **Core Principles:**
 - Before updating, always confirm which todos have been completed since the last update.
@@ -29,48 +32,226 @@ Replace the entire TODO list with an updated checklist reflecting the current st
 - Do not remove any unfinished todos unless explicitly instructed.
 - Always retain all unfinished tasks, updating their status as needed.
 - Only mark a task as completed when it is fully accomplished (no partials, no unresolved dependencies).
-- If a task is blocked, keep it as in_progress and add a new todo describing what needs to be resolved.
+- If a task is blocked, keep it as in progress and add a new todo describing what needs to be resolved.
 - Remove tasks only if they are no longer relevant or if the user requests deletion.
+- Tasks are divided into three types: \`analysis\`, \`planning\`, \`editing\`. When creating a to-do task, please categorize it into these three types and mark the beginning of the task with [xxx]
 
 **Usage Example:**
 <update_todo_list>
 <todos>
-[x] Analyze requirements
-[x] Design architecture
-[-] Implement core logic
-[ ] Write tests
-[ ] Update documentation
+[x] [analysis] Analyze requirements
+[x] [planning] Design architecture
+[-] [editing] Implement core logic
+[ ] [editing] Write tests
+[ ] [editing] Update documentation
 </todos>
 </update_todo_list>
 
 *After completing "Implement core logic" and starting "Write tests":*
 <update_todo_list>
 <todos>
-[x] Analyze requirements
-[x] Design architecture
-[x] Implement core logic
-[-] Write tests
-[ ] Update documentation
-[ ] Add performance benchmarks
+[x] [analysis] Analyze requirements
+[x] [planning] Design architecture
+[x] [editing] Implement core logic
+[-] [editing] Write tests
+[ ] [editing] Update documentation
+[ ] [editing] Add performance benchmarks
 </todos>
 </update_todo_list>
 
-**When to Use:**
-- The task is complicated or involves multiple steps or requires ongoing tracking.
-- You need to update the status of several todos at once.
-- New actionable items are discovered during task execution.
-- The user requests a todo list or provides multiple tasks.
-- The task is complex and benefits from clear, stepwise progress tracking.
-
-**When NOT to Use:**
-- There is only a single, trivial task.
-- The task can be completed in one or two simple steps.
-- The request is purely conversational or informational.
-
-**Task Management Guidelines:**
-- Mark task as completed immediately after all work of the current task is done.
-- Start the next task by marking it as in_progress.
-- Add new todos as soon as they are identified.
-- Use clear, descriptive task names.
+**When to Use This Tool:**
+ 
+Use this tool proactively in these scenarios:
+
+1.  Complex multi-step tasks - When a task requires 3 or more distinct steps or actions
+2.  Non-trivial and complex tasks - Tasks that require careful planning or multiple operations
+3.  User explicitly requests todo list - When the user directly asks you to use the todo list
+4.  User provides multiple tasks - When users provide a list of things to be done (numbered or comma-separated)
+5.  After receiving new instructions - Immediately capture user requirements as todos
+6.  When you start working on a task - Mark it as [-](\`in progress\`) BEFORE beginning work. Ideally you should only have one todo as [-](\`in progress)\` at a time
+7.  After completing a task - Mark it as [x](\`completed\`) and add any new follow-up tasks discovered during implementation
+
+
+**When NOT to Use This Tool:**
+
+Skip using this tool when:
+1.  There is only a single, straightforward task
+2.  The task is trivial and tracking it provides no organizational benefit
+3.  The task can be completed in less than 3 trivial steps
+4.  The task is purely conversational or informational
+
+> **NOTE** that you should not use this tool if there is only one trivial task to do. In this case you are better off just doing the task directly.
+
+
+
+### Examples of When to Use the Todo List
+
+#### Example 1
+
+> **User:** I want to add a dark mode toggle to the application settings. Make sure you run the tests and build when you're done!
+>
+> **Assistant:** I'll help add a dark mode toggle to your application settings. Let me create a todo list to track this implementation.
+>
+> *Creates todo list with the following items:*
+> 1.  Creating dark mode toggle component in Settings page
+> 2.  Adding dark mode state management (context/store)
+> 3.  Implementing CSS-in-JS styles for dark theme
+> 4.  Updating existing components to support theme switching
+> 5.  Running tests and build process, addressing any failures or errors that occur
+>
+> *Begins working on the first task*
+>
+> **Reasoning:**
+> The assistant used the todo list because:
+> 1.  Adding dark mode is a multi-step feature requiring UI, state management, and styling changes
+> 2.  The user explicitly requested tests and build be run afterward
+> 3.  The assistant inferred that tests and build need to pass by adding "Ensure tests and build succeed" as the final task
+>
+#### Example 2
+
+> **User:** Help me rename the function \`getCwd\` to \`getCurrentWorkingDirectory\` across my project
+>
+> **Assistant:** Let me first search through your codebase to find all occurrences of 'getCwd'.
+>
+> *Uses \`grep\` or search tools to locate all instances of \`getCwd\` in the codebase*
+>
+> **Assistant:** I've found 15 instances of 'getCwd' across 8 different files. Let me create a todo list to track these changes.
+>
+> *Creates todo list with specific items for each file that needs updating*
+>
+> **Reasoning:**
+> The assistant used the todo list because:
+> 1.  First, the assistant searched to understand the scope of the task
+> 2.  Upon finding multiple occurrences across different files, it determined this was a complex task with multiple steps
+> 3.  The todo list helps ensure every instance is tracked and updated systematically
+> 4.  This approach prevents missing any occurrences and maintains code consistency
+
+#### Example 3
+
+> **User:** I need to implement these features for my e-commerce site: user registration, product catalog, shopping cart, and checkout flow.
+>
+> **Assistant:** I'll help implement these features. First, let's add all the features to the todo list.
+>
+> *Creates a todo list breaking down each feature into specific tasks based on the project architecture*
+>
+> **Assistant:** Let's start with user registration. This will involve creating database models, API endpoints, and frontend forms that integrate with your existing authentication system.
+>
+> **Reasoning:**
+> The assistant used the todo list because:
+> 1.  The user provided multiple complex features to implement in a comma separated list
+> 2.  The todo list helps organize these large features into manageable tasks
+> 3.  This approach allows for tracking progress across the entire implementation
+
+#### Example 4
+
+> **User:** Can you help optimize my React application? It's rendering slowly and has performance issues.
+>
+> **Assistant:** I'll help optimize your React application. First, let me examine your codebase to identify potential performance bottlenecks.
+>
+> *Reviews component structure, render patterns, state management, and data fetching*
+>
+> **Assistant:** After analyzing your codebase, I've identified several performance issues. Let me create a todo list to track our optimization efforts.
+>
+> *Creates todo list with items like: 1) Implementing memoization for expensive calculations in ProductList, 2) Adding virtualization for long lists in Dashboard, 3) Optimizing image loading in Gallery component, 4) Fixing state update loops in ShoppingCart, 5) Reviewing bundle size and implementing code splitting*
+>
+> Let's start by implementing memoization for the expensive calculations in your ProductList component.
+>
+> **Reasoning:**
+> The assistant used the todo list because:
+> 1.  First, the assistant examined the codebase to identify specific performance issues
+> 2.  Based on this analysis, it identified multiple optimization opportunities
+> 3.  Performance optimization is a non-trivial task requiring multiple steps
+> 4.  The todo list helps methodically track improvements across different components
+> 5.  This systematic approach ensures all performance bottlenecks are addressed
+
+
+### Examples of When NOT to Use the Todo List
+
+#### Example 1
+
+> **User:** How do I print 'Hello World' in Python?
+>
+> **Assistant:** In Python, you can print "Hello World" with this simple code:
+>
+> \`\`\`python
+> print("Hello World")
+> \`\`\`
+>
+> This will output the text "Hello World" to the console when executed.
+>
+> **Reasoning:**
+> The assistant did not use the todo list because this is a single, trivial task that can be completed in one step. There's no need to track multiple tasks or steps for such a straightforward request.
+
+#### Example 2
+
+> **User:** What does the \`git status\` command do?
+>
+> **Assistant:** The \`git status\` command shows the current state of your working directory and staging area. It displays which changes have been staged, which haven't, and which files aren't being tracked by Git. This is a fundamental command for understanding the current state of your repository before making commits.
+>
+> **Reasoning:**
+> The assistant did not use the todo list because this is an informational request with no actual coding task to complete. The user is simply asking for an explanation, not for the assistant to perform multiple steps or tasks.
+
+#### Example 3
+
+> **User:** Can you add a comment to the \`calculateTotal\` function to explain what it does?
+>
+> **Assistant:** Sure, let me add a comment to the \`calculateTotal\` function to explain what it does.
+>
+> *Uses the Edit tool to add a comment to the \`calculateTotal\` function*
+>
+> **Reasoning:**
+> The assistant did not use the todo list because this is a single, straightforward task confined to one location in the code. Adding a comment doesn't require tracking multiple steps or systematic organization.
+
+#### Example 4
+
+> **User:** Run \`npm install\` for me and tell me what happens.
+>
+> **Assistant:** I'll run the \`npm install\` command for you.
+>
+> *Executes: \`npm install\`*
+>
+> The command completed successfully. Here's the output:
+> [Output of \`npm install\` command]
+>
+> All dependencies have been installed according to your \`package.json\` file.
+>
+> **Reasoning:**
+> The assistant did not use the todo list because this is a single command execution with immediate results. There are no multiple steps to track or organize, making the todo list unnecessary for this straightforward task.
+
+
+### Task States and Management
+
+1.  **Task States**: Use these states to track progress:
+    *   \`pending\`: Task not yet started
+    *   \`in progress\`: Currently working on (limit to ONE task at a time)
+    *   \`completed\`: Task finished successfully
+
+    **IMPORTANT**: Task descriptions must have two forms:
+    *   \`content\`: The imperative form describing what needs to be done (e.g., "Run tests", "Build the project")
+    *   \`activeForm\`: The present continuous form shown during execution (e.g., "Running tests", "Building the project")
+
+2.  **Task Management**:
+    *   Update task status in real-time as you work
+    *   Mark tasks complete IMMEDIATELY after finishing (don't batch completions)
+    *   Exactly ONE task must be \`in progress\` at any time (not less, not more)
+    *   Complete current tasks before starting new ones
+    *   Remove tasks that are no longer relevant from the list entirely
+
+3.  **Task Completion Requirements**:
+    *   ONLY mark a task as completed when you have FULLY accomplished it
+    *   If you encounter errors, blockers, or cannot finish, keep the task as \`in progress\`
+    *   When blocked, create a new task describing what needs to be resolved
+    *   Never mark a task as completed if:
+        *   Tests are failing
+        *   Implementation is partial
+        *   You encountered unresolved errors
+        *   You couldn't find necessary files or dependencies
+
+4.  **Task Breakdown**:
+    *   Create specific, actionable items
+    *   Break complex tasks into smaller, manageable steps
+    *   Use clear, descriptive task names
+
+When in doubt, use this tool. Being proactive with task management demonstrates attentiveness and ensures you complete all requirements successfully.
 `
 }
diff --git a/src/core/prompts/tools/use-mcp-tool.ts b/src/core/prompts/tools/use-mcp-tool.ts
index ac9ef5b0..aa168ffa 100644
--- a/src/core/prompts/tools/use-mcp-tool.ts
+++ b/src/core/prompts/tools/use-mcp-tool.ts
@@ -4,6 +4,7 @@ export function getUseMcpToolDescription(args: ToolArgs): string | undefined {
 	if (!args.mcpHub) {
 		return undefined
 	}
+
 	return `## use_mcp_tool
 Description: Request to use a tool provided by a connected MCP server. Each MCP server can provide multiple tools with different capabilities. Tools have defined input schemas that specify required and optional parameters.
 Parameters:
diff --git a/src/core/task/Task.ts b/src/core/task/Task.ts
index 9d68640f..f4abc4f3 100644
--- a/src/core/task/Task.ts
+++ b/src/core/task/Task.ts
@@ -47,7 +47,7 @@ import { EXPERIMENT_IDS, experiments } from "../../shared/experiments"
 import { getModelMaxOutputTokens } from "../../shared/api"
 
 // services
-import { UrlContentFetcher } from "../../services/browser/UrlContentFetcher"
+import { UrlContentFetcher } from "../../services/browser/UrlContentFetcher-riddler"
 import { BrowserSession } from "../../services/browser/BrowserSession"
 import { McpHub } from "../../services/mcp/McpHub"
 import { McpServerManager } from "../../services/mcp/McpServerManager"
@@ -78,7 +78,7 @@ import { ClineProvider } from "../webview/ClineProvider"
 import { MultiSearchReplaceDiffStrategy } from "../diff/strategies/multi-search-replace"
 import { MultiFileSearchReplaceDiffStrategy } from "../diff/strategies/multi-file-search-replace"
 import { readApiMessages, saveApiMessages, readTaskMessages, saveTaskMessages, taskMetadata } from "../task-persistence"
-import { getEnvironmentDetails } from "../environment/getEnvironmentDetails"
+import { getEnvironmentDetails, getUserSuggestions } from "../environment/getEnvironmentDetails"
 import {
 	type CheckpointDiffOptions,
 	type CheckpointRestoreOptions,
@@ -90,9 +90,14 @@ import {
 import { processUserContentMentions } from "../mentions/processUserContentMentions"
 import { ApiMessage } from "../task-persistence/apiMessages"
 import { getMessagesSinceLastSummary, summarizeConversation } from "../condense"
+import { summarizeSubTask } from "../summarize-subtask"
 import { maybeRemoveImageBlocks } from "../../api/transform/image-cleaning"
 import { restoreTodoListForTask } from "../tools/updateTodoListTool"
 
+
+import { userExecuteCommand } from "../tools/executeCommandTool"
+
+
 const MAX_EXPONENTIAL_BACKOFF_SECONDS = 600 // 10 minutes
 
 export type TaskEvents = {
@@ -241,6 +246,7 @@ export class Task extends EventEmitter<TaskEvents> {
 	consecutiveMistakeLimit: number
 	consecutiveMistakeCountForApplyDiff: Map<string, number> = new Map()
 	toolUsage: ToolUsage = {}
+	public toolSequence: string[] = [];
 
 	// Checkpoints
 	enableCheckpoints: boolean
@@ -260,6 +266,11 @@ export class Task extends EventEmitter<TaskEvents> {
 	didAlreadyUseTool = false
 	didCompleteReadingStream = false
 
+	// Riddler
+	postprocess = {
+		_execute_command: undefined as string | undefined,
+	}
+
 	constructor({
 		provider,
 		apiConfiguration,
@@ -800,6 +811,83 @@ export class Task extends EventEmitter<TaskEvents> {
 		)
 	}
 
+	public async summarizeSubTaskContext(): Promise<string> {
+		const systemPrompt = await this.getSystemPrompt()
+
+		// Get condensing configuration
+		// Using type assertion to handle the case where Phase 1 hasn't been implemented yet
+		const state = await this.providerRef.deref()?.getState()
+		const customCondensingPrompt = state ? (state as any).customCondensingPrompt : undefined
+		const condensingApiConfigId = state ? (state as any).condensingApiConfigId : undefined
+		const listApiConfigMeta = state ? (state as any).listApiConfigMeta : undefined
+
+		// Determine API handler to use
+		let condensingApiHandler: ApiHandler | undefined
+		if (condensingApiConfigId && listApiConfigMeta && Array.isArray(listApiConfigMeta)) {
+			// Using type assertion for the id property to avoid implicit any
+			const matchingConfig = listApiConfigMeta.find((config: any) => config.id === condensingApiConfigId)
+			if (matchingConfig) {
+				const profile = await this.providerRef.deref()?.providerSettingsManager.getProfile({
+					id: condensingApiConfigId,
+				})
+				// Ensure profile and apiProvider exist before trying to build handler
+				if (profile && profile.apiProvider) {
+					condensingApiHandler = buildApiHandler(profile)
+				}
+			}
+		}
+
+		const { contextTokens: prevContextTokens } = this.getTokenUsage()
+		const {
+			messages,
+			summary,
+			cost,
+			newContextTokens = 0,
+			error,
+			inputTokens = 0,
+			outputTokens = 0,
+			cacheWriteTokens = 0,
+			cacheReadTokens = 0,
+		} = await summarizeSubTask(
+			this,
+			this.apiConversationHistory,
+			this.api, // Main API handler (fallback)
+			systemPrompt, // Default summarization prompt (fallback)
+			this.taskId,
+			prevContextTokens,
+			false, // manual trigger
+			customCondensingPrompt, // User's custom prompt
+			condensingApiHandler, // Specific handler for condensing
+		)
+		
+		const maybe_cost:number = cost > 0 ? cost :
+			calculateApiCostAnthropic(
+				this.api.getModel().info,
+				inputTokens,
+				outputTokens,
+				cacheWriteTokens,
+				cacheReadTokens,
+			)
+		// Record cost using the new cost_tracking message type
+		if (maybe_cost > 0) {
+			const contextCondense: ContextCondense = { summary, cost:maybe_cost, newContextTokens, prevContextTokens }
+			await this.say(
+				"cost_tracking",
+				undefined /* text */,
+				undefined /* images */,
+				false /* partial */,
+				undefined /* checkpoint */,
+				undefined /* progressStatus */,
+				{ isNonInteractive: true } /* options */,
+				contextCondense,
+			)
+		}
+		if (error) {
+			return ""
+		}
+		return summary
+	}
+
 	async say(
 		type: ClineSay,
 		text?: string,
@@ -1415,6 +1503,7 @@ export class Task extends EventEmitter<TaskEvents> {
 			fileContextTracker: this.fileContextTracker,
 			rooIgnoreController: this.rooIgnoreController,
 			showRooIgnoredFiles,
+			globalStoragePath: this.globalStoragePath,
 			includeDiagnosticMessages,
 			maxDiagnosticMessages,
 			maxReadFileLine,
@@ -1424,8 +1513,20 @@ export class Task extends EventEmitter<TaskEvents> {
 
 		// Add environment details as its own text block, separate from tool
 		// results.
-		const finalUserContent = [...parsedUserContent, { type: "text" as const, text: environmentDetails }]
+		let userExecuteCommandResult: string | undefined = undefined
+		if (this.postprocess._execute_command) {
+			const cmd = this.postprocess._execute_command
+			this.postprocess._execute_command = undefined
+			userExecuteCommandResult = await userExecuteCommand(this, cmd)
+		}
 
+		const finalUserContent = [
+			...parsedUserContent, 
+			...(userExecuteCommandResult !== undefined ? [{ type: "text" as const, text: `User called the command in the task above in the local terminal, and the terminal displayed the following results (the results are in the <user_execute_command_result></user_execute_command_result> tag):\n<user_execute_command_result>\n${userExecuteCommandResult}\n</user_execute_command_result>` }] : []),
+			...(this.parentTask && includeFileDetails ? [{ type: "text" as const, text: "- IMPORTANT: **SUBTASK RULES** - When you complete your tasks and finally call the \`attempt_completion\` tool for summarization, you MUST describe in more detail all the tasks you completed and the conclusions you reached." }] : []), 
+			{ type: "text" as const, text: environmentDetails },
+		]
+		
 		await this.addToApiConversationHistory({ role: "user", content: finalUserContent })
 		TelemetryService.instance.captureConversationMessage(this.taskId, "user")
 
@@ -1449,6 +1550,8 @@ export class Task extends EventEmitter<TaskEvents> {
 			let inputTokens = 0
 			let outputTokens = 0
 			let totalCost: number | undefined
+			let tps: number = 0
+			let latency: number = 0
 
 			// We can't use `api_req_finished` anymore since it's a unique case
 			// where it could come after a streaming message (i.e. in the middle
@@ -1465,6 +1568,8 @@ export class Task extends EventEmitter<TaskEvents> {
 					tokensOut: outputTokens,
 					cacheWrites: cacheWriteTokens,
 					cacheReads: cacheReadTokens,
+					tps: tps ?? 0, // tokens per second
+					latency: latency ?? 0, // latency in milliseconds
 					cost:
 						totalCost ??
 						calculateApiCostAnthropic(
@@ -1561,6 +1666,8 @@ export class Task extends EventEmitter<TaskEvents> {
 							outputTokens += chunk.outputTokens
 							cacheWriteTokens += chunk.cacheWriteTokens ?? 0
 							cacheReadTokens += chunk.cacheReadTokens ?? 0
+							latency += chunk.latency ?? 0
+							tps = chunk.tps ?? 0
 							totalCost = chunk.totalCost
 							break
 						case "text": {
@@ -1968,6 +2075,22 @@ export class Task extends EventEmitter<TaskEvents> {
 			({ role, content }) => ({ role, content }),
 		)
 
+		const userSuggestions = await getUserSuggestions(this)
+		
+		// 如果cleanConversationHistory最后一个对话角色是user，且是一个数组，且userSuggestions不是undefined，
+		// 则把userSuggestions加入数组的最后一个，属性是text
+		if (cleanConversationHistory.length > 0) {
+			const lastMessage = cleanConversationHistory[cleanConversationHistory.length - 1]
+			if (lastMessage.role === "user" && Array.isArray(lastMessage.content)) {
+				if (userSuggestions !== undefined) {
+					lastMessage.content.push({
+						type: "text",
+						text: `<user_suggestions>\n${userSuggestions}\nThis is just a gentle reminder - ignore if not applicable.\n</user_suggestions>`
+					})
+				}
+			}
+		}
+
 		// Check if we've reached the maximum number of auto-approved requests
 		const maxRequests = state?.allowedMaxRequests || Infinity
 
@@ -2114,7 +2237,7 @@ export class Task extends EventEmitter<TaskEvents> {
 		if (!this.toolUsage[toolName]) {
 			this.toolUsage[toolName] = { attempts: 0, failures: 0 }
 		}
-
+		this.toolSequence.push(toolName);
 		this.toolUsage[toolName].attempts++
 	}
 
@@ -2122,7 +2245,6 @@ export class Task extends EventEmitter<TaskEvents> {
 		if (!this.toolUsage[toolName]) {
 			this.toolUsage[toolName] = { attempts: 0, failures: 0 }
 		}
-
 		this.toolUsage[toolName].failures++
 
 		if (error) {
diff --git a/src/core/tools/applyDiffTool.ts b/src/core/tools/applyDiffTool.ts
index 903e3c84..b1f96ce0 100644
--- a/src/core/tools/applyDiffTool.ts
+++ b/src/core/tools/applyDiffTool.ts
@@ -212,6 +212,18 @@ export async function applyDiffToolLegacy(
 
 				// Call saveChanges to update the DiffViewProvider properties
 				await cline.diffViewProvider.saveChanges(diagnosticsEnabled, writeDelayMs)
+
+				let newContent: string | undefined = await fs.readFile(absolutePath, "utf-8")
+
+				const agentEdits = formatResponse.createPrettyPatch(absolutePath, originalContent, newContent ?? undefined)
+				const say: ClineSayTool = {
+					tool: (!fileExists) ? "newFileCreated" : "editedExistingFile",
+					path: getReadablePath(cline.cwd, relPath),
+					diff: `# agentEdits\n${agentEdits}\n`,
+				}
+	
+				// Send the user feedback
+				await cline.say("user_feedback_diff", JSON.stringify(say))
 			}
 
 			// Track file edit operation
diff --git a/src/core/tools/attemptCompletionTool.ts b/src/core/tools/attemptCompletionTool.ts
index ef788185..2d972aa0 100644
--- a/src/core/tools/attemptCompletionTool.ts
+++ b/src/core/tools/attemptCompletionTool.ts
@@ -98,9 +98,11 @@ export async function attemptCompletionTool(
 				if (!didApprove) {
 					return
 				}
+				
+				const summary = await cline.summarizeSubTaskContext()
 
 				// tell the provider to remove the current subtask and resume the previous task in the stack
-				await cline.providerRef.deref()?.finishSubTask(result)
+				await cline.providerRef.deref()?.finishSubTask(`## Result \n${result} \n\n## Summary  \n${summary}\n\n`)
 				return
 			}
 
diff --git a/src/core/tools/codebaseSearchTool.ts b/src/core/tools/codebaseSearchTool.ts
index 236b0663..18c7a64b 100644
--- a/src/core/tools/codebaseSearchTool.ts
+++ b/src/core/tools/codebaseSearchTool.ts
@@ -17,7 +17,7 @@ export async function codebaseSearchTool(
 	removeClosingTag: RemoveClosingTag,
 ) {
 	const toolName = "codebase_search"
-	const workspacePath = getWorkspacePath()
+	const workspacePath = (cline.cwd && cline.cwd.trim() !== '') ? cline.cwd : getWorkspacePath()
 
 	if (!workspacePath) {
 		// This case should ideally not happen if Cline is initialized correctly
@@ -29,6 +29,13 @@ export async function codebaseSearchTool(
 	let query: string | undefined = block.params.query
 	let directoryPrefix: string | undefined = block.params.path
 
+
+	if (query === undefined) {
+		await getSummary(cline, block, askApproval, handleError, pushToolResult, removeClosingTag)
+		return
+	}
+
+
 	query = removeClosingTag("query", query)
 
 	if (directoryPrefix) {
@@ -132,7 +139,7 @@ ${jsonResult.results
 		(result) => `File path: ${result.filePath}
 Score: ${result.score}
 Lines: ${result.startLine}-${result.endLine}
-Code Chunk: ${result.codeChunk}
+Code Chunk: \n${result.codeChunk}
 `,
 	)
 	.join("\n")}`
@@ -142,3 +149,129 @@ Code Chunk: ${result.codeChunk}
 		await handleError(toolName, error) // Use the standard error handler
 	}
 }
+
+
+
+async function getSummary(
+	cline: Task,
+	block: ToolUse,
+	askApproval: AskApproval,
+	handleError: HandleError,
+	pushToolResult: PushToolResult,
+	removeClosingTag: RemoveClosingTag
+) {
+	const toolName = "codebase_search"
+	const workspacePath = getWorkspacePath()
+
+	if (!workspacePath) {
+		// This case should ideally not happen if Cline is initialized correctly
+		await handleError(toolName, new Error("Could not determine workspace path."))
+		return
+	}
+
+	// --- Parameter Extraction and Validation ---
+	let directoryPrefix: string | undefined = block.params.path
+	
+	if (directoryPrefix) {
+		directoryPrefix = removeClosingTag("path", directoryPrefix)
+		directoryPrefix = path.normalize(directoryPrefix)
+	} else {
+		directoryPrefix = '.'
+	}
+
+	const sharedMessageProps = {
+		tool: "codebaseSearch",
+		query: "（获取摘要）",
+		path: directoryPrefix,
+		isOutsideWorkspace: false,
+	}
+
+	if (block.partial) {
+		await cline.ask("tool", JSON.stringify(sharedMessageProps), block.partial).catch(() => {})
+		return
+	}
+
+	// if (!query) {
+	// 	cline.consecutiveMistakeCount++
+	// 	pushToolResult(await cline.sayAndCreateMissingParamError(toolName, "query"))
+	// 	return
+	// }
+
+	const didApprove = await askApproval("tool", JSON.stringify(sharedMessageProps))
+	if (!didApprove) {
+		pushToolResult(formatResponse.toolDenied())
+		return
+	}
+
+	cline.consecutiveMistakeCount = 0
+
+	// --- Core Logic ---
+	try {
+		const context = cline.providerRef.deref()?.context
+		if (!context) {
+			throw new Error("Extension context is not available.")
+		}
+
+		const manager = CodeIndexManager.getInstance(context)
+
+		if (!manager) {
+			throw new Error("CodeIndexManager is not available.")
+		}
+
+		if (!manager.isFeatureEnabled) {
+			throw new Error("Code Indexing is disabled in the settings.")
+		}
+		if (!manager.isFeatureConfigured) {
+			throw new Error("Code Indexing is not configured (Missing OpenAI Key or Qdrant URL).")
+		}
+
+		const summaryResults: VectorStoreSearchResult[] = await manager.searchSummary(directoryPrefix ?? "")
+
+		// 3. Format and push results
+		if (!summaryResults || summaryResults.length === 0) {
+			pushToolResult(`No summary found in path: "${directoryPrefix}"`) // Use simple string for no results
+			return
+		}
+
+		const jsonResult = {
+			query: "（获取摘要）",
+			results: [],
+		} as {
+			query: string
+			results: Array<{
+				filePath: string
+				score: number
+				startLine: number
+				endLine: number
+				codeChunk: string
+			}>
+		}
+
+		summaryResults.forEach((result) => {
+			if (!result.payload) return
+			if (!("filePath" in result.payload)) return
+
+			const relativePath = vscode.workspace.asRelativePath(result.payload.filePath, false)
+
+			jsonResult.results.push({
+				filePath: relativePath,
+				score: result.score,
+				startLine: result.payload.startLine,
+				endLine: result.payload.endLine,
+				codeChunk: result.payload.codeChunk.trim(),
+			})
+		})
+
+
+		// Send results to UI
+		const payload = { tool: "codebaseSearch", content: jsonResult }
+		await cline.say("codebase_search_result", JSON.stringify(payload))
+
+		// Push results to AI
+		const output = `# Codebase summary in ${directoryPrefix}:\n\n${jsonResult.results.map(result => `## File: ${result.filePath}\n${result.codeChunk}`).join("\n\n")}`
+
+		pushToolResult(output)
+	} catch (error: any) {
+		await handleError(toolName, error) // Use the standard error handler
+	}
+}
\ No newline at end of file
diff --git a/src/core/tools/executeCommandTool.ts b/src/core/tools/executeCommandTool.ts
index c346526a..95c80e6a 100644
--- a/src/core/tools/executeCommandTool.ts
+++ b/src/core/tools/executeCommandTool.ts
@@ -362,3 +362,124 @@ export async function executeCommand(
 		]
 	}
 }
+
+
+
+
+export async function userExecuteCommand(
+	task: Task,
+	cmd: string,
+): Promise<string> {
+	const handleError = async (action: string, error: Error) => {
+		const errorString = `Error ${action}: ${cmd}`
+
+		await task.say(
+			"error",
+			`Error ${action}:\n${cmd}`,
+		)
+
+		return errorString
+	}
+
+	const askApproval = async (
+		cmd:string
+	) => {
+		const { response } = await task.ask(
+			"command",
+			cmd,
+			false,
+			undefined,
+			true,
+		)
+
+		if (response !== "yesButtonClicked") {
+			throw new Error("loggic error")
+		}
+
+		return true
+	}
+	
+	try {
+		if (!cmd) {
+			return "Error: command is empty!"
+		}
+
+		task.consecutiveMistakeCount = 0
+
+		cmd = unescapeHtmlEntities(cmd) // Unescape HTML entities.
+
+		const didApprove = await askApproval(cmd)
+		delay(100)
+		if (!didApprove) {
+			return "Error: command rejected!"
+		}
+
+		const executionId = task.lastMessageTs?.toString() ?? Date.now().toString()
+		const provider = await task.providerRef.deref()
+		const providerState = await provider?.getState()
+
+		const {
+			terminalOutputLineLimit = 500,
+			terminalOutputCharacterLimit = DEFAULT_TERMINAL_OUTPUT_CHARACTER_LIMIT,
+			terminalShellIntegrationDisabled = false,
+		} = providerState ?? {}
+
+		// Get command execution timeout from VSCode configuration (in seconds)
+		const commandExecutionTimeoutSeconds = vscode.workspace
+			.getConfiguration(Package.name)
+			.get<number>("commandExecutionTimeout", 0)
+
+		// Get command timeout allowlist from VSCode configuration
+		const commandTimeoutAllowlist = vscode.workspace
+			.getConfiguration(Package.name)
+			.get<string[]>("commandTimeoutAllowlist", [])
+
+		// Check if command matches any prefix in the allowlist
+		const isCommandAllowlisted = commandTimeoutAllowlist.some((prefix) => cmd!.startsWith(prefix.trim()))
+
+		// Convert seconds to milliseconds for internal use, but skip timeout if command is allowlisted
+		const commandExecutionTimeout = isCommandAllowlisted ? 0 : commandExecutionTimeoutSeconds * 1000
+
+		const options: ExecuteCommandOptions = {
+			executionId,
+			command:cmd,
+			customCwd:task.cwd,
+			terminalShellIntegrationDisabled,
+			terminalOutputLineLimit,
+			terminalOutputCharacterLimit,
+			commandExecutionTimeout,
+		}
+
+		try {
+			const [rejected, result] = await executeCommand(task, options)
+
+			if (rejected) {
+				return "Error: command rejected!"
+			}
+
+			return result as string
+		} catch (error: unknown) {
+			const status: CommandExecutionStatus = { executionId, status: "fallback" }
+			provider?.postMessageToWebview({ type: "commandExecutionStatus", text: JSON.stringify(status) })
+			await task.say("shell_integration_warning")
+
+			if (error instanceof ShellIntegrationError) {
+				const [rejected, result] = await executeCommand(task, {
+					...options,
+					terminalShellIntegrationDisabled: true,
+				})
+
+				if (rejected) {
+					return "Error: command rejected!"
+				}
+
+				return result as string
+			} else {
+				return(`Command failed to execute in terminal due to a shell integration error.`)
+			}
+		}
+	} catch (error) {
+		return await handleError("executing command", error)
+	}
+}
+
diff --git a/src/core/tools/multiApplyDiffTool.ts b/src/core/tools/multiApplyDiffTool.ts
index db514d2b..c47aafe4 100644
--- a/src/core/tools/multiApplyDiffTool.ts
+++ b/src/core/tools/multiApplyDiffTool.ts
@@ -407,7 +407,7 @@ Original error: ${errorMessage}`
 			const fileExists = opResult.fileExists!
 
 			try {
-				let originalContent: string | null = await fs.readFile(absolutePath, "utf-8")
+				let originalContent: string = await fs.readFile(absolutePath, "utf-8")
 				let successCount = 0
 				let formattedError = ""
 
@@ -426,7 +426,7 @@ Original error: ${errorMessage}`
 				}
 
 				// Release the original content from memory as it's no longer needed
-				originalContent = null
+				// originalContent = null
 
 				if (!diffResult.success) {
 					cline.consecutiveMistakeCount++
@@ -611,6 +611,20 @@ ${errorDetails ? `\nTechnical details:\n${errorDetails}\n` : ""}
 					}
 				}
 
+				
+				let newContent: string | null = await fs.readFile(absolutePath, "utf-8")
+				
+				const agentEdits = formatResponse.createPrettyPatch(absolutePath, originalContent ?? "", newContent ?? undefined)
+				const say: ClineSayTool = {
+					tool: (!fileExists) ? "newFileCreated" : "editedExistingFile",
+					path: getReadablePath(cline.cwd, relPath),
+					diff: `# agentEdits\n${agentEdits}\n`,
+				}
+	
+				// Send the user feedback
+				await cline.say("user_feedback_diff", JSON.stringify(say))
+				
+
 				// Track file edit operation
 				await cline.fileContextTracker.trackFileContext(relPath, "roo_edited" as RecordSource)
 
diff --git a/src/core/tools/newTaskTool.ts b/src/core/tools/newTaskTool.ts
index cc56659d..1cbbe325 100644
--- a/src/core/tools/newTaskTool.ts
+++ b/src/core/tools/newTaskTool.ts
@@ -6,6 +6,9 @@ import { defaultModeSlug, getModeBySlug } from "../../shared/modes"
 import { formatResponse } from "../prompts/responses"
 import { t } from "../../i18n"
 
+import { ApiMessage } from "../task-persistence/apiMessages"
+import { getMessagesSinceLastSummary } from "../condense"
+
 export async function newTaskTool(
 	cline: Task,
 	block: ToolUse,
@@ -87,6 +90,25 @@ export async function newTaskTool(
 				return
 			}
 
+			const parentMessages:ApiMessage[] = [
+				...getMessagesSinceLastSummary(cline.apiConversationHistory), 
+				{
+					role: "user",
+					content: `现在得你，是一个由主要智能体创建的子智能体，用于完成父任务中的一个子任务。` +
+						`在此之前的对话都是主要智能体完成父任务时，所进行的对话上下文记录`,
+					ts: Date.now(),
+				},
+				{
+					role: "assistant",
+					content: `那么作为一个子智能体，我当前的任务是什么呢？`,
+					ts: Date.now(),
+				}
+			]
+			// 将父任务的对话上下文传递给子任务
+			if (cline.apiConversationHistory && cline.apiConversationHistory.length > 0) {
+				await newCline.overwriteApiConversationHistory(parentMessages)
+			}
+
 			// Now switch the newly created task to the desired mode
 			await provider.handleModeSwitch(mode)
 
diff --git a/src/core/tools/readFileTool.ts b/src/core/tools/readFileTool.ts
index 01427f4d..4b583791 100644
--- a/src/core/tools/readFileTool.ts
+++ b/src/core/tools/readFileTool.ts
@@ -578,7 +578,7 @@ export async function readFileTool(
 						if (defResult) {
 							xmlInfo += `<list_code_definition_names>${defResult}</list_code_definition_names>\n`
 						}
-						xmlInfo += `<notice>Showing only ${maxReadFileLine} of ${totalLines} total lines. Use line_range if you need to read more lines</notice>\n`
+						xmlInfo += `<notice>Showing only ${maxReadFileLine} of ${totalLines} total lines. Use line_range if you need to read more lines. Read the specified line number range to learn more details you want to know</notice>\n`
 						updateFileResult(relPath, {
 							xmlContent: `<file><path>${relPath}</path>\n${xmlInfo}</file>`,
 						})
diff --git a/src/core/tools/searchFilesTool.ts b/src/core/tools/searchFilesTool.ts
index b6ee97f8..2504e146 100644
--- a/src/core/tools/searchFilesTool.ts
+++ b/src/core/tools/searchFilesTool.ts
@@ -5,7 +5,7 @@ import { ToolUse, AskApproval, HandleError, PushToolResult, RemoveClosingTag } f
 import { ClineSayTool } from "../../shared/ExtensionMessage"
 import { getReadablePath } from "../../utils/path"
 import { isPathOutsideWorkspace } from "../../utils/pathUtils"
-import { regexSearchFiles } from "../../services/ripgrep"
+import { regexSearchFiles, OutputMode } from "../../services/ripgrep"
 
 export async function searchFilesTool(
 	cline: Task,
@@ -18,6 +18,7 @@ export async function searchFilesTool(
 	const relDirPath: string | undefined = block.params.path
 	const regex: string | undefined = block.params.regex
 	const filePattern: string | undefined = block.params.file_pattern
+	const outputMode: string | undefined = block.params.output_mode
 
 	const absolutePath = relDirPath ? path.resolve(cline.cwd, relDirPath) : cline.cwd
 	const isOutsideWorkspace = isPathOutsideWorkspace(absolutePath)
@@ -52,12 +53,22 @@ export async function searchFilesTool(
 
 			cline.consecutiveMistakeCount = 0
 
+			// Validate and set output mode
+			let validatedOutputMode: OutputMode = "content" // default
+			if (outputMode) {
+				const cleanedMode = removeClosingTag("output_mode", outputMode)
+				if (cleanedMode === "content" || cleanedMode === "files_with_matches") {
+					validatedOutputMode = cleanedMode as OutputMode
+				}
+			}
+
 			const results = await regexSearchFiles(
 				cline.cwd,
 				absolutePath,
 				regex,
 				filePattern,
 				cline.rooIgnoreController,
+				validatedOutputMode,
 			)
 
 			const completeMessage = JSON.stringify({ ...sharedMessageProps, content: results } satisfies ClineSayTool)
diff --git a/src/core/tools/useMcpToolTool.ts b/src/core/tools/useMcpToolTool.ts
index 30dff5ce..1b0440fb 100644
--- a/src/core/tools/useMcpToolTool.ts
+++ b/src/core/tools/useMcpToolTool.ts
@@ -4,6 +4,7 @@ import { formatResponse } from "../prompts/responses"
 import { ClineAskUseMcpServer } from "../../shared/ExtensionMessage"
 import { McpExecutionStatus } from "@roo-code/types"
 import { t } from "../../i18n"
+import { McpToolCallResponse } from "../../shared/mcp"
 
 interface McpToolParams {
 	server_name?: string
@@ -127,7 +128,9 @@ async function executeToolAndProcessResult(
 		toolName,
 	})
 
-	const toolResult = await cline.providerRef.deref()?.getMcpHub()?.callTool(serverName, toolName, parsedArguments)
+	let toolResult: McpToolCallResponse | undefined = undefined
+	// Removed inline tools - now use dedicated tools
+	toolResult = await cline.providerRef.deref()?.getMcpHub()?.callTool(serverName, toolName, parsedArguments)
 
 	let toolResultPretty = "(No response)"
 
diff --git a/src/core/webview/ClineProvider.ts b/src/core/webview/ClineProvider.ts
index e013525e..07852903 100644
--- a/src/core/webview/ClineProvider.ts
+++ b/src/core/webview/ClineProvider.ts
@@ -572,48 +572,48 @@ export class ClineProvider
 	public async initClineWithHistoryItem(historyItem: HistoryItem & { rootTask?: Task; parentTask?: Task }) {
 		await this.removeClineFromStack()
 
-		// If the history item has a saved mode, restore it and its associated API configuration
-		if (historyItem.mode) {
-			// Validate that the mode still exists
-			const customModes = await this.customModesManager.getCustomModes()
-			const modeExists = getModeBySlug(historyItem.mode, customModes) !== undefined
-
-			if (!modeExists) {
-				// Mode no longer exists, fall back to default mode
-				this.log(
-					`Mode '${historyItem.mode}' from history no longer exists. Falling back to default mode '${defaultModeSlug}'.`,
-				)
-				historyItem.mode = defaultModeSlug
-			}
-
-			await this.updateGlobalState("mode", historyItem.mode)
-
-			// Load the saved API config for the restored mode if it exists
-			const savedConfigId = await this.providerSettingsManager.getModeConfigId(historyItem.mode)
-			const listApiConfig = await this.providerSettingsManager.listConfig()
-
-			// Update listApiConfigMeta first to ensure UI has latest data
-			await this.updateGlobalState("listApiConfigMeta", listApiConfig)
-
-			// If this mode has a saved config, use it
-			if (savedConfigId) {
-				const profile = listApiConfig.find(({ id }) => id === savedConfigId)
-
-				if (profile?.name) {
-					try {
-						await this.activateProviderProfile({ name: profile.name })
-					} catch (error) {
-						// Log the error but continue with task restoration
-						this.log(
-							`Failed to restore API configuration for mode '${historyItem.mode}': ${
-								error instanceof Error ? error.message : String(error)
-							}. Continuing with default configuration.`,
-						)
-						// The task will continue with the current/default configuration
-					}
-				}
-			}
-		}
+		// // If the history item has a saved mode, restore it and its associated API configuration
+		// if (historyItem.mode) {
+		// 	// Validate that the mode still exists
+		// 	const customModes = await this.customModesManager.getCustomModes()
+		// 	const modeExists = getModeBySlug(historyItem.mode, customModes) !== undefined
+
+		// 	if (!modeExists) {
+		// 		// Mode no longer exists, fall back to default mode
+		// 		this.log(
+		// 			`Mode '${historyItem.mode}' from history no longer exists. Falling back to default mode '${defaultModeSlug}'.`,
+		// 		)
+		// 		historyItem.mode = defaultModeSlug
+		// 	}
+
+		// 	await this.updateGlobalState("mode", historyItem.mode)
+
+		// 	// Load the saved API config for the restored mode if it exists
+		// 	const savedConfigId = await this.providerSettingsManager.getModeConfigId(historyItem.mode)
+		// 	const listApiConfig = await this.providerSettingsManager.listConfig()
+
+		// 	// Update listApiConfigMeta first to ensure UI has latest data
+		// 	await this.updateGlobalState("listApiConfigMeta", listApiConfig)
+
+		// 	// If this mode has a saved config, use it
+		// 	if (savedConfigId) {
+		// 		const profile = listApiConfig.find(({ id }) => id === savedConfigId)
+
+		// 		if (profile?.name) {
+		// 			try {
+		// 				await this.activateProviderProfile({ name: profile.name })
+		// 			} catch (error) {
+		// 				// Log the error but continue with task restoration
+		// 				this.log(
+		// 					`Failed to restore API configuration for mode '${historyItem.mode}': ${
+		// 						error instanceof Error ? error.message : String(error)
+		// 					}. Continuing with default configuration.`,
+		// 				)
+		// 				// The task will continue with the current/default configuration
+		// 			}
+		// 		}
+		// 	}
+		// }
 
 		const {
 			apiConfiguration,
@@ -1254,7 +1254,7 @@ export class ClineProvider
 		await this.postMessageToWebview({ type: "condenseTaskContextResponse", text: taskId })
 	}
 
-	// this function deletes a task from task hidtory, and deletes it's checkpoints and delete the task folder
+	// this function deletes a task from task history, and deletes it's checkpoints and delete the task folder
 	async deleteTaskWithId(id: string) {
 		try {
 			// get the task directory full path
diff --git a/src/core/webview/webviewMessageHandler.ts b/src/core/webview/webviewMessageHandler.ts
index b1b62229..b641f1bb 100644
--- a/src/core/webview/webviewMessageHandler.ts
+++ b/src/core/webview/webviewMessageHandler.ts
@@ -50,6 +50,8 @@ import { GetModelsOptions } from "../../shared/api"
 import { generateSystemPrompt } from "./generateSystemPrompt"
 import { getCommand } from "../../utils/commands"
 
+import { saveMemory } from "./Memory-rid"
+
 const ALLOWED_VSCODE_SETTINGS = new Set(["terminal.integrated.inheritEnv"])
 
 import { MarketplaceManager, MarketplaceItemType } from "../../services/marketplace"
@@ -1336,6 +1338,23 @@ export const webviewMessageHandler = async (
 			await updateGlobalState("autoApprovalEnabled", message.bool ?? false)
 			await provider.postStateToWebview()
 			break
+		case "useTerminalCommand":
+			const tryExecuteCommand = async (retryCount = 0) => {
+				const currentCline = provider.getCurrentCline()
+				if (currentCline) {
+					currentCline.postprocess._execute_command = message.text
+				} else if (retryCount < 5) { // 最多重试5次
+					setTimeout(() => tryExecuteCommand(retryCount + 1), 100)
+				} else {
+					console.error("Failed to get current Cline instance for terminal command")
+				}
+			}
+			tryExecuteCommand()
+			break
+		case "saveMemory":
+			// 调用保存记忆函数，函数内部会发送相应的消息
+			await saveMemory(provider, message.text??"")
+			break
 		case "enhancePrompt":
 			if (message.text) {
 				try {
@@ -2042,6 +2061,9 @@ export const webviewMessageHandler = async (
 						settings.codebaseIndexMistralApiKey,
 					)
 				}
+				if (settings.codeIndexOpenAiKey !== undefined) {
+					await provider.contextProxy.storeSecret("codeIndexOpenAiKey", settings.codeIndexOpenAiKey)
+				}
 
 				// Send success response first - settings are saved regardless of validation
 				await provider.postMessageToWebview({
@@ -2194,11 +2216,10 @@ export const webviewMessageHandler = async (
 					provider.log("Cannot start indexing: No workspace folder open")
 					return
 				}
+				if (!manager.isInitialized) {
+					await manager.initialize(provider.contextProxy)
+				}
 				if (manager.isFeatureEnabled && manager.isFeatureConfigured) {
-					if (!manager.isInitialized) {
-						await manager.initialize(provider.contextProxy)
-					}
-
 					manager.startIndexing()
 				}
 			} catch (error) {
diff --git a/src/integrations/claude-code/run.ts b/src/integrations/claude-code/run.ts
index 65e32bd9..51618247 100644
--- a/src/integrations/claude-code/run.ts
+++ b/src/integrations/claude-code/run.ts
@@ -97,7 +97,7 @@ const claudeCodeTools = [
 	"Grep",
 	"LS",
 	"exit_plan_mode",
-	"Read",
+	,
 	"Edit",
 	"MultiEdit",
 	"Write",
diff --git a/src/package.json b/src/package.json
index d29a00e8..ed0e830e 100644
--- a/src/package.json
+++ b/src/package.json
@@ -3,7 +3,7 @@
 	"displayName": "%extension.displayName%",
 	"description": "%extension.description%",
 	"publisher": "RooVeterinaryInc",
-	"version": "3.25.4",
+	"version": "3.25.504",
 	"icon": "assets/icons/icon.png",
 	"galleryBanner": {
 		"color": "#617A91",
diff --git a/src/services/code-index/__tests__/manager.spec.ts b/src/services/code-index/__tests__/manager.spec.ts
index 8c64c2fd..929f6f93 100644
--- a/src/services/code-index/__tests__/manager.spec.ts
+++ b/src/services/code-index/__tests__/manager.spec.ts
@@ -1,23 +1,62 @@
 import { CodeIndexManager } from "../manager"
 import { CodeIndexServiceFactory } from "../service-factory"
 import type { MockedClass } from "vitest"
+import * as path from "path"
 
 // Mock vscode module
-vi.mock("vscode", () => ({
-	workspace: {
-		workspaceFolders: [
-			{
-				uri: { fsPath: "/test/workspace" },
-				name: "test",
-				index: 0,
-			},
-		],
+vi.mock("vscode", () => {
+	const testPath = require("path")
+	const testWorkspacePath = testPath.join(testPath.sep, "test", "workspace")
+	return {
+		window: {
+			activeTextEditor: null,
+		},
+		workspace: {
+			workspaceFolders: [
+				{
+					uri: { fsPath: testWorkspacePath },
+					name: "test",
+					index: 0,
+				},
+			],
+			createFileSystemWatcher: vi.fn().mockReturnValue({
+				onDidCreate: vi.fn().mockReturnValue({ dispose: vi.fn() }),
+				onDidChange: vi.fn().mockReturnValue({ dispose: vi.fn() }),
+				onDidDelete: vi.fn().mockReturnValue({ dispose: vi.fn() }),
+				dispose: vi.fn(),
+			}),
+		},
+		RelativePattern: vi.fn().mockImplementation((base, pattern) => ({ base, pattern })),
+	}
+})
+
+// Mock only the essential dependencies
+vi.mock("../../../utils/path", () => {
+	const testPath = require("path")
+	const testWorkspacePath = testPath.join(testPath.sep, "test", "workspace")
+	return {
+		getWorkspacePath: vi.fn(() => testWorkspacePath),
+	}
+})
+
+// Mock fs/promises for RooIgnoreController
+vi.mock("fs/promises", () => ({
+	default: {
+		readFile: vi.fn().mockRejectedValue(new Error("File not found")), // Simulate no .gitignore/.rooignore
 	},
 }))
 
-// Mock only the essential dependencies
-vi.mock("../../../utils/path", () => ({
-	getWorkspacePath: vi.fn(() => "/test/workspace"),
+// Mock file utils for RooIgnoreController
+vi.mock("../../../utils/fs", () => ({
+	fileExistsAtPath: vi.fn().mockResolvedValue(false), // Simulate no .rooignore file
+}))
+
+// Mock ignore module
+vi.mock("ignore", () => ({
+	default: vi.fn().mockReturnValue({
+		add: vi.fn(),
+		ignores: vi.fn().mockReturnValue(false),
+	}),
 }))
 
 vi.mock("../state-manager", () => ({
@@ -45,6 +84,13 @@ describe("CodeIndexManager - handleSettingsChange regression", () => {
 	let mockContext: any
 	let manager: CodeIndexManager
 
+	// Define test paths for use in tests
+	const testWorkspacePath = path.join(path.sep, "test", "workspace")
+	const testExtensionPath = path.join(path.sep, "test", "extension")
+	const testStoragePath = path.join(path.sep, "test", "storage")
+	const testGlobalStoragePath = path.join(path.sep, "test", "global-storage")
+	const testLogPath = path.join(path.sep, "test", "log")
+
 	beforeEach(() => {
 		// Clear all instances before each test
 		CodeIndexManager.disposeAll()
@@ -54,14 +100,14 @@ describe("CodeIndexManager - handleSettingsChange regression", () => {
 			workspaceState: {} as any,
 			globalState: {} as any,
 			extensionUri: {} as any,
-			extensionPath: "/test/extension",
+			extensionPath: testExtensionPath,
 			asAbsolutePath: vi.fn(),
 			storageUri: {} as any,
-			storagePath: "/test/storage",
+			storagePath: testStoragePath,
 			globalStorageUri: {} as any,
-			globalStoragePath: "/test/global-storage",
+			globalStoragePath: testGlobalStoragePath,
 			logUri: {} as any,
-			logPath: "/test/log",
+			logPath: testLogPath,
 			extensionMode: 3, // vscode.ExtensionMode.Test
 			secrets: {} as any,
 			environmentVariableCollection: {} as any,
@@ -115,7 +161,7 @@ describe("CodeIndexManager - handleSettingsChange regression", () => {
 			// Mock service factory to handle _recreateServices call
 			const mockServiceFactoryInstance = {
 				configManager: mockConfigManager,
-				workspacePath: "/test/workspace",
+				workspacePath: testWorkspacePath,
 				cacheManager: mockCacheManager,
 				createEmbedder: vi.fn().mockReturnValue({ embedderInfo: { name: "openai" } }),
 				createVectorStore: vi.fn().mockReturnValue({}),
@@ -189,7 +235,7 @@ describe("CodeIndexManager - handleSettingsChange regression", () => {
 			// Mock service factory to handle _recreateServices call
 			const mockServiceFactoryInstance = {
 				configManager: mockConfigManager,
-				workspacePath: "/test/workspace",
+				workspacePath: testWorkspacePath,
 				cacheManager: mockCacheManager,
 				createEmbedder: vi.fn().mockReturnValue({ embedderInfo: { name: "openai" } }),
 				createVectorStore: vi.fn().mockReturnValue({}),
@@ -367,4 +413,199 @@ describe("CodeIndexManager - handleSettingsChange regression", () => {
 			expect(mockServiceFactoryInstance.validateEmbedder).not.toHaveBeenCalled()
 		})
 	})
+
+	describe("recoverFromError", () => {
+		let mockConfigManager: any
+		let mockCacheManager: any
+		let mockStateManager: any
+
+		beforeEach(() => {
+			// Mock config manager
+			mockConfigManager = {
+				loadConfiguration: vi.fn().mockResolvedValue({ requiresRestart: false }),
+				isFeatureConfigured: true,
+				isFeatureEnabled: true,
+				getConfig: vi.fn().mockReturnValue({
+					isConfigured: true,
+					embedderProvider: "openai",
+					modelId: "text-embedding-3-small",
+					openAiOptions: { openAiNativeApiKey: "test-key" },
+					qdrantUrl: "http://localhost:6333",
+					qdrantApiKey: "test-key",
+					searchMinScore: 0.4,
+				}),
+			}
+			;(manager as any)._configManager = mockConfigManager
+
+			// Mock cache manager
+			mockCacheManager = {
+				initialize: vi.fn(),
+				clearCacheFile: vi.fn(),
+			}
+			;(manager as any)._cacheManager = mockCacheManager
+
+			// Mock state manager
+			mockStateManager = (manager as any)._stateManager
+			mockStateManager.setSystemState = vi.fn()
+			mockStateManager.getCurrentStatus = vi.fn().mockReturnValue({
+				systemStatus: "Error",
+				message: "Failed during initial scan: fetch failed",
+				processedItems: 0,
+				totalItems: 0,
+				currentItemUnit: "items",
+			})
+
+			// Mock orchestrator and search service to simulate initialized state
+			;(manager as any)._orchestrator = { stopWatcher: vi.fn(), state: "Error" }
+			;(manager as any)._searchService = {}
+			;(manager as any)._serviceFactory = {}
+		})
+
+		it("should clear error state when recoverFromError is called", async () => {
+			// Act
+			await manager.recoverFromError()
+
+			// Assert
+			expect(mockStateManager.setSystemState).toHaveBeenCalledWith("Standby", "")
+		})
+
+		it("should reset internal service instances", async () => {
+			// Verify initial state
+			expect((manager as any)._configManager).toBeDefined()
+			expect((manager as any)._serviceFactory).toBeDefined()
+			expect((manager as any)._orchestrator).toBeDefined()
+			expect((manager as any)._searchService).toBeDefined()
+
+			// Act
+			await manager.recoverFromError()
+
+			// Assert - all service instances should be undefined
+			expect((manager as any)._configManager).toBeUndefined()
+			expect((manager as any)._serviceFactory).toBeUndefined()
+			expect((manager as any)._orchestrator).toBeUndefined()
+			expect((manager as any)._searchService).toBeUndefined()
+		})
+
+		it("should make manager report as not initialized after recovery", async () => {
+			// Verify initial state
+			expect(manager.isInitialized).toBe(true)
+
+			// Act
+			await manager.recoverFromError()
+
+			// Assert
+			expect(manager.isInitialized).toBe(false)
+		})
+
+		it("should allow re-initialization after recovery", async () => {
+			// Setup mock for re-initialization
+			const mockServiceFactoryInstance = {
+				createServices: vi.fn().mockReturnValue({
+					embedder: { embedderInfo: { name: "openai" } },
+					vectorStore: {},
+					scanner: {},
+					fileWatcher: {
+						onDidStartBatchProcessing: vi.fn(),
+						onBatchProgressUpdate: vi.fn(),
+						watch: vi.fn(),
+						stopWatcher: vi.fn(),
+						dispose: vi.fn(),
+					},
+				}),
+				validateEmbedder: vi.fn().mockResolvedValue({ valid: true }),
+			}
+			MockedCodeIndexServiceFactory.mockImplementation(() => mockServiceFactoryInstance as any)
+
+			// Act - recover from error
+			await manager.recoverFromError()
+
+			// Verify manager is not initialized
+			expect(manager.isInitialized).toBe(false)
+
+			// Mock context proxy for initialization
+			const mockContextProxy = {
+				getValue: vi.fn(),
+				setValue: vi.fn(),
+				storeSecret: vi.fn(),
+				getSecret: vi.fn(),
+				refreshSecrets: vi.fn().mockResolvedValue(undefined),
+				getGlobalState: vi.fn().mockReturnValue({
+					codebaseIndexEnabled: true,
+					codebaseIndexQdrantUrl: "http://localhost:6333",
+					codebaseIndexEmbedderProvider: "openai",
+					codebaseIndexEmbedderModelId: "text-embedding-3-small",
+					codebaseIndexEmbedderModelDimension: 1536,
+					codebaseIndexSearchMaxResults: 10,
+					codebaseIndexSearchMinScore: 0.4,
+				}),
+			}
+
+			// Re-initialize
+			await manager.initialize(mockContextProxy as any)
+
+			// Assert - manager should be initialized again
+			expect(manager.isInitialized).toBe(true)
+			expect(mockServiceFactoryInstance.createServices).toHaveBeenCalled()
+			expect(mockServiceFactoryInstance.validateEmbedder).toHaveBeenCalled()
+		})
+
+		it("should be safe to call when not in error state (idempotent)", async () => {
+			// Setup manager in non-error state
+			mockStateManager.getCurrentStatus.mockReturnValue({
+				systemStatus: "Standby",
+				message: "",
+				processedItems: 0,
+				totalItems: 0,
+				currentItemUnit: "items",
+			})
+
+			// Verify initial state is not error
+			const initialStatus = manager.getCurrentStatus()
+			expect(initialStatus.systemStatus).not.toBe("Error")
+
+			// Act - call recoverFromError when not in error state
+			await expect(manager.recoverFromError()).resolves.not.toThrow()
+
+			// Assert - should still clear state and service instances
+			expect(mockStateManager.setSystemState).toHaveBeenCalledWith("Standby", "")
+			expect((manager as any)._configManager).toBeUndefined()
+			expect((manager as any)._serviceFactory).toBeUndefined()
+			expect((manager as any)._orchestrator).toBeUndefined()
+			expect((manager as any)._searchService).toBeUndefined()
+		})
+
+		it("should continue recovery even if setSystemState throws", async () => {
+			// Setup state manager to throw on setSystemState
+			mockStateManager.setSystemState.mockImplementation(() => {
+				throw new Error("State update failed")
+			})
+
+			// Setup manager with service instances
+			;(manager as any)._configManager = mockConfigManager
+			;(manager as any)._serviceFactory = {}
+			;(manager as any)._orchestrator = { stopWatcher: vi.fn() }
+			;(manager as any)._searchService = {}
+
+			// Spy on console.error
+			const consoleErrorSpy = vi.spyOn(console, "error").mockImplementation(() => {})
+
+			// Act - should not throw despite setSystemState error
+			await expect(manager.recoverFromError()).resolves.not.toThrow()
+
+			// Assert - error should be logged
+			expect(consoleErrorSpy).toHaveBeenCalledWith(
+				"Failed to clear error state during recovery:",
+				expect.any(Error),
+			)
+
+			// Assert - service instances should still be cleared
+			expect((manager as any)._configManager).toBeUndefined()
+			expect((manager as any)._serviceFactory).toBeUndefined()
+			expect((manager as any)._orchestrator).toBeUndefined()
+			expect((manager as any)._searchService).toBeUndefined()
+
+			// Cleanup
+			consoleErrorSpy.mockRestore()
+		})
+	})
 })
diff --git a/src/services/code-index/interfaces/vector-store.ts b/src/services/code-index/interfaces/vector-store.ts
index dde602fb..80978c35 100644
--- a/src/services/code-index/interfaces/vector-store.ts
+++ b/src/services/code-index/interfaces/vector-store.ts
@@ -33,6 +33,16 @@ export interface IVectorStore {
 		directoryPrefix?: string,
 		minScore?: number,
 		maxResults?: number,
+		query?:string,
+	): Promise<VectorStoreSearchResult[]>
+
+	/**
+	 * Searches for similar vectors
+	 * @param directoryPrefix Optional directory prefix to filter results
+	 * @returns Promise resolving to search results
+	 */
+	summary(
+		directoryPrefix?: string,
 	): Promise<VectorStoreSearchResult[]>
 
 	/**
diff --git a/src/services/code-index/manager.ts b/src/services/code-index/manager.ts
index 18e0752c..d512b3c2 100644
--- a/src/services/code-index/manager.ts
+++ b/src/services/code-index/manager.ts
@@ -9,6 +9,7 @@ import { CodeIndexServiceFactory } from "./service-factory"
 import { CodeIndexSearchService } from "./search-service"
 import { CodeIndexOrchestrator } from "./orchestrator"
 import { CacheManager } from "./cache-manager"
+import { RooIgnoreController } from "../../core/ignore/RooIgnoreController"
 import fs from "fs/promises"
 import ignore from "ignore"
 import path from "path"
@@ -28,17 +29,27 @@ export class CodeIndexManager {
 	private _searchService: CodeIndexSearchService | undefined
 	private _cacheManager: CacheManager | undefined
 
-	public static getInstance(context: vscode.ExtensionContext): CodeIndexManager | undefined {
-		// Use first workspace folder consistently
-		const workspaceFolders = vscode.workspace.workspaceFolders
-		if (!workspaceFolders || workspaceFolders.length === 0) {
-			return undefined
-		}
+	// Flag to prevent race conditions during error recovery
+	private _isRecoveringFromError = false
 
-		// Always use the first workspace folder for consistency across all indexing operations.
-		// This ensures that the same workspace context is used throughout the indexing pipeline,
-		// preventing path resolution errors in multi-workspace scenarios.
-		const workspacePath = workspaceFolders[0].uri.fsPath
+	public static getInstance(context: vscode.ExtensionContext, workspacePath?: string): CodeIndexManager | undefined {
+		// If workspacePath is not provided, try to get it from the active editor or first workspace folder
+		if (!workspacePath) {
+			const activeEditor = vscode.window.activeTextEditor
+			if (activeEditor) {
+				const workspaceFolder = vscode.workspace.getWorkspaceFolder(activeEditor.document.uri)
+				workspacePath = workspaceFolder?.uri.fsPath
+			}
+
+			if (!workspacePath) {
+				const workspaceFolders = vscode.workspace.workspaceFolders
+				if (!workspaceFolders || workspaceFolders.length === 0) {
+					return undefined
+				}
+				// Use the first workspace folder as fallback
+				workspacePath = workspaceFolders[0].uri.fsPath
+			}
+		}
 
 		if (!CodeIndexManager.instances.has(workspacePath)) {
 			CodeIndexManager.instances.set(workspacePath, new CodeIndexManager(workspacePath, context))
@@ -157,12 +168,26 @@ export class CodeIndexManager {
 
 	/**
 	 * Initiates the indexing process (initial scan and starts watcher).
+	 * Automatically recovers from error state if needed before starting.
+	 *
+	 * @important This method should NEVER be awaited as it starts a long-running background process.
+	 * The indexing will continue asynchronously and progress will be reported through events.
 	 */
-
 	public async startIndexing(): Promise<void> {
 		if (!this.isFeatureEnabled) {
 			return
 		}
+
+		// Check if we're in error state and recover if needed
+		const currentStatus = this.getCurrentStatus()
+		if (currentStatus.systemStatus === "Error") {
+			await this.recoverFromError()
+
+			// After recovery, we need to reinitialize since recoverFromError clears all services
+			// This will be handled by the caller (webviewMessageHandler) checking isInitialized
+			return
+		}
+
 		this.assertInitialized()
 		await this._orchestrator!.startIndexing()
 	}
@@ -179,6 +204,46 @@ export class CodeIndexManager {
 		}
 	}
 
+	/**
+	 * Recovers from error state by clearing the error and resetting internal state.
+	 * This allows the manager to be re-initialized after a recoverable error.
+	 *
+	 * This method clears all service instances (configManager, serviceFactory, orchestrator, searchService)
+	 * to force a complete re-initialization on the next operation. This ensures a clean slate
+	 * after recovering from errors such as network failures or configuration issues.
+	 *
+	 * @remarks
+	 * - Safe to call even when not in error state (idempotent)
+	 * - Does not restart indexing automatically - call initialize() after recovery
+	 * - Service instances will be recreated on next initialize() call
+	 * - Prevents race conditions from multiple concurrent recovery attempts
+	 */
+	public async recoverFromError(): Promise<void> {
+		// Prevent race conditions from multiple rapid recovery attempts
+		if (this._isRecoveringFromError) {
+			return
+		}
+
+		this._isRecoveringFromError = true
+		try {
+			// Clear error state
+			this._stateManager.setSystemState("Standby", "")
+		} catch (error) {
+			// Log error but continue with recovery - clearing service instances is more important
+			console.error("Failed to clear error state during recovery:", error)
+		} finally {
+			// Force re-initialization by clearing service instances
+			// This ensures a clean slate even if state update failed
+			this._configManager = undefined
+			this._serviceFactory = undefined
+			this._orchestrator = undefined
+			this._searchService = undefined
+
+			// Reset the flag after recovery is complete
+			this._isRecoveringFromError = false
+		}
+	}
+
 	/**
 	 * Cleans up the manager instance.
 	 */
@@ -205,7 +270,11 @@ export class CodeIndexManager {
 	// --- Private Helpers ---
 
 	public getCurrentStatus() {
-		return this._stateManager.getCurrentStatus()
+		const status = this._stateManager.getCurrentStatus()
+		return {
+			...status,
+			workspacePath: this.workspacePath,
+		}
 	}
 
 	public async searchIndex(query: string, directoryPrefix?: string): Promise<VectorStoreSearchResult[]> {
@@ -216,6 +285,15 @@ export class CodeIndexManager {
 		return this._searchService!.searchIndex(query, directoryPrefix)
 	}
 
+
+	public async searchSummary(directoryPrefix: string): Promise<VectorStoreSearchResult[]> {
+		if (!this.isFeatureEnabled) {
+			return []
+		}
+		this.assertInitialized()
+		return this._searchService!.searchSummary(directoryPrefix)
+	}
+
 	/**
 	 * Private helper method to recreate services with current configuration.
 	 * Used by both initialize() and handleSettingsChange().
@@ -244,6 +322,7 @@ export class CodeIndexManager {
 			return
 		}
 
+		// Create .gitignore instance
 		const ignorePath = path.join(workspacePath, ".gitignore")
 		try {
 			const content = await fs.readFile(ignorePath, "utf8")
@@ -259,11 +338,16 @@ export class CodeIndexManager {
 			})
 		}
 
+		// Create RooIgnoreController instance
+		const rooIgnoreController = new RooIgnoreController(workspacePath)
+		await rooIgnoreController.initialize()
+
 		// (Re)Create shared service instances
 		const { embedder, vectorStore, scanner, fileWatcher } = this._serviceFactory.createServices(
 			this.context,
 			this._cacheManager!,
 			ignoreInstance,
+			rooIgnoreController,
 		)
 
 		// Validate embedder configuration before proceeding
diff --git a/src/services/code-index/processors/file-watcher.ts b/src/services/code-index/processors/file-watcher.ts
index 6dc1cd18..c798ec57 100644
--- a/src/services/code-index/processors/file-watcher.ts
+++ b/src/services/code-index/processors/file-watcher.ts
@@ -7,7 +7,7 @@ import {
 	INITIAL_RETRY_DELAY_MS,
 } from "../constants"
 import { createHash } from "crypto"
-import { RooIgnoreController } from "../../../core/ignore/RooIgnoreController"
+import { RooIgnoreController, CodebaseIgnoreController } from "../../../core/ignore/RooIgnoreController"
 import { v5 as uuidv5 } from "uuid"
 import { Ignore } from "ignore"
 import { scannerExtensions } from "../shared/supported-extensions"
@@ -18,8 +18,9 @@ import {
 	IVectorStore,
 	PointStruct,
 	BatchProcessingSummary,
+	ICodeParser,
 } from "../interfaces"
-import { codeParser } from "./parser"
+// import { codeParser } from "./parser"
 import { CacheManager } from "../cache-manager"
 import { generateNormalizedAbsolutePath, generateRelativeFilePath } from "../shared/get-relative-path"
 import { isPathInIgnoredDirectory } from "../../glob/ignore-utils"
@@ -34,6 +35,7 @@ export class FileWatcher implements IFileWatcher {
 	private ignoreInstance?: Ignore
 	private fileWatcher?: vscode.FileSystemWatcher
 	private ignoreController: RooIgnoreController
+	private cbIgnoreController: CodebaseIgnoreController
 	private accumulatedEvents: Map<string, { uri: vscode.Uri; type: "create" | "change" | "delete" }> = new Map()
 	private batchProcessDebounceTimer?: NodeJS.Timeout
 	private readonly BATCH_DEBOUNCE_DELAY_MS = 500
@@ -67,15 +69,16 @@ export class FileWatcher implements IFileWatcher {
 	 * @param workspacePath Path to the workspace
 	 * @param context VS Code extension context
 	 * @param embedder Optional embedder
-	 * @param vectorStore Optional vector store
+	 * @param qdrantClient Optional vector store
 	 * @param cacheManager Cache manager
 	 */
 	constructor(
 		private workspacePath: string,
 		private context: vscode.ExtensionContext,
 		private readonly cacheManager: CacheManager,
+		private readonly codeParser: ICodeParser,
 		private embedder?: IEmbedder,
-		private vectorStore?: IVectorStore,
+		private qdrantClient?: IVectorStore,
 		ignoreInstance?: Ignore,
 		ignoreController?: RooIgnoreController,
 	) {
@@ -83,6 +86,7 @@ export class FileWatcher implements IFileWatcher {
 		if (ignoreInstance) {
 			this.ignoreInstance = ignoreInstance
 		}
+		this.cbIgnoreController = new CodebaseIgnoreController(workspacePath)
 	}
 
 	/**
@@ -190,9 +194,9 @@ export class FileWatcher implements IFileWatcher {
 			}
 		}
 
-		if (allPathsToClearFromDB.size > 0 && this.vectorStore) {
+		if (allPathsToClearFromDB.size > 0 && this.qdrantClient) {
 			try {
-				await this.vectorStore.deletePointsByMultipleFilePaths(Array.from(allPathsToClearFromDB))
+				await this.qdrantClient.deletePointsByMultipleFilePaths(Array.from(allPathsToClearFromDB))
 
 				for (const path of pathsToExplicitlyDelete) {
 					this.cacheManager.deleteHash(path)
@@ -204,15 +208,20 @@ export class FileWatcher implements IFileWatcher {
 						currentFile: path,
 					})
 				}
-			} catch (error) {
-				overallBatchError = error as Error
+			} catch (error: any) {
+				const errorStatus = error?.status || error?.response?.status || error?.statusCode
+				const errorMessage = error instanceof Error ? error.message : String(error)
+
 				// Log telemetry for deletion error
 				TelemetryService.instance.captureEvent(TelemetryEventName.CODE_INDEX_ERROR, {
-					error: sanitizeErrorMessage(overallBatchError.message),
+					error: sanitizeErrorMessage(errorMessage),
 					location: "deletePointsByMultipleFilePaths",
 					errorType: "deletion_error",
+					errorStatus: errorStatus,
 				})
 
+				// Mark all paths as error
+				overallBatchError = error as Error
 				for (const path of pathsToExplicitlyDelete) {
 					batchResults.push({ path, status: "error", error: error as Error })
 					processedCountInBatch++
@@ -334,7 +343,7 @@ export class FileWatcher implements IFileWatcher {
 		batchResults: FileProcessingResult[],
 		overallBatchError?: Error,
 	): Promise<Error | undefined> {
-		if (pointsForBatchUpsert.length > 0 && this.vectorStore && !overallBatchError) {
+		if (pointsForBatchUpsert.length > 0 && this.qdrantClient && !overallBatchError) {
 			try {
 				for (let i = 0; i < pointsForBatchUpsert.length; i += BATCH_SEGMENT_THRESHOLD) {
 					const batch = pointsForBatchUpsert.slice(i, i + BATCH_SEGMENT_THRESHOLD)
@@ -343,7 +352,7 @@ export class FileWatcher implements IFileWatcher {
 
 					while (retryCount < MAX_BATCH_RETRIES) {
 						try {
-							await this.vectorStore.upsertPoints(batch)
+							await this.qdrantClient.upsertPoints(batch)
 							break
 						} catch (error) {
 							upsertError = error as Error
@@ -497,7 +506,7 @@ export class FileWatcher implements IFileWatcher {
 
 			// Check if file should be ignored
 			const relativeFilePath = generateRelativeFilePath(filePath, this.workspacePath)
-			if (
+			if (!this.cbIgnoreController.validateAccess(filePath) ||
 				!this.ignoreController.validateAccess(filePath) ||
 				(this.ignoreInstance && this.ignoreInstance.ignores(relativeFilePath))
 			) {
@@ -535,7 +544,7 @@ export class FileWatcher implements IFileWatcher {
 			}
 
 			// Parse file
-			const blocks = await codeParser.parseFile(filePath, { content, fileHash: newHash })
+			const blocks = await this.codeParser.parseFile(filePath, { content, fileHash: newHash })
 
 			// Prepare points for batch processing
 			let pointsToUpsert: PointStruct[] = []
diff --git a/src/services/code-index/processors/index.ts b/src/services/code-index/processors/index.ts
index c244d9b8..8b018f6e 100644
--- a/src/services/code-index/processors/index.ts
+++ b/src/services/code-index/processors/index.ts
@@ -1,3 +1,4 @@
 export * from "./parser"
 export * from "./scanner"
 export * from "./file-watcher"
+export * from "./parser-riddler"
diff --git a/src/services/code-index/processors/parser.ts b/src/services/code-index/processors/parser.ts
index 96d747c4..8611884a 100644
--- a/src/services/code-index/processors/parser.ts
+++ b/src/services/code-index/processors/parser.ts
@@ -5,7 +5,7 @@ import { Node } from "web-tree-sitter"
 import { LanguageParser, loadRequiredLanguageParsers } from "../../tree-sitter/languageParser"
 import { parseMarkdown } from "../../tree-sitter/markdownParser"
 import { ICodeParser, CodeBlock } from "../interfaces"
-import { scannerExtensions } from "../shared/supported-extensions"
+import { scannerExtensions, shouldUseFallbackChunking } from "../shared/supported-extensions"
 import { MAX_BLOCK_CHARS, MIN_BLOCK_CHARS, MIN_CHUNK_REMAINDER_CHARS, MAX_CHARS_TOLERANCE_FACTOR } from "../constants"
 import { TelemetryService } from "@roo-code/telemetry"
 import { TelemetryEventName } from "@roo-code/types"
@@ -101,6 +101,11 @@ export class CodeParser implements ICodeParser {
 			return this.parseMarkdownContent(filePath, content, fileHash, seenSegmentHashes)
 		}
 
+		// Check if this extension should use fallback chunking
+		if (shouldUseFallbackChunking(`.${ext}`)) {
+			return this._performFallbackChunking(filePath, content, fileHash, seenSegmentHashes)
+		}
+
 		// Check if we already have the parser loaded
 		if (!this.loadedParsers[ext]) {
 			const pendingLoad = this.pendingLoads.get(ext)
diff --git a/src/services/code-index/processors/scanner.ts b/src/services/code-index/processors/scanner.ts
index 3203076d..fc6f3779 100644
--- a/src/services/code-index/processors/scanner.ts
+++ b/src/services/code-index/processors/scanner.ts
@@ -1,6 +1,6 @@
 import { listFiles } from "../../glob/list-files"
 import { Ignore } from "ignore"
-import { RooIgnoreController } from "../../../core/ignore/RooIgnoreController"
+import { RooIgnoreController, CodebaseIgnoreController } from "../../../core/ignore/RooIgnoreController"
 import { stat } from "fs/promises"
 import * as path from "path"
 import { generateNormalizedAbsolutePath, generateRelativeFilePath } from "../shared/get-relative-path"
@@ -30,6 +30,9 @@ import { TelemetryService } from "@roo-code/telemetry"
 import { TelemetryEventName } from "@roo-code/types"
 import { sanitizeErrorMessage } from "../shared/validation-helpers"
 
+import { RiddlerEmbedder } from "../embedders/embedding-riddler"
+
+
 export class DirectoryScanner implements IDirectoryScanner {
 	constructor(
 		private readonly embedder: IEmbedder,
@@ -58,7 +61,7 @@ export class DirectoryScanner implements IDirectoryScanner {
 		const scanWorkspace = getWorkspacePathForContext(directoryPath)
 
 		// Get all files recursively (handles .gitignore automatically)
-		const [allPaths, _] = await listFiles(directoryPath, true, MAX_LIST_FILES_LIMIT_CODE_INDEX)
+		const [allPaths, _] = await listFiles(directoryPath, true, MAX_LIST_FILES_LIMIT_CODE_INDEX + 1000)
 
 		// Filter out directories (marked with trailing '/')
 		const filePaths = allPaths.filter((p) => !p.endsWith("/"))
@@ -69,10 +72,18 @@ export class DirectoryScanner implements IDirectoryScanner {
 		await ignoreController.initialize()
 
 		// Filter paths using .rooignore
-		const allowedPaths = ignoreController.filterPaths(filePaths)
+		let allowedPaths = ignoreController.filterPaths(filePaths)
+
+		// Initialize RooIgnoreController if not provided
+		const cbignoreController = new CodebaseIgnoreController(directoryPath)
+
+		await cbignoreController.initialize()
+
+		// Filter paths using .rooignore
+		allowedPaths = cbignoreController.filterPaths(allowedPaths)
 
 		// Filter by supported extensions, ignore patterns, and excluded directories
-		const supportedPaths = allowedPaths.filter((filePath) => {
+		let supportedPaths = allowedPaths.filter((filePath) => {
 			const ext = path.extname(filePath).toLowerCase()
 			const relativeFilePath = generateRelativeFilePath(filePath, scanWorkspace)
 
@@ -84,6 +95,14 @@ export class DirectoryScanner implements IDirectoryScanner {
 			return scannerExtensions.includes(ext) && !this.ignoreInstance.ignores(relativeFilePath)
 		})
 
+		const real_limit = this.embedder instanceof RiddlerEmbedder ? 800 : MAX_LIST_FILES_LIMIT_CODE_INDEX
+
+		// Sort files by most recent activity (modified, created, or accessed time)
+		// This ensures recently active files are prioritized for indexing
+		const sortedPaths = await this.sortFilesByRecentActivity(supportedPaths)
+
+		supportedPaths = sortedPaths.slice(0, real_limit)
+
 		// Initialize tracking variables
 		const processedFiles = new Set<string>()
 		let processedCount = 0
@@ -153,7 +172,7 @@ export class DirectoryScanner implements IDirectoryScanner {
 									addedBlocksFromFile = true
 
 									// Check if batch threshold is met
-									if (currentBatchBlocks.length >= BATCH_SEGMENT_THRESHOLD) {
+									if (currentBatchBlocks.length >= (this.embedder instanceof RiddlerEmbedder?1:BATCH_SEGMENT_THRESHOLD)) {
 										// Wait if we've reached the maximum pending batches
 										while (pendingBatchCount >= MAX_PENDING_BATCHES) {
 											// Wait for at least one batch to complete
@@ -281,17 +300,24 @@ export class DirectoryScanner implements IDirectoryScanner {
 					try {
 						await this.qdrantClient.deletePointsByFilePath(cachedFilePath)
 						await this.cacheManager.deleteHash(cachedFilePath)
-					} catch (error) {
+					} catch (error: any) {
+						const errorStatus = error?.status || error?.response?.status || error?.statusCode
+						const errorMessage = error instanceof Error ? error.message : String(error)
+
 						console.error(
 							`[DirectoryScanner] Failed to delete points for ${cachedFilePath} in workspace ${scanWorkspace}:`,
 							error,
 						)
+
 						TelemetryService.instance.captureEvent(TelemetryEventName.CODE_INDEX_ERROR, {
-							error: sanitizeErrorMessage(error instanceof Error ? error.message : String(error)),
+							error: sanitizeErrorMessage(errorMessage),
 							stack: error instanceof Error ? sanitizeErrorMessage(error.stack || "") : undefined,
 							location: "scanDirectory:deleteRemovedFiles",
+							errorStatus: errorStatus,
 						})
+
 						if (onError) {
+							// Report error to error handler
 							onError(
 								error instanceof Error
 									? new Error(
@@ -304,7 +330,8 @@ export class DirectoryScanner implements IDirectoryScanner {
 										),
 							)
 						}
-						// Decide if we should re-throw or just log
+						// Log error and continue processing instead of re-throwing
+						console.error(`Failed to delete points for removed file: ${cachedFilePath}`, error)
 					}
 				}
 			}
@@ -347,25 +374,30 @@ export class DirectoryScanner implements IDirectoryScanner {
 				if (uniqueFilePaths.length > 0) {
 					try {
 						await this.qdrantClient.deletePointsByMultipleFilePaths(uniqueFilePaths)
-					} catch (deleteError) {
+					} catch (deleteError: any) {
+						const errorStatus =
+							deleteError?.status || deleteError?.response?.status || deleteError?.statusCode
+						const errorMessage = deleteError instanceof Error ? deleteError.message : String(deleteError)
+
 						console.error(
 							`[DirectoryScanner] Failed to delete points for ${uniqueFilePaths.length} files before upsert in workspace ${scanWorkspace}:`,
 							deleteError,
 						)
+
 						TelemetryService.instance.captureEvent(TelemetryEventName.CODE_INDEX_ERROR, {
-							error: sanitizeErrorMessage(
-								deleteError instanceof Error ? deleteError.message : String(deleteError),
-							),
+							error: sanitizeErrorMessage(errorMessage),
 							stack:
 								deleteError instanceof Error
 									? sanitizeErrorMessage(deleteError.stack || "")
 									: undefined,
 							location: "processBatch:deletePointsByMultipleFilePaths",
 							fileCount: uniqueFilePaths.length,
+							errorStatus: errorStatus,
 						})
-						// Re-throw the error with workspace context
+
+						// Re-throw with workspace context
 						throw new Error(
-							`Failed to delete points for ${uniqueFilePaths.length} files. Workspace: ${scanWorkspace}. ${deleteError instanceof Error ? deleteError.message : String(deleteError)}`,
+							`Failed to delete points for ${uniqueFilePaths.length} files. Workspace: ${scanWorkspace}. ${errorMessage}`,
 							{ cause: deleteError },
 						)
 					}
@@ -443,4 +475,55 @@ export class DirectoryScanner implements IDirectoryScanner {
 			}
 		}
 	}
+
+	/**
+	 * Sort files by most recent activity (modified, created, or accessed time)
+	 * @param filePaths Array of file paths to sort
+	 * @returns Promise<string[]> Sorted array with most recently active files first
+	 */
+	private async sortFilesByRecentActivity(filePaths: string[]): Promise<string[]> {
+		interface FileWithTime {
+			path: string
+			mostRecentTime: number
+		}
+
+		const filesWithTimes: FileWithTime[] = []
+
+		// Get file stats for all files with concurrency control
+		const statLimiter = pLimit(50) // Limit concurrent stat operations
+		const statPromises = filePaths.map((filePath) =>
+			statLimiter(async () => {
+				try {
+					const stats = await stat(filePath)
+					
+					// Get the most recent time from modification, creation, and access times
+					const mostRecentTime = Math.max(
+						stats.mtimeMs, // Modified time
+						stats.birthtimeMs, // Creation time
+						stats.atimeMs // Access time
+					)
+
+					return {
+						path: filePath,
+						mostRecentTime
+					}
+				} catch (error) {
+					// If we can't stat the file, give it a very old timestamp so it goes to the end
+					console.warn(`Failed to stat file ${filePath}:`, error)
+					return {
+						path: filePath,
+						mostRecentTime: 0
+					}
+				}
+			})
+		)
+
+		const results = await Promise.all(statPromises)
+		filesWithTimes.push(...results)
+
+		// Sort by most recent time (descending - newest first)
+		filesWithTimes.sort((a, b) => b.mostRecentTime - a.mostRecentTime)
+
+		return filesWithTimes.map(file => file.path)
+	}
 }
diff --git a/src/services/code-index/search-service.ts b/src/services/code-index/search-service.ts
index a56f5cc6..98fcb52f 100644
--- a/src/services/code-index/search-service.ts
+++ b/src/services/code-index/search-service.ts
@@ -55,7 +55,7 @@ export class CodeIndexSearchService {
 			}
 
 			// Perform search
-			const results = await this.vectorStore.search(vector, normalizedPrefix, minScore, maxResults)
+			const results = await this.vectorStore.search(vector, normalizedPrefix, minScore, maxResults, query)
 			return results
 		} catch (error) {
 			console.error("[CodeIndexSearchService] Error during search:", error)
@@ -71,4 +71,48 @@ export class CodeIndexSearchService {
 			throw error // Re-throw the error after setting state
 		}
 	}
+
+	/**
+	 * Gets summary information from the code index.
+	 * @param directoryPrefix Optional directory path to filter results by
+	 * @returns Array of summary strings
+	 * @throws Error if the service is not properly configured or ready
+	 */
+	public async searchSummary(directoryPrefix?: string): Promise<VectorStoreSearchResult[]> {
+		if (!this.configManager.isFeatureEnabled || !this.configManager.isFeatureConfigured) {
+			throw new Error("Code index feature is disabled or not configured.")
+		}
+
+		const currentState = this.stateManager.getCurrentStatus().systemStatus
+		if (currentState !== "Indexed" && currentState !== "Indexing") {
+			// Allow summary during Indexing too
+			throw new Error(`Code index is not ready for summary. Current state: ${currentState}`)
+		}
+
+		try {
+			// Handle directory prefix
+			let normalizedPrefix: string | undefined = undefined
+			if (directoryPrefix) {
+				normalizedPrefix = path.normalize(directoryPrefix)
+			}
+
+			// Perform summary request
+			const results = await this.vectorStore.summary(normalizedPrefix)
+			return results
+		} catch (error) {
+			console.error("[CodeIndexSearchService] Error during summary:", error)
+			this.stateManager.setSystemState("Error", `Summary failed: ${(error as Error).message}`)
+
+			// Capture telemetry for the error
+			TelemetryService.instance.captureEvent(TelemetryEventName.CODE_INDEX_ERROR, {
+				error: (error as Error).message,
+				stack: (error as Error).stack,
+				location: "searchSummary",
+			})
+
+			throw error // Re-throw the error after setting state
+		}
+	}
+
+
 }
diff --git a/src/services/code-index/service-factory.ts b/src/services/code-index/service-factory.ts
index 68b0f5c0..a62959e0 100644
--- a/src/services/code-index/service-factory.ts
+++ b/src/services/code-index/service-factory.ts
@@ -1,15 +1,18 @@
 import * as vscode from "vscode"
 import { OpenAiEmbedder } from "./embedders/openai"
+import { RiddlerEmbedder } from "./embedders/embedding-riddler"
 import { CodeIndexOllamaEmbedder } from "./embedders/ollama"
 import { OpenAICompatibleEmbedder } from "./embedders/openai-compatible"
 import { GeminiEmbedder } from "./embedders/gemini"
 import { MistralEmbedder } from "./embedders/mistral"
 import { EmbedderProvider, getDefaultModelId, getModelDimension } from "../../shared/embeddingModels"
 import { QdrantVectorStore } from "./vector-store/qdrant-client"
-import { codeParser, DirectoryScanner, FileWatcher } from "./processors"
+import { RiddlerVectorStore } from "./vector-store/client-riddler"
+import { codeParser, DirectoryScanner, FileWatcher, riddlerCodeParser } from "./processors"
 import { ICodeParser, IEmbedder, IFileWatcher, IVectorStore } from "./interfaces"
 import { CodeIndexConfigManager } from "./config-manager"
 import { CacheManager } from "./cache-manager"
+import { RooIgnoreController } from "../../core/ignore/RooIgnoreController"
 import { Ignore } from "ignore"
 import { t } from "../../i18n"
 import { TelemetryService } from "@roo-code/telemetry"
@@ -45,12 +48,13 @@ export class CodeIndexServiceFactory {
 			})
 		} else if (provider === "ollama") {
 			if (!config.ollamaOptions?.ollamaBaseUrl) {
-				throw new Error(t("embeddings:serviceFactory.ollamaConfigMissing"))
+				throw new Error(`创建 Codebase-Service 缺少URL配置`)
 			}
-			return new CodeIndexOllamaEmbedder({
-				...config.ollamaOptions,
-				ollamaModelId: config.modelId,
-			})
+			return new RiddlerEmbedder()
+			// return new CodeIndexOllamaEmbedder({
+			// 	...config.ollamaOptions,
+			// 	ollamaModelId: config.modelId,
+			// })
 		} else if (provider === "openai-compatible") {
 			if (!config.openAiCompatibleOptions?.baseUrl || !config.openAiCompatibleOptions?.apiKey) {
 				throw new Error(t("embeddings:serviceFactory.openAiCompatibleConfigMissing"))
@@ -112,6 +116,13 @@ export class CodeIndexServiceFactory {
 		// Use the embedding model ID from config, not the chat model IDs
 		const modelId = config.modelId ?? defaultModel
 
+		if (provider === "ollama") {
+			if (!config.ollamaOptions?.ollamaBaseUrl) {
+				throw new Error(`创建 Codebase-Service 缺少URL配置`)
+			}
+			return new RiddlerVectorStore(this.workspacePath, config.ollamaOptions?.ollamaBaseUrl, 1024, config.qdrantApiKey)
+		}
+
 		let vectorSize: number | undefined
 
 		// First try to get the model-specific dimension from profiles
@@ -160,9 +171,20 @@ export class CodeIndexServiceFactory {
 		embedder: IEmbedder,
 		vectorStore: IVectorStore,
 		cacheManager: CacheManager,
+		codeParser: ICodeParser,
 		ignoreInstance: Ignore,
+		rooIgnoreController?: RooIgnoreController,
 	): IFileWatcher {
-		return new FileWatcher(this.workspacePath, context, cacheManager, embedder, vectorStore, ignoreInstance)
+		return new FileWatcher(
+			this.workspacePath,
+			context,
+			cacheManager,
+			codeParser,
+			embedder,
+			vectorStore,
+			ignoreInstance,
+			rooIgnoreController,
+		)
 	}
 
 	/**
@@ -173,6 +195,7 @@ export class CodeIndexServiceFactory {
 		context: vscode.ExtensionContext,
 		cacheManager: CacheManager,
 		ignoreInstance: Ignore,
+		rooIgnoreController?: RooIgnoreController,
 	): {
 		embedder: IEmbedder
 		vectorStore: IVectorStore
@@ -183,12 +206,22 @@ export class CodeIndexServiceFactory {
 		if (!this.configManager.isFeatureConfigured) {
 			throw new Error(t("embeddings:serviceFactory.codeIndexingNotConfigured"))
 		}
+		const config = this.configManager.getConfig()
+		const provider = config.embedderProvider as EmbedderProvider
 
 		const embedder = this.createEmbedder()
 		const vectorStore = this.createVectorStore()
-		const parser = codeParser
+		const parser = provider !== "ollama" ? codeParser : riddlerCodeParser
 		const scanner = this.createDirectoryScanner(embedder, vectorStore, parser, ignoreInstance)
-		const fileWatcher = this.createFileWatcher(context, embedder, vectorStore, cacheManager, ignoreInstance)
+		const fileWatcher = this.createFileWatcher(
+			context,
+			embedder,
+			vectorStore,
+			cacheManager,
+			parser,
+			ignoreInstance,
+			rooIgnoreController,
+		)
 
 		return {
 			embedder,
diff --git a/src/services/code-index/shared/supported-extensions.ts b/src/services/code-index/shared/supported-extensions.ts
index a5205631..80dd7102 100644
--- a/src/services/code-index/shared/supported-extensions.ts
+++ b/src/services/code-index/shared/supported-extensions.ts
@@ -2,3 +2,33 @@ import { extensions as allExtensions } from "../../tree-sitter"
 
 // Include all extensions including markdown for the scanner
 export const scannerExtensions = allExtensions
+
+/**
+ * Extensions that should always use fallback chunking instead of tree-sitter parsing.
+ * These are typically languages that don't have a proper WASM parser available
+ * or where the parser doesn't work correctly.
+ *
+ * NOTE: Only extensions that are already in the supported extensions list can be added here.
+ * To add support for new file types, they must first be added to the tree-sitter extensions list.
+ *
+ * HOW TO ADD A NEW FALLBACK EXTENSION:
+ * 1. First ensure the extension is in src/services/tree-sitter/index.ts extensions array
+ * 2. Add the extension to the fallbackExtensions array below
+ * 3. The file will automatically use length-based chunking for indexing
+ *
+ * Note: Do NOT remove parser cases from languageParser.ts as they may be used elsewhere
+ */
+export const fallbackExtensions = [
+	".vb", // Visual Basic .NET - no dedicated WASM parser
+	".scala", // Scala - uses fallback chunking instead of Lua query workaround
+	".swift", // Swift - uses fallback chunking due to parser instability
+]
+
+/**
+ * Check if a file extension should use fallback chunking
+ * @param extension File extension (including the dot)
+ * @returns true if the extension should use fallback chunking
+ */
+export function shouldUseFallbackChunking(extension: string): boolean {
+	return fallbackExtensions.includes(extension.toLowerCase())
+}
diff --git a/src/services/code-index/vector-store/__tests__/qdrant-client.spec.ts b/src/services/code-index/vector-store/__tests__/qdrant-client.spec.ts
index e539c2ed..822832d1 100644
--- a/src/services/code-index/vector-store/__tests__/qdrant-client.spec.ts
+++ b/src/services/code-index/vector-store/__tests__/qdrant-client.spec.ts
@@ -21,10 +21,14 @@ vitest.mock("../../../../i18n", () => ({
 		return key // Just return the key for other cases
 	},
 }))
-vitest.mock("path", () => ({
-	...vitest.importActual("path"),
-	sep: "/",
-}))
+vitest.mock("path", async () => {
+	const actual = await vitest.importActual("path")
+	return {
+		...actual,
+		sep: "/",
+		posix: actual.posix,
+	}
+})
 
 const mockQdrantClientInstance = {
 	getCollection: vitest.fn(),
@@ -1526,5 +1530,205 @@ describe("QdrantVectorStore", () => {
 			expect(callArgs.limit).toBe(DEFAULT_MAX_SEARCH_RESULTS)
 			expect(callArgs.score_threshold).toBe(DEFAULT_SEARCH_MIN_SCORE)
 		})
+
+		describe("current directory path handling", () => {
+			it("should not apply filter when directoryPrefix is '.'", async () => {
+				const queryVector = [0.1, 0.2, 0.3]
+				const directoryPrefix = "."
+				const mockQdrantResults = {
+					points: [
+						{
+							id: "test-id-1",
+							score: 0.85,
+							payload: {
+								filePath: "src/test.ts",
+								codeChunk: "test code",
+								startLine: 1,
+								endLine: 5,
+								pathSegments: { "0": "src", "1": "test.ts" },
+							},
+						},
+					],
+				}
+
+				mockQdrantClientInstance.query.mockResolvedValue(mockQdrantResults)
+
+				const results = await vectorStore.search(queryVector, directoryPrefix)
+
+				expect(mockQdrantClientInstance.query).toHaveBeenCalledWith(expectedCollectionName, {
+					query: queryVector,
+					filter: undefined, // Should be undefined for current directory
+					score_threshold: DEFAULT_SEARCH_MIN_SCORE,
+					limit: DEFAULT_MAX_SEARCH_RESULTS,
+					params: {
+						hnsw_ef: 128,
+						exact: false,
+					},
+					with_payload: {
+						include: ["filePath", "codeChunk", "startLine", "endLine", "pathSegments"],
+					},
+				})
+
+				expect(results).toEqual(mockQdrantResults.points)
+			})
+
+			it("should not apply filter when directoryPrefix is './'", async () => {
+				const queryVector = [0.1, 0.2, 0.3]
+				const directoryPrefix = "./"
+				const mockQdrantResults = { points: [] }
+
+				mockQdrantClientInstance.query.mockResolvedValue(mockQdrantResults)
+
+				await vectorStore.search(queryVector, directoryPrefix)
+
+				expect(mockQdrantClientInstance.query).toHaveBeenCalledWith(expectedCollectionName, {
+					query: queryVector,
+					filter: undefined, // Should be undefined for current directory
+					score_threshold: DEFAULT_SEARCH_MIN_SCORE,
+					limit: DEFAULT_MAX_SEARCH_RESULTS,
+					params: {
+						hnsw_ef: 128,
+						exact: false,
+					},
+					with_payload: {
+						include: ["filePath", "codeChunk", "startLine", "endLine", "pathSegments"],
+					},
+				})
+			})
+
+			it("should not apply filter when directoryPrefix is empty string", async () => {
+				const queryVector = [0.1, 0.2, 0.3]
+				const directoryPrefix = ""
+				const mockQdrantResults = { points: [] }
+
+				mockQdrantClientInstance.query.mockResolvedValue(mockQdrantResults)
+
+				await vectorStore.search(queryVector, directoryPrefix)
+
+				expect(mockQdrantClientInstance.query).toHaveBeenCalledWith(expectedCollectionName, {
+					query: queryVector,
+					filter: undefined, // Should be undefined for empty string
+					score_threshold: DEFAULT_SEARCH_MIN_SCORE,
+					limit: DEFAULT_MAX_SEARCH_RESULTS,
+					params: {
+						hnsw_ef: 128,
+						exact: false,
+					},
+					with_payload: {
+						include: ["filePath", "codeChunk", "startLine", "endLine", "pathSegments"],
+					},
+				})
+			})
+
+			it("should not apply filter when directoryPrefix is '.\\' (Windows style)", async () => {
+				const queryVector = [0.1, 0.2, 0.3]
+				const directoryPrefix = ".\\"
+				const mockQdrantResults = { points: [] }
+
+				mockQdrantClientInstance.query.mockResolvedValue(mockQdrantResults)
+
+				await vectorStore.search(queryVector, directoryPrefix)
+
+				expect(mockQdrantClientInstance.query).toHaveBeenCalledWith(expectedCollectionName, {
+					query: queryVector,
+					filter: undefined, // Should be undefined for Windows current directory
+					score_threshold: DEFAULT_SEARCH_MIN_SCORE,
+					limit: DEFAULT_MAX_SEARCH_RESULTS,
+					params: {
+						hnsw_ef: 128,
+						exact: false,
+					},
+					with_payload: {
+						include: ["filePath", "codeChunk", "startLine", "endLine", "pathSegments"],
+					},
+				})
+			})
+
+			it("should not apply filter when directoryPrefix has trailing slashes", async () => {
+				const queryVector = [0.1, 0.2, 0.3]
+				const directoryPrefix = ".///"
+				const mockQdrantResults = { points: [] }
+
+				mockQdrantClientInstance.query.mockResolvedValue(mockQdrantResults)
+
+				await vectorStore.search(queryVector, directoryPrefix)
+
+				expect(mockQdrantClientInstance.query).toHaveBeenCalledWith(expectedCollectionName, {
+					query: queryVector,
+					filter: undefined, // Should be undefined after normalizing trailing slashes
+					score_threshold: DEFAULT_SEARCH_MIN_SCORE,
+					limit: DEFAULT_MAX_SEARCH_RESULTS,
+					params: {
+						hnsw_ef: 128,
+						exact: false,
+					},
+					with_payload: {
+						include: ["filePath", "codeChunk", "startLine", "endLine", "pathSegments"],
+					},
+				})
+			})
+
+			it("should still apply filter for relative paths like './src'", async () => {
+				const queryVector = [0.1, 0.2, 0.3]
+				const directoryPrefix = "./src"
+				const mockQdrantResults = { points: [] }
+
+				mockQdrantClientInstance.query.mockResolvedValue(mockQdrantResults)
+
+				await vectorStore.search(queryVector, directoryPrefix)
+
+				expect(mockQdrantClientInstance.query).toHaveBeenCalledWith(expectedCollectionName, {
+					query: queryVector,
+					filter: {
+						must: [
+							{
+								key: "pathSegments.0",
+								match: { value: "src" },
+							},
+						],
+					}, // Should normalize "./src" to "src"
+					score_threshold: DEFAULT_SEARCH_MIN_SCORE,
+					limit: DEFAULT_MAX_SEARCH_RESULTS,
+					params: {
+						hnsw_ef: 128,
+						exact: false,
+					},
+					with_payload: {
+						include: ["filePath", "codeChunk", "startLine", "endLine", "pathSegments"],
+					},
+				})
+			})
+
+			it("should still apply filter for regular directory paths", async () => {
+				const queryVector = [0.1, 0.2, 0.3]
+				const directoryPrefix = "src"
+				const mockQdrantResults = { points: [] }
+
+				mockQdrantClientInstance.query.mockResolvedValue(mockQdrantResults)
+
+				await vectorStore.search(queryVector, directoryPrefix)
+
+				expect(mockQdrantClientInstance.query).toHaveBeenCalledWith(expectedCollectionName, {
+					query: queryVector,
+					filter: {
+						must: [
+							{
+								key: "pathSegments.0",
+								match: { value: "src" },
+							},
+						],
+					}, // Should still create filter for regular paths
+					score_threshold: DEFAULT_SEARCH_MIN_SCORE,
+					limit: DEFAULT_MAX_SEARCH_RESULTS,
+					params: {
+						hnsw_ef: 128,
+						exact: false,
+					},
+					with_payload: {
+						include: ["filePath", "codeChunk", "startLine", "endLine", "pathSegments"],
+					},
+				})
+			})
+		})
 	})
 })
diff --git a/src/services/code-index/vector-store/qdrant-client.ts b/src/services/code-index/vector-store/qdrant-client.ts
index 5121d65b..1a866dbe 100644
--- a/src/services/code-index/vector-store/qdrant-client.ts
+++ b/src/services/code-index/vector-store/qdrant-client.ts
@@ -7,6 +7,8 @@ import { Payload, VectorStoreSearchResult } from "../interfaces"
 import { DEFAULT_MAX_SEARCH_RESULTS, DEFAULT_SEARCH_MIN_SCORE } from "../constants"
 import { t } from "../../../i18n"
 
+import * as vscode from "vscode"
+
 /**
  * Qdrant implementation of the vector store interface
  */
@@ -76,7 +78,7 @@ export class QdrantVectorStore implements IVectorStore {
 		}
 
 		// Generate collection name from workspace path
-		const hash = createHash("sha256").update(workspacePath).digest("hex")
+		const hash = createHash("sha256").update(`${vscode.env.machineId}@${workspacePath}`).digest("hex")
 		this.vectorSize = vectorSize
 		this.collectionName = `ws-${hash.substring(0, 16)}`
 	}
@@ -370,18 +372,32 @@ export class QdrantVectorStore implements IVectorStore {
 		directoryPrefix?: string,
 		minScore?: number,
 		maxResults?: number,
+		query?:string,
 	): Promise<VectorStoreSearchResult[]> {
 		try {
 			let filter = undefined
 
 			if (directoryPrefix) {
-				const segments = directoryPrefix.split(path.sep).filter(Boolean)
-
-				filter = {
-					must: segments.map((segment, index) => ({
-						key: `pathSegments.${index}`,
-						match: { value: segment },
-					})),
+				// Check if the path represents current directory
+				const normalizedPrefix = path.posix.normalize(directoryPrefix.replace(/\\/g, "/"))
+				// Note: path.posix.normalize("") returns ".", and normalize("./") returns "./"
+				if (normalizedPrefix === "." || normalizedPrefix === "./") {
+					// Don't create a filter - search entire workspace
+					filter = undefined
+				} else {
+					// Remove leading "./" from paths like "./src" to normalize them
+					const cleanedPrefix = path.posix.normalize(
+						normalizedPrefix.startsWith("./") ? normalizedPrefix.slice(2) : normalizedPrefix,
+					)
+					const segments = cleanedPrefix.split("/").filter(Boolean)
+					if (segments.length > 0) {
+						filter = {
+							must: segments.map((segment, index) => ({
+								key: `pathSegments.${index}`,
+								match: { value: segment },
+							})),
+						}
+					}
 				}
 			}
 
@@ -423,27 +439,62 @@ export class QdrantVectorStore implements IVectorStore {
 		}
 
 		try {
+			// First check if the collection exists
+			const collectionExists = await this.collectionExists()
+			if (!collectionExists) {
+				console.warn(
+					`[QdrantVectorStore] Skipping deletion - collection "${this.collectionName}" does not exist`,
+				)
+				return
+			}
+
 			const workspaceRoot = getWorkspacePath()
-			const normalizedPaths = filePaths.map((filePath) => {
-				const absolutePath = path.resolve(workspaceRoot, filePath)
-				return path.normalize(absolutePath)
+
+			// Build filters using pathSegments to match the indexed fields
+			const filters = filePaths.map((filePath) => {
+				// IMPORTANT: Use the relative path to match what's stored in upsertPoints
+				// upsertPoints stores the relative filePath, not the absolute path
+				const relativePath = path.isAbsolute(filePath) ? path.relative(workspaceRoot, filePath) : filePath
+
+				// Normalize the relative path
+				const normalizedRelativePath = path.normalize(relativePath)
+
+				// Split the path into segments like we do in upsertPoints
+				const segments = normalizedRelativePath.split(path.sep).filter(Boolean)
+
+				// Create a filter that matches all segments of the path
+				// This ensures we only delete points that match the exact file path
+				const mustConditions = segments.map((segment, index) => ({
+					key: `pathSegments.${index}`,
+					match: { value: segment },
+				}))
+
+				return { must: mustConditions }
 			})
 
-			const filter = {
-				should: normalizedPaths.map((normalizedPath) => ({
-					key: "filePath",
-					match: {
-						value: normalizedPath,
-					},
-				})),
-			}
+			// Use 'should' to match any of the file paths (OR condition)
+			const filter = filters.length === 1 ? filters[0] : { should: filters }
 
 			await this.client.delete(this.collectionName, {
 				filter,
 				wait: true,
 			})
-		} catch (error) {
-			console.error("Failed to delete points by file paths:", error)
+		} catch (error: any) {
+			// Extract more detailed error information
+			const errorMessage = error?.message || String(error)
+			const errorStatus = error?.status || error?.response?.status || error?.statusCode
+			const errorDetails = error?.response?.data || error?.data || ""
+
+			console.error(`[QdrantVectorStore] Failed to delete points by file paths:`, {
+				error: errorMessage,
+				status: errorStatus,
+				details: errorDetails,
+				collection: this.collectionName,
+				fileCount: filePaths.length,
+				// Include first few file paths for debugging (avoid logging too many)
+				samplePaths: filePaths.slice(0, 3),
+			})
+
 			throw error
 		}
 	}
@@ -463,6 +514,12 @@ export class QdrantVectorStore implements IVectorStore {
 		}
 	}
 
+	async summary(
+		directoryPrefix?: string,
+	): Promise<VectorStoreSearchResult[]> {
+		return []
+	}
+
 	/**
 	 * Clears all points from the collection
 	 */
diff --git a/src/services/mcp/McpHub.ts b/src/services/mcp/McpHub.ts
index 10a74712..b4562d4e 100644
--- a/src/services/mcp/McpHub.ts
+++ b/src/services/mcp/McpHub.ts
@@ -46,6 +46,8 @@ const BaseConfigSchema = z.object({
 	alwaysAllow: z.array(z.string()).default([]),
 	watchPaths: z.array(z.string()).optional(), // paths to watch for changes and restart server
 	disabledTools: z.array(z.string()).default([]),
+	enabledModes: z.array(z.string()).default([]),
+	disabledModes: z.array(z.string()).default([]),
 })
 
 // Custom error messages for better user feedback
diff --git a/src/services/ripgrep/index.ts b/src/services/ripgrep/index.ts
index c6942219..ff4fd2ae 100644
--- a/src/services/ripgrep/index.ts
+++ b/src/services/ripgrep/index.ts
@@ -6,6 +6,9 @@ import * as vscode from "vscode"
 
 import { RooIgnoreController } from "../../core/ignore/RooIgnoreController"
 import { fileExistsAtPath } from "../../utils/fs"
+
+// Output mode enum
+export type OutputMode = "content" | "files_with_matches"
 /*
 This file provides functionality to perform regex searches on files using ripgrep.
 Inspired by: https://github.com/DiscreteTom/vscode-ripgrep-utils
@@ -28,6 +31,8 @@ The search results include:
 
 Usage example:
 const results = await regexSearchFiles('/path/to/cwd', '/path/to/search', 'TODO:', '*.ts');
+// Or with files_with_matches mode (uses -l flag, returns plain text file paths):
+const filesOnly = await regexSearchFiles('/path/to/cwd', '/path/to/search', 'TODO:', '*.ts', undefined, 'files_with_matches');
 
 rel/path/to/app.ts
 │----
@@ -96,7 +101,7 @@ export async function getBinPath(vscodeAppRoot: string): Promise<string | undefi
 	)
 }
 
-async function execRipgrep(bin: string, args: string[]): Promise<string> {
+async function execRipgrep(bin: string, args: string[], mode:string): Promise<string> {
 	return new Promise((resolve, reject) => {
 		const rgProcess = childProcess.spawn(bin, args)
 		// cross-platform alternative to head, which is ripgrep author's recommendation for limiting output.
@@ -107,7 +112,7 @@ async function execRipgrep(bin: string, args: string[]): Promise<string> {
 
 		let output = ""
 		let lineCount = 0
-		const maxLines = MAX_RESULTS * 5 // limiting ripgrep output with max lines since there's no other way to limit results. it's okay that we're outputting as json, since we're parsing it line by line and ignore anything that's not part of a match. This assumes each result is at most 5 lines.
+		const maxLines = MAX_RESULTS * (mode === "content" ? 5 : 1) // limiting ripgrep output with max lines since there's no other way to limit results. it's okay that we're outputting as json, since we're parsing it line by line and ignore anything that's not part of a match. This assumes each result is at most 5 lines.
 
 		rl.on("line", (line) => {
 			if (lineCount < maxLines) {
@@ -142,6 +147,7 @@ export async function regexSearchFiles(
 	regex: string,
 	filePattern?: string,
 	rooIgnoreController?: RooIgnoreController,
+	outputMode: OutputMode = "content",
 ): Promise<string> {
 	const vscodeAppRoot = vscode.env.appRoot
 	const rgPath = await getBinPath(vscodeAppRoot)
@@ -150,65 +156,99 @@ export async function regexSearchFiles(
 		throw new Error("Could not find ripgrep binary")
 	}
 
-	const args = ["--json", "-e", regex, "--glob", filePattern || "*", "--context", "1", directoryPath]
+	// Adjust args based on output mode
+	const args = outputMode === "files_with_matches"
+		? ["-l", "-e", regex, "--glob", filePattern || "*", directoryPath]
+		: ["--json", "-e", regex, "--glob", filePattern || "*", "--context", "1", directoryPath]
 
 	let output: string
 	try {
-		output = await execRipgrep(rgPath, args)
+		output = await execRipgrep(rgPath, args, outputMode)
 	} catch (error) {
 		console.error("Error executing ripgrep:", error)
 		return "No results found"
 	}
 
+	try {
+		if (((outputMode === "files_with_matches" && !output) || (outputMode === "content" && output.split("\n").length < 3)) && filePattern?.includes(",")) {
+			let outs:any[] = []
+			const _filePattern = filePattern?.split(",")
+			_filePattern.forEach((pattern) => {
+				const args = outputMode === "files_with_matches"
+					? ["-l", "-e", regex, "--glob", pattern.trim(), directoryPath]
+					: ["--json", "-e", regex, "--glob", pattern.trim(), "--context", "1", directoryPath]
+				outs.push(execRipgrep(rgPath, args, outputMode))
+			})
+			output = (await Promise.all(outs)).join("\n")
+		}
+	} catch (error) {
+		output = ""
+	}
+
 	const results: SearchFileResult[] = []
 	let currentFile: SearchFileResult | null = null
 
-	output.split("\n").forEach((line) => {
-		if (line) {
-			try {
-				const parsed = JSON.parse(line)
-				if (parsed.type === "begin") {
-					currentFile = {
-						file: parsed.data.path.text.toString(),
-						searchResults: [],
-					}
-				} else if (parsed.type === "end") {
-					// Reset the current result when a new file is encountered
-					results.push(currentFile as SearchFileResult)
-					currentFile = null
-				} else if ((parsed.type === "match" || parsed.type === "context") && currentFile) {
-					const line = {
-						line: parsed.data.line_number,
-						text: truncateLine(parsed.data.lines.text),
-						isMatch: parsed.type === "match",
-						...(parsed.type === "match" && { column: parsed.data.absolute_offset }),
-					}
-
-					const lastResult = currentFile.searchResults[currentFile.searchResults.length - 1]
-					if (lastResult?.lines.length > 0) {
-						const lastLine = lastResult.lines[lastResult.lines.length - 1]
+	// Handle files_with_matches mode differently - ripgrep with -l outputs plain text file paths
+	if (outputMode === "files_with_matches") {
+		// Parse plain text output (one file path per line)
+		output.split("\n").forEach((line) => {
+			const trimmedLine = line.trim()
+			if (trimmedLine) {
+				results.push({
+					file: trimmedLine,
+					searchResults: [], // Empty for files_with_matches mode
+				})
+			}
+		})
+	} else {
+		// Original content mode logic - parse JSON output
+		output.split("\n").forEach((line) => {
+			if (line) {
+				try {
+					const parsed = JSON.parse(line)
+					if (parsed.type === "begin") {
+						currentFile = {
+							file: parsed.data.path.text.toString(),
+							searchResults: [],
+						}
+					} else if (parsed.type === "end") {
+						// Reset the current result when a new file is encountered
+						results.push(currentFile as SearchFileResult)
+						currentFile = null
+					} else if ((parsed.type === "match" || parsed.type === "context") && currentFile) {
+						const line = {
+							line: parsed.data.line_number,
+							text: truncateLine(parsed.data.lines.text),
+							isMatch: parsed.type === "match",
+							...(parsed.type === "match" && { column: parsed.data.absolute_offset }),
+						}
 
-						// If this line is contiguous with the last result, add to it
-						if (parsed.data.line_number <= lastLine.line + 1) {
-							lastResult.lines.push(line)
+						const lastResult = currentFile.searchResults[currentFile.searchResults.length - 1]
+						if (lastResult?.lines.length > 0) {
+							const lastLine = lastResult.lines[lastResult.lines.length - 1]
+
+							// If this line is contiguous with the last result, add to it
+							if (parsed.data.line_number <= lastLine.line + 1) {
+								lastResult.lines.push(line)
+							} else {
+								// Otherwise create a new result
+								currentFile.searchResults.push({
+									lines: [line],
+								})
+							}
 						} else {
-							// Otherwise create a new result
+							// First line in file
 							currentFile.searchResults.push({
 								lines: [line],
 							})
 						}
-					} else {
-						// First line in file
-						currentFile.searchResults.push({
-							lines: [line],
-						})
 					}
+				} catch (error) {
+					console.error("Error parsing ripgrep output:", error)
 				}
-			} catch (error) {
-				console.error("Error parsing ripgrep output:", error)
 			}
-		}
-	})
+		})
+	}
 
 	// console.log(results)
 
@@ -217,18 +257,39 @@ export async function regexSearchFiles(
 		? results.filter((result) => rooIgnoreController.validateAccess(result.file))
 		: results
 
-	return formatResults(filteredResults, cwd)
+	return formatResults(filteredResults, cwd, outputMode)
 }
 
-function formatResults(fileResults: SearchFileResult[], cwd: string): string {
+function formatResults(fileResults: SearchFileResult[], cwd: string, outputMode: OutputMode = "content"): string {
+	// Handle files_with_matches mode
+	if (outputMode === "files_with_matches") {
+		let output = ""
+		const totalFiles = fileResults.length
+		
+		if (totalFiles >= MAX_RESULTS) {
+			output += `Showing first ${MAX_RESULTS} of ${MAX_RESULTS}+ files with matches. **You should use a more specific search, or switch to multiple SMALLER DIRECTORY SCOPES to searching separately. Do not give up on the search, as it may lead to the loss of relevant contextual information.**\n\n`
+		} else {
+			output += `Found ${totalFiles === 1 ? "1 file" : `${totalFiles.toLocaleString()} files`} with matches${totalFiles === 0 ? ". You should try switching the search scope or changing the search content." : ''}.\n\n`
+		}
+
+		// List files only
+		fileResults.slice(0, MAX_RESULTS).forEach((file) => {
+			const relativeFilePath = path.relative(cwd, file.file)
+			output += `${relativeFilePath.toPosix()}\n`
+		})
+
+		return output.trim()
+	}
+
+	// Original content mode logic
 	const groupedResults: { [key: string]: SearchResult[] } = {}
 
 	let totalResults = fileResults.reduce((sum, file) => sum + file.searchResults.length, 0)
 	let output = ""
 	if (totalResults >= MAX_RESULTS) {
-		output += `Showing first ${MAX_RESULTS} of ${MAX_RESULTS}+ results. Use a more specific search if necessary.\n\n`
+		output += `Showing first ${MAX_RESULTS} of ${MAX_RESULTS}+ results. **You should use a more specific search, or switch to multiple SMALLER DIRECTORY SCOPES to searching separately. Do not give up on the search, as it may lead to the loss of relevant contextual information.**\n\n`
 	} else {
-		output += `Found ${totalResults === 1 ? "1 result" : `${totalResults.toLocaleString()} results`}.\n\n`
+		output += `Found ${totalResults === 1 ? "1 result" : `${totalResults.toLocaleString()} results${totalResults === 0 ? ". You should try switching the search scope or changing the search content." : ''}`}.\n\n`
 	}
 
 	// Group results by file name
diff --git a/src/services/search/file-search.ts b/src/services/search/file-search.ts
index a25dd406..ba70bda5 100644
--- a/src/services/search/file-search.ts
+++ b/src/services/search/file-search.ts
@@ -155,7 +155,64 @@ export async function searchWorkspaceFiles(
 			}),
 		)
 
-		return verifiedResults
+		// return verifiedResults
+		let inc: typeof allItems = []
+		const _query = query.replace(/\\/g, '/')
+		if (_query.includes("/")) {
+			inc = allItems
+				.filter((item) => {
+					const itemAbsolutePath = path.resolve(workspacePath, item.path).replace(/\\/g, '/')
+					if (!itemAbsolutePath.includes(_query)) {
+						return false
+					}
+					// const afterQuery = itemAbsolutePath.split(abs_queryPath).pop() || ""
+					// const slashCount = (afterQuery.match(/\//g) || []).length
+					// if (slashCount > 1 && !afterQuery.endsWith('/')) {
+					// 	return false
+					// }
+					return true
+				})
+				.map((item) => {
+					const fullPath = path.join(workspacePath, item.path)
+					const isDirectory = fs.lstatSync(fullPath).isDirectory()
+					return {
+						...item,
+						path: item.path.toPosix(),
+						type: isDirectory ? ("folder" as const) : ("file" as const),
+					}
+				})
+		}
+		
+		// const abs_queryPath = path.resolve(workspacePath, query).replace(/\\/g, '/')
+		// let inc: typeof allItems = []
+		// if (fs.existsSync(abs_queryPath)) {
+		// 	inc = allItems
+		// 		.filter((item) => {
+		// 			const itemAbsolutePath = path.resolve(workspacePath, item.path).replace(/\\/g, '/')
+		// 			if (!itemAbsolutePath.includes(abs_queryPath)) {
+		// 				return false
+		// 			}
+		// 			const afterQuery = itemAbsolutePath.split(abs_queryPath).pop() || ""
+		// 			const slashCount = (afterQuery.match(/\//g) || []).length
+		// 			if (slashCount > 1 && !afterQuery.endsWith('/')) {
+		// 				return false
+		// 			}
+		// 			return true
+		// 		})
+		// 		.map((item) => {
+		// 			const fullPath = path.join(workspacePath, item.path)
+		// 			const isDirectory = fs.lstatSync(fullPath).isDirectory()
+		// 			return {
+		// 				...item,
+		// 				path: item.path.toPosix(),
+		// 				type: isDirectory ? ("folder" as const) : ("file" as const),
+		// 			}
+		// 		})
+		// }
+
+		return [...inc,...verifiedResults,].filter((item) => {
+			return item.type !== "folder"
+		})
 	} catch (error) {
 		console.error("Error in searchWorkspaceFiles:", error)
 		return []
diff --git a/src/shared/ExtensionMessage.ts b/src/shared/ExtensionMessage.ts
index 1e562bb9..222a8594 100644
--- a/src/shared/ExtensionMessage.ts
+++ b/src/shared/ExtensionMessage.ts
@@ -69,6 +69,7 @@ export interface ExtensionMessage {
 		| "messageUpdated"
 		| "mcpServers"
 		| "enhancedPrompt"
+		| "savedMemory"
 		| "commitSearchResults"
 		| "listApiConfig"
 		| "routerModels"
@@ -103,6 +104,7 @@ export interface ExtensionMessage {
 		| "setHistoryPreviewCollapsed"
 		| "commandExecutionStatus"
 		| "mcpExecutionStatus"
+		| "toolExecutionStatus"
 		| "vsCodeSetting"
 		| "authenticatedUser"
 		| "condenseTaskContextResponse"
@@ -335,6 +337,8 @@ export interface ClineSayTool {
 		| "listFilesRecursive"
 		| "listCodeDefinitionNames"
 		| "searchFiles"
+		| "webSearch"
+		| "urlFetch"
 		| "switchMode"
 		| "newTask"
 		| "finishTask"
@@ -376,6 +380,18 @@ export interface ClineSayTool {
 		}>
 	}>
 	question?: string
+	// Tool execution properties
+	url?: string
+	toolName?: string
+	toolDisplayName?: string
+	parameters?: Array<{
+		name: string
+		value: string
+		label?: string
+	}>
+	response?: string
+	status?: "executing" | "completed" | "error"
+	error?: string
 }
 
 // Must keep in sync with system prompt.
@@ -415,6 +431,16 @@ export interface ClineAskUseMcpServer {
 	response?: string
 }
 
+export interface ClineAskWebSearch {
+	query: string
+	response?: string
+}
+
+export interface ClineAskUrlFetch {
+	url: string
+	response?: string
+}
+
 export interface ClineApiReqInfo {
 	request?: string
 	tokensIn?: number
@@ -425,6 +451,8 @@ export interface ClineApiReqInfo {
 	cancelReason?: ClineApiReqCancelReason
 	streamingFailedMessage?: string
 	apiProtocol?: "anthropic" | "openai"
+	tps?: number
+	latency?: number // optional latency in milliseconds
 }
 
 export type ClineApiReqCancelReason = "streaming_failed" | "user_cancelled"
diff --git a/src/shared/ProfileValidator.ts b/src/shared/ProfileValidator.ts
index 9cfba84a..58e89b9d 100644
--- a/src/shared/ProfileValidator.ts
+++ b/src/shared/ProfileValidator.ts
@@ -62,6 +62,8 @@ export class ProfileValidator {
 			case "bedrock":
 			case "vertex":
 			case "gemini":
+			case "doubao":	
+			case "modelscope":
 			case "mistral":
 			case "deepseek":
 			case "xai":
diff --git a/src/shared/WebviewMessage.ts b/src/shared/WebviewMessage.ts
index 0b0cc068..9e1973b6 100644
--- a/src/shared/WebviewMessage.ts
+++ b/src/shared/WebviewMessage.ts
@@ -108,6 +108,9 @@ export interface WebviewMessage {
 		| "updateMcpTimeout"
 		| "fuzzyMatchThreshold"
 		| "writeDelayMs"
+		| "useTerminalCommand"
+		| "saveMemory"
+		| "savedMemory"
 		| "diagnosticsEnabled"
 		| "enhancePrompt"
 		| "enhancedPrompt"
diff --git a/src/shared/__tests__/experiments.spec.ts b/src/shared/__tests__/experiments.spec.ts
index 607c1e0b..0af7733f 100644
--- a/src/shared/__tests__/experiments.spec.ts
+++ b/src/shared/__tests__/experiments.spec.ts
@@ -29,6 +29,7 @@ describe("experiments", () => {
 				powerSteering: false,
 				multiFileApplyDiff: false,
 				preventFocusDisruption: false,
+				allowedMultiCall: false,
 			}
 			expect(Experiments.isEnabled(experiments, EXPERIMENT_IDS.POWER_STEERING)).toBe(false)
 		})
@@ -38,6 +39,7 @@ describe("experiments", () => {
 				powerSteering: true,
 				multiFileApplyDiff: false,
 				preventFocusDisruption: false,
+				allowedMultiCall: false,
 			}
 			expect(Experiments.isEnabled(experiments, EXPERIMENT_IDS.POWER_STEERING)).toBe(true)
 		})
@@ -47,6 +49,7 @@ describe("experiments", () => {
 				powerSteering: false,
 				multiFileApplyDiff: false,
 				preventFocusDisruption: false,
+				allowedMultiCall: false,
 			}
 			expect(Experiments.isEnabled(experiments, EXPERIMENT_IDS.POWER_STEERING)).toBe(false)
 		})
diff --git a/src/shared/__tests__/getApiMetrics.spec.ts b/src/shared/__tests__/getApiMetrics.spec.ts
index 02f45c5c..7792606f 100644
--- a/src/shared/__tests__/getApiMetrics.spec.ts
+++ b/src/shared/__tests__/getApiMetrics.spec.ts
@@ -34,6 +34,24 @@ describe("getApiMetrics", () => {
 		ts,
 	})
 
+	// Helper function to create a cost_tracking message
+	const createCostTrackingMessage = (
+		cost: number = 0.005,
+		newContextTokens: number = 100,
+		prevContextTokens: number = 200,
+		ts: number = 3000,
+	): ClineMessage => ({
+		type: "say",
+		say: "cost_tracking",
+		contextCondense: {
+			cost,
+			newContextTokens,
+			prevContextTokens,
+			summary: "Operation cost tracked",
+		},
+		ts,
+	})
+
 	// Helper function to create a non-API message
 	const createOtherMessage = (
 		say: "text" | "error" | "reasoning" | "completion_result" = "text",
@@ -326,4 +344,68 @@ describe("getApiMetrics", () => {
 			console.error = originalConsoleError
 		})
 	})
+
+	describe("cost_tracking message type", () => {
+		it("should include cost from cost_tracking messages", () => {
+			const messages: ClineMessage[] = [
+				createCostTrackingMessage(0.015, 100, 200, 1000),
+			]
+
+			const result = getApiMetrics(messages)
+
+			expect(result.totalCost).toBe(0.015)
+		})
+
+		it("should calculate contextTokens from cost_tracking messages", () => {
+			const messages: ClineMessage[] = [
+				createCostTrackingMessage(0.015, 300, 600, 1000),
+			]
+
+			const result = getApiMetrics(messages)
+
+			expect(result.contextTokens).toBe(300)
+		})
+
+		it("should accumulate costs from multiple cost_tracking messages", () => {
+			const messages: ClineMessage[] = [
+				createCostTrackingMessage(0.010, 100, 200, 1000),
+				createCostTrackingMessage(0.005, 150, 300, 2000),
+			]
+
+			const result = getApiMetrics(messages)
+
+			expect(result.totalCost).toBe(0.015)
+			// Should use the last message for context tokens
+			expect(result.contextTokens).toBe(150)
+		})
+
+		it("should combine cost_tracking with other message types", () => {
+			const messages: ClineMessage[] = [
+				createApiReqStartedMessage('{"tokensIn":100,"tokensOut":200,"cost":0.020}', 1000),
+				createCondenseContextMessage(0.005, 400, 800, 2000),
+				createCostTrackingMessage(0.010, 300, 600, 3000),
+			]
+
+			const result = getApiMetrics(messages)
+
+			expect(result.totalCost).toBe(0.035) // 0.020 + 0.005 + 0.010
+			expect(result.contextTokens).toBe(300) // From the last cost_tracking message
+		})
+
+		it("should handle cost_tracking messages with missing contextCondense", () => {
+			const messages: ClineMessage[] = [
+				{
+					type: "say",
+					say: "cost_tracking",
+					ts: 1000,
+					// Missing contextCondense
+				} as ClineMessage,
+			]
+
+			const result = getApiMetrics(messages)
+
+			expect(result.totalCost).toBe(0)
+			expect(result.contextTokens).toBe(0)
+		})
+	})
 })
diff --git a/src/shared/combineCommandSequences.ts b/src/shared/combineCommandSequences.ts
index 2f655feb..346485eb 100644
--- a/src/shared/combineCommandSequences.ts
+++ b/src/shared/combineCommandSequences.ts
@@ -6,10 +6,12 @@ export const COMMAND_OUTPUT_STRING = "Output:"
 /**
  * Combines sequences of command and command_output messages in an array of ClineMessages.
  * Also combines sequences of use_mcp_server and mcp_server_response messages.
+ * Also handles web_search, url_fetch and other tool execution status messages.
  *
  * This function processes an array of ClineMessages objects, looking for sequences
  * where a 'command' message is followed by one or more 'command_output' messages,
- * or where a 'use_mcp_server' message is followed by one or more 'mcp_server_response' messages.
+ * or where a 'use_mcp_server' message is followed by one or more 'mcp_server_response' messages,
+ * or where tool execution messages (web_search, url_fetch) need to preserve their response data.
  * When such a sequence is found, it combines them into a single message, merging
  * their text contents.
  *
@@ -33,9 +35,9 @@ export function combineCommandSequences(messages: ClineMessage[]): ClineMessage[
 	for (let i = 0; i < messages.length; i++) {
 		const msg = messages[i]
 
-		// Handle MCP server requests
-		if (msg.type === "ask" && msg.ask === "use_mcp_server") {
-			// Look ahead for MCP responses
+		// Handle MCP server requests and other tool requests (web_search, url_fetch)
+		if (msg.type === "ask" && (msg.ask === "use_mcp_server" || msg.ask === "web_search" || msg.ask === "url_fetch")) {
+			// Look ahead for tool responses
 			let responses: string[] = []
 			let j = i + 1
 
@@ -44,8 +46,8 @@ export function combineCommandSequences(messages: ClineMessage[]): ClineMessage[
 					responses.push(messages[j].text || "")
 					processedIndices.add(j)
 					j++
-				} else if (messages[j].type === "ask" && messages[j].ask === "use_mcp_server") {
-					// Stop if we encounter another MCP request
+				} else if (messages[j].type === "ask" && (messages[j].ask === "use_mcp_server" || messages[j].ask === "web_search" || messages[j].ask === "url_fetch")) {
+					// Stop if we encounter another tool request
 					break
 				} else {
 					j++
diff --git a/src/shared/experiments.ts b/src/shared/experiments.ts
index 548b55f6..245e44d9 100644
--- a/src/shared/experiments.ts
+++ b/src/shared/experiments.ts
@@ -4,6 +4,7 @@ export const EXPERIMENT_IDS = {
 	MULTI_FILE_APPLY_DIFF: "multiFileApplyDiff",
 	POWER_STEERING: "powerSteering",
 	PREVENT_FOCUS_DISRUPTION: "preventFocusDisruption",
+	ALLOWED_MULTI_CALL: "allowedMultiCall",
 } as const satisfies Record<string, ExperimentId>
 
 type _AssertExperimentIds = AssertEqual<Equals<ExperimentId, Values<typeof EXPERIMENT_IDS>>>
@@ -18,6 +19,7 @@ export const experimentConfigsMap: Record<ExperimentKey, ExperimentConfig> = {
 	MULTI_FILE_APPLY_DIFF: { enabled: false },
 	POWER_STEERING: { enabled: false },
 	PREVENT_FOCUS_DISRUPTION: { enabled: false },
+	ALLOWED_MULTI_CALL: { enabled: false },
 }
 
 export const experimentDefault = Object.fromEntries(
diff --git a/src/shared/getApiMetrics.ts b/src/shared/getApiMetrics.ts
index dcd9ae9e..1d3b4cab 100644
--- a/src/shared/getApiMetrics.ts
+++ b/src/shared/getApiMetrics.ts
@@ -63,6 +63,10 @@ export function getApiMetrics(messages: ClineMessage[]) {
 			}
 		} else if (message.type === "say" && message.say === "condense_context") {
 			result.totalCost += message.contextCondense?.cost ?? 0
+		} else if (message.type === "say" && message.say === "save_memory") {
+			result.totalCost += message.contextCondense?.cost ?? 0
+		} else if (message.type === "say" && message.say === "cost_tracking") {
+			result.totalCost += message.contextCondense?.cost ?? 0
 		}
 	})
 
@@ -88,6 +92,10 @@ export function getApiMetrics(messages: ClineMessage[]) {
 			}
 		} else if (message.type === "say" && message.say === "condense_context") {
 			result.contextTokens = message.contextCondense?.newContextTokens ?? 0
+		} else if (message.type === "say" && message.say === "save_memory") {
+			continue
+		} else if (message.type === "say" && message.say === "cost_tracking") {
+			continue
 		}
 		if (result.contextTokens) {
 			break
diff --git a/src/shared/support-prompt.ts b/src/shared/support-prompt.ts
index 51f4310f..58364c57 100644
--- a/src/shared/support-prompt.ts
+++ b/src/shared/support-prompt.ts
@@ -49,7 +49,7 @@ const supportPromptConfigs: Record<SupportPromptType, SupportPromptConfig> = {
 	ENHANCE: {
 		template: `Generate an enhanced version of this prompt (reply with only the enhanced prompt - no conversation, explanations, lead-in, bullet points, placeholders, or surrounding quotes):
 
-\${userInput}`,
+\${userInput}\n`,
 	},
 	CONDENSE: {
 		template: `Your task is to create a detailed summary of the conversation so far, paying close attention to the user's explicit requests and your previous actions.
@@ -88,7 +88,7 @@ Example summary structure:
   - [Task 2 details & next steps]
   - [...]
 
-Output only the summary of the conversation so far, without any additional commentary or explanation.`,
+Output only the summary of the conversation so far, without any additional commentary or explanation.\n`,
 	},
 	EXPLAIN: {
 		template: `Explain the following code from file path \${filePath}:\${startLine}-\${endLine}
@@ -101,7 +101,7 @@ Output only the summary of the conversation so far, without any additional comme
 Please provide a clear and concise explanation of what this code does, including:
 1. The purpose and functionality
 2. Key components and their interactions
-3. Important patterns or techniques used`,
+3. Important patterns or techniques used\n`,
 	},
 	FIX: {
 		template: `Fix any issues in the following code from file path \${filePath}:\${startLine}-\${endLine}
@@ -116,7 +116,7 @@ Please:
 1. Address all detected problems listed above (if any)
 2. Identify any other potential bugs or issues
 3. Provide corrected code
-4. Explain what was fixed and why`,
+4. Explain what was fixed and why\n`,
 	},
 	IMPROVE: {
 		template: `Improve the following code from file path \${filePath}:\${startLine}-\${endLine}
@@ -132,20 +132,20 @@ Please suggest improvements for:
 3. Best practices and patterns
 4. Error handling and edge cases
 
-Provide the improved code along with explanations for each enhancement.`,
+Provide the improved code along with explanations for each enhancement.\n`,
 	},
 	ADD_TO_CONTEXT: {
 		template: `\${filePath}:\${startLine}-\${endLine}
 \`\`\`
 \${selectedText}
-\`\`\``,
+\`\`\`\n`,
 	},
 	TERMINAL_ADD_TO_CONTEXT: {
 		template: `\${userInput}
 Terminal output:
 \`\`\`
 \${terminalContent}
-\`\`\``,
+\`\`\`\n`,
 	},
 	TERMINAL_FIX: {
 		template: `\${userInput}
@@ -157,7 +157,7 @@ Fix this terminal command:
 Please:
 1. Identify any issues in the command
 2. Provide the corrected command
-3. Explain what was fixed and why`,
+3. Explain what was fixed and why\n`,
 	},
 	TERMINAL_EXPLAIN: {
 		template: `\${userInput}
@@ -169,10 +169,10 @@ Explain this terminal command:
 Please provide:
 1. What the command does
 2. Explanation of each part/flag
-3. Expected output and behavior`,
+3. Expected output and behavior\n`,
 	},
 	NEW_TASK: {
-		template: `\${userInput}`,
+		template: `\${userInput}\n`,
 	},
 } as const
 
diff --git a/src/shared/tools.ts b/src/shared/tools.ts
index 67972243..4de351fd 100644
--- a/src/shared/tools.ts
+++ b/src/shared/tools.ts
@@ -65,6 +65,7 @@ export const toolParamNames = [
 	"query",
 	"args",
 	"todos",
+	"output_mode",
 ] as const
 
 export type ToolParamName = (typeof toolParamNames)[number]
@@ -110,7 +111,7 @@ export interface CodebaseSearchToolUse extends ToolUse {
 
 export interface SearchFilesToolUse extends ToolUse {
 	name: "search_files"
-	params: Partial<Pick<Record<ToolParamName, string>, "path" | "regex" | "file_pattern">>
+	params: Partial<Pick<Record<ToolParamName, string>, "path" | "regex" | "file_pattern" | "output_mode">>
 }
 
 export interface ListFilesToolUse extends ToolUse {
@@ -164,6 +165,16 @@ export interface SearchAndReplaceToolUse extends ToolUse {
 		Partial<Pick<Record<ToolParamName, string>, "use_regex" | "ignore_case" | "start_line" | "end_line">>
 }
 
+export interface WebSearchToolUse extends ToolUse {
+	name: "web_search"
+	params: Partial<Pick<Record<ToolParamName, string>, "query">>
+}
+
+export interface UrlFetchToolUse extends ToolUse {
+	name: "url_fetch"
+	params: Partial<Pick<Record<ToolParamName, string>, "url">>
+}
+
 // Define tool group configuration
 export type ToolGroupConfig = {
 	tools: readonly string[]
@@ -190,6 +201,8 @@ export const TOOL_DISPLAY_NAMES: Record<ToolName, string> = {
 	search_and_replace: "search and replace",
 	codebase_search: "codebase search",
 	update_todo_list: "update todo list",
+	web_search: "web search",
+	url_fetch: "fetch url content",
 } as const
 
 // Define available tool groups.
@@ -202,6 +215,8 @@ export const TOOL_GROUPS: Record<ToolGroup, ToolGroupConfig> = {
 			"list_files",
 			"list_code_definition_names",
 			"codebase_search",
+			"web_search",
+			"url_fetch",
 		],
 	},
 	edit: {
diff --git a/webview-ui/src/components/chat/ChatRow.tsx b/webview-ui/src/components/chat/ChatRow.tsx
index 4fa921f4..0d3143d1 100644
--- a/webview-ui/src/components/chat/ChatRow.tsx
+++ b/webview-ui/src/components/chat/ChatRow.tsx
@@ -1,6 +1,7 @@
 import React, { memo, useCallback, useEffect, useMemo, useRef, useState } from "react"
 import { appendImages } from "@src/utils/imageUtils"
 import { McpExecution } from "./McpExecution"
+import { ToolExecution } from "./ToolExecution"
 import { useSize } from "react-use"
 import { useTranslation, Trans } from "react-i18next"
 import deepEqual from "fast-deep-equal"
@@ -45,6 +46,7 @@ import { CommandExecution } from "./CommandExecution"
 import { CommandExecutionError } from "./CommandExecutionError"
 import { AutoApprovedRequestLimitWarning } from "./AutoApprovedRequestLimitWarning"
 import { CondenseContextErrorRow, CondensingContextRow, ContextCondenseRow } from "./ContextCondenseRow"
+import { SaveMemoryErrorRow, SavingMemoryRow, SaveMemoryRow } from "./saveMemoryRow-rid"
 import CodebaseSearchResultsDisplay from "./CodebaseSearchResultsDisplay"
 
 interface ChatRowProps {
@@ -123,7 +125,7 @@ export const ChatRowContent = ({
 	const [editMode, setEditMode] = useState<Mode>(mode || "code")
 	const [editImages, setEditImages] = useState<string[]>([])
 	const { copyWithFeedback } = useCopyToClipboard()
-
+	
 	// Handle message events for image selection during edit mode
 	useEffect(() => {
 		const handleMessage = (event: MessageEvent) => {
@@ -177,15 +179,23 @@ export const ChatRowContent = ({
 		vscode.postMessage({ type: "selectImages", context: "edit", messageTs: message.ts })
 	}, [message.ts])
 
-	const [cost, apiReqCancelReason, apiReqStreamingFailedMessage] = useMemo(() => {
+	const [cost, apiReqCancelReason, apiReqStreamingFailedMessage, tps, latency] = useMemo(() => {
 		if (message.text !== null && message.text !== undefined && message.say === "api_req_started") {
 			const info = safeJsonParse<ClineApiReqInfo>(message.text)
-			return [info?.cost, info?.cancelReason, info?.streamingFailedMessage]
+			return [info?.cost, info?.cancelReason, info?.streamingFailedMessage, info?.tps, info?.latency]
 		}
 
-		return [undefined, undefined, undefined]
+		return [undefined, undefined, undefined, undefined, undefined]
 	}, [message.text, message.say])
 
+
+
+	console.log(`lastModifiedMessage.ask = ${lastModifiedMessage?.ask || ''}, 
+		Message.ask = ${message.ask}
+		tps = ${tps}
+		cost = ${cost}`)
+	
+
 	// When resuming task, last wont be api_req_failed but a resume_task
 	// message, so api_req_started will show loading spinner. That's why we just
 	// remove the last api_req_started that failed without streaming anything.
@@ -259,6 +269,20 @@ export const ChatRowContent = ({
 						style={{ color: successColor, marginBottom: "-1.5px" }}></span>,
 					<span style={{ color: successColor, fontWeight: "bold" }}>{t("chat:taskCompleted")}</span>,
 				]
+			case "user_feedback":
+				return [
+					<span
+						className="codicon codicon-account"
+						style={{ color: "var(--vscode-charts-blue)", marginBottom: "-1.5px" }}></span>,
+					<span style={{ color: "var(--vscode-charts-blue)", fontWeight: "bold" }}>{"用户反馈"}</span>,
+				]
+			case "save_memory_tag":
+				return [
+					<span
+						className="codicon codicon-save"
+						style={{ color: "#00a3af44", marginBottom: "-1.5px" }}></span>,
+					<span style={{  color: "#00a3af77", fontWeight: "bold" }}>{"记忆说明"}</span>,
+				]
 			case "api_req_retry_delayed":
 				return []
 			case "api_req_started":
@@ -317,6 +341,32 @@ export const ChatRowContent = ({
 					/>,
 					<span style={{ color: normalColor, fontWeight: "bold" }}>{t("chat:questions.hasQuestion")}</span>,
 				]
+			case "web_search":
+				return [
+					isMcpServerResponding ? (
+						<ProgressIndicator />
+					) : (
+						<span
+							className="codicon codicon-search"
+							style={{ color: normalColor, marginBottom: "-1.5px" }}></span>
+					),
+					<span style={{ color: normalColor, fontWeight: "bold" }}>
+						{isMcpServerResponding ? "网络搜索" : "网络搜索完成"}
+					</span>,
+				]
+			case "url_fetch":
+				return [
+					isMcpServerResponding ? (
+						<ProgressIndicator />
+					) : (
+						<span
+							className="codicon codicon-globe"
+							style={{ color: normalColor, marginBottom: "-1.5px" }}></span>
+					),
+					<span style={{ color: normalColor, fontWeight: "bold" }}>
+						{isMcpServerResponding ? "URL内容分析" : "URL内容分析完成"}
+					</span>,
+				]
 			default:
 				return [null, null]
 		}
@@ -972,6 +1022,8 @@ export const ChatRowContent = ({
 									style={{
 										padding: "12px 16px",
 										backgroundColor: "var(--vscode-editor-background)",
+										maxHeight: "360px",
+										overflowY: "auto",
 									}}>
 									<MarkdownBlock markdown={message.text} />
 								</div>
@@ -1013,6 +1065,14 @@ export const ChatRowContent = ({
 										style={{ opacity: cost !== null && cost !== undefined && cost > 0 ? 1 : 0 }}>
 										${Number(cost || 0)?.toFixed(4)}
 									</VSCodeBadge>
+									<VSCodeBadge
+										style={{ opacity: tps !== null && tps !== undefined && tps > 0 ? 1 : 0 }}>
+										{Number(tps || 0).toFixed(1)} tokens/s
+									</VSCodeBadge>
+									<VSCodeBadge
+										style={{ opacity: latency !== null && latency !== undefined && latency > 0 ? 1 : 0 }}>
+										{Number(latency || 0)} ms
+									</VSCodeBadge>
 								</div>
 								<span className={`codicon codicon-chevron-${isExpanded ? "up" : "down"}`}></span>
 							</div>
@@ -1060,59 +1120,30 @@ export const ChatRowContent = ({
 					)
 				case "user_feedback":
 					return (
-						<div className="bg-vscode-editor-background border rounded-xs p-1 overflow-hidden whitespace-pre-wrap">
-							{isEditing ? (
-								<div className="flex flex-col gap-2 p-2">
-									<ChatTextArea
-										inputValue={editedContent}
-										setInputValue={setEditedContent}
-										sendingDisabled={false}
-										selectApiConfigDisabled={true}
-										placeholderText={t("chat:editMessage.placeholder")}
-										selectedImages={editImages}
-										setSelectedImages={setEditImages}
-										onSend={handleSaveEdit}
-										onSelectImages={handleSelectImages}
-										shouldDisableImages={false}
-										mode={editMode}
-										setMode={setEditMode}
-										modeShortcutText=""
-										isEditMode={true}
-										onCancel={handleCancelEdit}
-									/>
-								</div>
-							) : (
-								<div className="flex justify-between">
-									<div className="flex-grow px-2 py-1 wrap-anywhere">
-										<Mention text={message.text} withShadow />
-									</div>
-									<div className="flex">
-										<Button
-											variant="ghost"
-											size="icon"
-											className="shrink-0 hidden"
-											disabled={isStreaming}
-											onClick={(e) => {
-												e.stopPropagation()
-												handleEditClick()
-											}}>
-											<span className="codicon codicon-edit" />
-										</Button>
-										<Button
-											variant="ghost"
-											size="icon"
-											className="shrink-0"
-											disabled={isStreaming}
-											onClick={(e) => {
-												e.stopPropagation()
-												vscode.postMessage({ type: "deleteMessage", value: message.ts })
-											}}>
-											<span className="codicon codicon-trash" />
-										</Button>
-									</div>
+						// <div className="bg-vscode-editor-background border rounded-xs p-1 overflow-hidden whitespace-pre-wrap">
+						<div>
+							<div style={headerStyle}>
+								{icon}
+								{title}
+							</div>
+							<div className="flex justify-between">
+								<div className="flex-grow px-2 py-1 wrap-anywhere" style={{ color: "var(--vscode-charts-blue)" , paddingTop: 10 }}>
+									{/* <Mention text={message.text} withShadow /> */}
+									<Markdown markdown={message.text} partial={message.partial} />
 								</div>
-							)}
-							{!isEditing && message.images && message.images.length > 0 && (
+								<Button
+									variant="ghost"
+									size="icon"
+									className="shrink-0"
+									disabled={isStreaming}
+									onClick={(e) => {
+										e.stopPropagation()
+										vscode.postMessage({ type: "deleteMessage", value: message.ts })
+									}}>
+									<span className="codicon codicon-trash" />
+								</Button>
+							</div>
+							{message.images && message.images.length > 0 && (
 								<Thumbnails images={message.images} style={{ marginTop: "8px" }} />
 							)}
 						</div>
@@ -1165,6 +1196,25 @@ export const ChatRowContent = ({
 							checkpoint={message.checkpoint}
 						/>
 					)
+				case "save_memory":
+					if (message.partial) {
+						return <SavingMemoryRow />
+					}
+					return message.contextCondense ? <SaveMemoryRow {...message.contextCondense} /> : null
+				case "save_memory_error":
+					return <SaveMemoryErrorRow errorText={message.text} />
+				case "save_memory_tag":
+					return (<div>
+						<div style={headerStyle}>
+							{icon}
+							{title}
+						</div>
+						<div className="flex justify-between">
+							<div className="flex-grow px-2 py-1 wrap-anywhere" style={{ color: "#00a3af77" , paddingTop: 10 }}>
+								<Markdown markdown={message.text} partial={message.partial} />
+							</div>
+						</div>
+					</div>)
 				case "condense_context":
 					if (message.partial) {
 						return <CondensingContextRow />
@@ -1341,6 +1391,84 @@ export const ChatRowContent = ({
 				case "auto_approval_max_req_reached": {
 					return <AutoApprovedRequestLimitWarning message={message} />
 				}
+				case "web_search":
+					// Parse the message text to get the web search request
+					const webSearchJson = safeJsonParse<any>(message.text, {})
+
+					// Extract the response field if it exists
+					const { response: webSearchResponse, ...webSearchRequest } = webSearchJson
+
+					// Create the webSearch object with the response field
+					const webSearch = {
+						...webSearchRequest,
+						response: webSearchResponse,
+					}
+
+					const webSearchParameters = [
+						{
+							name: "query",
+							value: webSearch.query || "",
+							label: "搜索查询",
+						},
+					]
+
+					return (
+						<>
+							<div style={headerStyle}>
+								{icon}
+								{title}
+							</div>
+							<ToolExecution
+								executionId={message.ts.toString()}
+								toolName="web_search"
+								toolDisplayName="搜索列表"
+								parameters={webSearchParameters}
+								response={webSearch.response}
+								isPartial={message.partial}
+								status={webSearch.status}
+								error={webSearch.error}
+							/>
+						</>
+					)
+				case "url_fetch":
+					// Parse the message text to get the url fetch request
+					const urlFetchJson = safeJsonParse<any>(message.text, {})
+
+					// Extract the response field if it exists
+					const { response: urlFetchResponse, ...urlFetchRequest } = urlFetchJson
+
+					// Create the urlFetch object with the response field
+					const urlFetch = {
+						...urlFetchRequest,
+						response: urlFetchResponse,
+					}
+
+					const urlFetchParameters = [
+						{
+							name: "url",
+							value: urlFetch.url || "",
+							label: "URL地址",
+						},
+					]
+
+					return (
+						<>
+							<div style={headerStyle}>
+								{icon}
+								{title}
+							</div>
+							<ToolExecution
+								executionId={message.ts.toString()}
+								toolName="url_fetch"
+								toolDisplayName="内容列表"
+								parameters={urlFetchParameters}
+								response={urlFetch.response}
+								isPartial={message.partial}
+								status={urlFetch.status}
+								error={urlFetch.error}
+							/>
+						</>
+					)
 				default:
 					return null
 			}
diff --git a/webview-ui/src/components/chat/ChatTextArea.tsx b/webview-ui/src/components/chat/ChatTextArea.tsx
index a52902f1..a4c93ed6 100644
--- a/webview-ui/src/components/chat/ChatTextArea.tsx
+++ b/webview-ui/src/components/chat/ChatTextArea.tsx
@@ -51,6 +51,8 @@ interface ChatTextAreaProps {
 	// Edit mode props
 	isEditMode?: boolean
 	onCancel?: () => void
+	isSavingMemory: boolean
+	setIsSavingMemory: (value: boolean) => void
 }
 
 const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
@@ -72,6 +74,8 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 			modeShortcutText,
 			isEditMode = false,
 			onCancel,
+			isSavingMemory,
+			setIsSavingMemory,
 		},
 		ref,
 	) => {
@@ -213,6 +217,9 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 		const contextMenuContainerRef = useRef<HTMLDivElement>(null)
 		const [isEnhancingPrompt, setIsEnhancingPrompt] = useState(false)
 		const [isFocused, setIsFocused] = useState(false)
+		const [isCommandMode, setIsCommandMode] = useState(false)
+		const [isMemoryMode, setIsMemoryMode] = useState(false)
+
 
 		// Use custom hook for prompt history navigation
 		const { handleHistoryNavigation, resetHistoryNavigation, resetOnInputChange } = usePromptHistory({
@@ -235,6 +242,10 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 		}, [selectedType, searchQuery])
 
 		const handleEnhancePrompt = useCallback(() => {
+			if (sendingDisabled) {
+				return
+			}
+
 			const trimmedInput = inputValue.trim()
 
 			if (trimmedInput) {
@@ -243,7 +254,7 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 			} else {
 				setInputValue(t("chat:enhancePromptDescription"))
 			}
-		}, [inputValue, setInputValue, t])
+		}, [inputValue, setInputValue, t, setIsEnhancingPrompt])
 
 		const allModes = useMemo(() => getAllModes(customModes), [customModes])
 
@@ -384,6 +395,36 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 			[setInputValue, cursorPosition],
 		)
 
+		const handleSend = useCallback(() => {
+			if (isCommandMode) {
+				onSend()
+				vscode.postMessage({ type: "useTerminalCommand" as const, text: inputValue })
+			} else if (isMemoryMode) {
+				const trimmedInput = inputValue.trim()
+				if (trimmedInput) {
+					setIsSavingMemory(true)
+					vscode.postMessage({ type: "saveMemory" as const, text: trimmedInput })
+				} else {
+					setIsSavingMemory(true)
+					vscode.postMessage({ type: "saveMemory" as const, text: "" })
+				}
+				setInputValue("")
+			} else {
+				resetHistoryNavigation()
+				onSend()
+			}
+			setIsCommandMode(false)
+			setIsMemoryMode(false)
+		}, [
+			onSend,
+			isCommandMode,
+			setIsCommandMode,
+			isMemoryMode,
+			setIsMemoryMode,
+			inputValue,
+			setInputValue,
+		])
+
 		const handleKeyDown = useCallback(
 			(event: React.KeyboardEvent<HTMLTextAreaElement>) => {
 				if (showContextMenu) {
@@ -455,6 +496,32 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 					}
 				}
 
+				// Handle command mode with "!" key
+				if (event.key === "~"  && !isCommandMode && !isMemoryMode && inputValue === "") {
+					event.preventDefault()
+					setIsCommandMode(true)
+					return
+				}
+				
+				// Handle command mode with "!" key
+				if (event.key === "#"  && !isMemoryMode && !isCommandMode && inputValue === "") {
+					event.preventDefault()
+					setIsMemoryMode(true)
+					return
+				}
+
+				// Handle ESC key in command mode
+				if (event.key === "Escape" && (isCommandMode || isMemoryMode)) {
+					event.preventDefault()
+					if (inputValue === "") {
+						setIsCommandMode(false)
+						setIsMemoryMode(false)
+					} else {
+						setInputValue("")
+					}
+					return
+				}
+
 				const isComposing = event.nativeEvent?.isComposing ?? false
 
 				// Handle prompt history navigation using custom hook
@@ -466,8 +533,7 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 					event.preventDefault()
 
 					// Always call onSend - let ChatView handle queueing when disabled
-					resetHistoryNavigation()
-					onSend()
+					handleSend()
 				}
 
 				if (event.key === "Backspace" && !isComposing) {
@@ -515,7 +581,7 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 				}
 			},
 			[
-				onSend,
+				handleSend,
 				showContextMenu,
 				searchQuery,
 				selectedMenuIndex,
@@ -531,6 +597,10 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 				handleHistoryNavigation,
 				resetHistoryNavigation,
 				commands,
+				isCommandMode,
+				setIsCommandMode,
+				isMemoryMode,
+				setIsMemoryMode
 			],
 		)
 
@@ -1061,10 +1131,10 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 						"cursor-text",
 						isEditMode ? "pt-1.5 pb-10 px-2" : "py-1.5 px-2",
 						isFocused
-							? "border border-vscode-focusBorder outline outline-vscode-focusBorder"
-							: isDraggingOver
-								? "border-2 border-dashed border-vscode-focusBorder"
-								: "border border-transparent",
+								? "border outline"
+								: isDraggingOver
+									? "border border-dashed border-vscode-focusBorder"
+									: "border border-transparent",
 						isDraggingOver
 							? "bg-[color-mix(in_srgb,var(--vscode-input-background)_95%,var(--vscode-focusBorder))]"
 							: "bg-vscode-input-background",
@@ -1082,6 +1152,12 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 						"scrollbar-none",
 						"scrollbar-hide",
 					)}
+					style={{
+						...(isFocused && {
+							borderColor: isCommandMode ? '#ef4444' : isMemoryMode ? '#00a5b1ff':'var(--vscode-focusBorder)',
+							outlineColor: isCommandMode ? '#ef4444' : isMemoryMode ? '#00a5b1ff':'var(--vscode-focusBorder)',
+						}),
+					}}
 					onScroll={() => updateHighlights()}
 				/>
 
@@ -1113,7 +1189,7 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 							<button
 								aria-label={t("chat:sendMessage")}
 								disabled={false}
-								onClick={onSend}
+								onClick={handleSend}
 								className={cn(
 									"relative inline-flex items-center justify-center",
 									"bg-transparent border-none p-1.5",
@@ -1233,7 +1309,7 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 							customModes={customModes}
 							customModePrompts={customModePrompts}
 							onCancel={onCancel}
-							onSend={onSend}
+							onSend={handleSend}
 							onSelectImages={onSelectImages}
 							sendingDisabled={sendingDisabled}
 							shouldDisableImages={shouldDisableImages}
diff --git a/webview-ui/src/components/chat/ChatView.tsx b/webview-ui/src/components/chat/ChatView.tsx
index bc9c36a6..e06df740 100644
--- a/webview-ui/src/components/chat/ChatView.tsx
+++ b/webview-ui/src/components/chat/ChatView.tsx
@@ -178,6 +178,7 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 	const [wasStreaming, setWasStreaming] = useState<boolean>(false)
 	const [showCheckpointWarning, setShowCheckpointWarning] = useState<boolean>(false)
 	const [isCondensing, setIsCondensing] = useState<boolean>(false)
+	const [isSavingMemory, setIsSavingMemory] = useState<boolean>(false)
 	const [showAnnouncementModal, setShowAnnouncementModal] = useState(false)
 	const everVisibleMessagesTsRef = useRef<LRUCache<number, boolean>>(
 		new LRUCache({
@@ -363,6 +364,26 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 							setPrimaryButtonText(t("chat:approve.title"))
 							setSecondaryButtonText(t("chat:reject.title"))
 							break
+						case "web_search":
+							if (!isAutoApproved(lastMessage) && !isPartial) {
+								playSound("notification")
+							}
+							setSendingDisabled(isPartial)
+							setClineAsk("web_search")
+							setEnableButtons(!isPartial)
+							setPrimaryButtonText(t("chat:approve.title"))
+							setSecondaryButtonText(t("chat:reject.title"))
+							break
+						case "url_fetch":
+							if (!isAutoApproved(lastMessage) && !isPartial) {
+								playSound("notification")
+							}
+							setSendingDisabled(isPartial)
+							setClineAsk("url_fetch")
+							setEnableButtons(!isPartial)
+							setPrimaryButtonText(t("chat:approve.title"))
+							setSecondaryButtonText(t("chat:reject.title"))
+							break
 						case "completion_result":
 							// extension waiting for feedback. but we can just present a new task button
 							if (!isPartial) {
@@ -721,6 +742,8 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 				case "tool":
 				case "browser_action_launch":
 				case "use_mcp_server":
+				case "web_search":
+				case "url_fetch":
 				case "resume_task":
 				case "mistake_limit_reached":
 					// Only send text/images if they exist
@@ -778,6 +801,8 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 				case "tool":
 				case "browser_action_launch":
 				case "use_mcp_server":
+				case "web_search":
+				case "url_fetch":
 					// Only send text/images if they exist
 					if (trimmedInput || (images && images.length > 0)) {
 						vscode.postMessage({
@@ -866,6 +891,14 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 						setIsCondensing(false)
 					}
 					break
+				case "savedMemory":
+					if (message.success !== undefined) {
+						setIsSavingMemory(false)
+						if (sendingDisabled) {
+							setSendingDisabled(false)
+						}
+					}
+					break
 			}
 			// textAreaRef.current is not explicitly required here since React
 			// guarantees that ref will be stable across re-renders, and we're
@@ -1100,8 +1133,17 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 				return alwaysAllowMcp && isMcpToolAlwaysAllowed(message)
 			}
 
+			if (message.ask === "web_search") {
+				return alwaysAllowReadOnly
+			}
+
+			if (message.ask === "url_fetch") {
+				return alwaysAllowReadOnly
+			}
+
+			// 将 message.isProtected 字段作为 forceApproval 字段
 			if (message.ask === "command") {
-				return alwaysAllowExecute && isAllowedCommand(message)
+				return alwaysAllowExecute && isAllowedCommand(message) || message.isProtected
 			}
 
 			// For read/write operations, check if it's outside workspace and if
@@ -1303,8 +1345,18 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 			})
 		}
 
+		if (isSavingMemory) {
+			// Show indicator after clicking save memory button
+			result.push({
+				type: "say",
+				say: "save_memory",
+				ts: Date.now(),
+				partial: true,
+			})
+		}
+
 		return result
-	}, [isCondensing, visibleMessages])
+	}, [isCondensing, isSavingMemory, visibleMessages])
 
 	// scrolling
 
@@ -1808,7 +1860,7 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 						{telemetrySetting === "unset" && <TelemetryBanner />}
 
 						<div className="mb-2.5">
-							{cloudIsAuthenticated || taskHistory.length < 4 ? <RooTips /> : <RooCloudCTA />}
+							{cloudIsAuthenticated || tasks.length < 1 ? <RooTips /> : <RooCloudCTA />}
 						</div>
 						{/* Show the task history preview if expanded and tasks exist */}
 						{taskHistory.length > 0 && isExpanded && <HistoryPreview />}
@@ -1973,6 +2025,8 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 				mode={mode}
 				setMode={setMode}
 				modeShortcutText={modeShortcutText}
+				isSavingMemory={isSavingMemory}
+				setIsSavingMemory={setIsSavingMemory}
 			/>
 
 			{isProfileDisabled && (
diff --git a/webview-ui/src/components/chat/CodeIndexPopover.tsx b/webview-ui/src/components/chat/CodeIndexPopover.tsx
index c85aaf6e..1de7d155 100644
--- a/webview-ui/src/components/chat/CodeIndexPopover.tsx
+++ b/webview-ui/src/components/chat/CodeIndexPopover.tsx
@@ -98,11 +98,11 @@ const createValidationSchema = (provider: EmbedderProvider, t: any) => {
 					.string()
 					.min(1, t("settings:codeIndex.validation.ollamaBaseUrlRequired"))
 					.url(t("settings:codeIndex.validation.invalidOllamaUrl")),
-				codebaseIndexEmbedderModelId: z.string().min(1, t("settings:codeIndex.validation.modelIdRequired")),
-				codebaseIndexEmbedderModelDimension: z
-					.number()
-					.min(1, t("settings:codeIndex.validation.modelDimensionRequired"))
-					.optional(),
+				// codebaseIndexEmbedderModelId: z.string().min(1, t("settings:codeIndex.validation.modelIdRequired")),
+				// codebaseIndexEmbedderModelDimension: z
+				// 	.number()
+				// 	.min(1, t("settings:codeIndex.validation.modelDimensionRequired"))
+				// 	.optional(),
 			})
 
 		case "openai-compatible":
@@ -147,7 +147,7 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 }) => {
 	const SECRET_PLACEHOLDER = "••••••••••••••••"
 	const { t } = useAppTranslation()
-	const { codebaseIndexConfig, codebaseIndexModels } = useExtensionState()
+	const { codebaseIndexConfig, codebaseIndexModels, cwd } = useExtensionState()
 	const [open, setOpen] = useState(false)
 	const [isAdvancedSettingsOpen, setIsAdvancedSettingsOpen] = useState(false)
 	const [isSetupSettingsOpen, setIsSetupSettingsOpen] = useState(false)
@@ -168,7 +168,7 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 	const getDefaultSettings = (): LocalCodeIndexSettings => ({
 		codebaseIndexEnabled: true,
 		codebaseIndexQdrantUrl: "",
-		codebaseIndexEmbedderProvider: "openai",
+		codebaseIndexEmbedderProvider: "openai-compatible",
 		codebaseIndexEmbedderBaseUrl: "",
 		codebaseIndexEmbedderModelId: "",
 		codebaseIndexEmbedderModelDimension: undefined,
@@ -199,7 +199,7 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 			const settings = {
 				codebaseIndexEnabled: codebaseIndexConfig.codebaseIndexEnabled ?? true,
 				codebaseIndexQdrantUrl: codebaseIndexConfig.codebaseIndexQdrantUrl || "",
-				codebaseIndexEmbedderProvider: codebaseIndexConfig.codebaseIndexEmbedderProvider || "openai",
+				codebaseIndexEmbedderProvider: codebaseIndexConfig.codebaseIndexEmbedderProvider || "openai-compatible",
 				codebaseIndexEmbedderBaseUrl: codebaseIndexConfig.codebaseIndexEmbedderBaseUrl || "",
 				codebaseIndexEmbedderModelId: codebaseIndexConfig.codebaseIndexEmbedderModelId || "",
 				codebaseIndexEmbedderModelDimension:
@@ -229,6 +229,18 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 			vscode.postMessage({ type: "requestIndexingStatus" })
 			vscode.postMessage({ type: "requestCodeIndexSecretStatus" })
 		}
+		const handleMessage = (event: MessageEvent) => {
+			if (event.data.type === "workspaceUpdated") {
+				// When workspace changes, request updated indexing status
+				if (open) {
+					vscode.postMessage({ type: "requestIndexingStatus" })
+					vscode.postMessage({ type: "requestCodeIndexSecretStatus" })
+				}
+			}
+		}
+
+		window.addEventListener("message", handleMessage)
+		return () => window.removeEventListener("message", handleMessage)
 	}, [open])
 
 	// Use a ref to capture current settings for the save handler
@@ -239,13 +251,15 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 	useEffect(() => {
 		const handleMessage = (event: MessageEvent<any>) => {
 			if (event.data.type === "indexingStatusUpdate") {
-				setIndexingStatus({
-					systemStatus: event.data.values.systemStatus,
-					message: event.data.values.message || "",
-					processedItems: event.data.values.processedItems,
-					totalItems: event.data.values.totalItems,
-					currentItemUnit: event.data.values.currentItemUnit || "items",
-				})
+				if (!event.data.values.workspacePath || event.data.values.workspacePath === cwd) {
+					setIndexingStatus({
+						systemStatus: event.data.values.systemStatus,
+						message: event.data.values.message || "",
+						processedItems: event.data.values.processedItems,
+						totalItems: event.data.values.totalItems,
+						currentItemUnit: event.data.values.currentItemUnit || "items",
+					})
+				}
 			} else if (event.data.type === "codeIndexSettingsSaved") {
 				if (event.data.success) {
 					setSaveStatus("saved")
@@ -273,7 +287,7 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 
 		window.addEventListener("message", handleMessage)
 		return () => window.removeEventListener("message", handleMessage)
-	}, [t])
+	}, [t, cwd])
 
 	// Listen for secret status
 	useEffect(() => {
@@ -613,21 +627,21 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 												<SelectValue />
 											</SelectTrigger>
 											<SelectContent>
-												<SelectItem value="openai">
-													{t("settings:codeIndex.openaiProvider")}
+												<SelectItem value="openai-compatible">
+													{t("settings:codeIndex.openaiCompatibleProvider")}
 												</SelectItem>
 												<SelectItem value="ollama">
-													{t("settings:codeIndex.ollamaProvider")}
+													{"Codebase-Service"}
 												</SelectItem>
-												<SelectItem value="openai-compatible">
-													{t("settings:codeIndex.openaiCompatibleProvider")}
+												{/* <SelectItem value="openai">
+													{t("settings:codeIndex.openaiProvider")}
 												</SelectItem>
 												<SelectItem value="gemini">
 													{t("settings:codeIndex.geminiProvider")}
 												</SelectItem>
 												<SelectItem value="mistral">
 													{t("settings:codeIndex.mistralProvider")}
-												</SelectItem>
+												</SelectItem> */}
 											</SelectContent>
 										</Select>
 									</div>
@@ -702,7 +716,7 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 										<>
 											<div className="space-y-2">
 												<label className="text-sm font-medium">
-													{t("settings:codeIndex.ollamaBaseUrlLabel")}
+													{"Codebase-Service URL"}
 												</label>
 												<VSCodeTextField
 													value={currentSettings.codebaseIndexEmbedderBaseUrl || ""}
@@ -731,7 +745,7 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 												)}
 											</div>
 
-											<div className="space-y-2">
+											{/* <div className="space-y-2">
 												<label className="text-sm font-medium">
 													{t("settings:codeIndex.modelLabel")}
 												</label>
@@ -778,7 +792,7 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 														{formErrors.codebaseIndexEmbedderModelDimension}
 													</p>
 												)}
-											</div>
+											</div> */}
 										</>
 									)}
 
@@ -1021,6 +1035,8 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 									)}
 
 									{/* Qdrant Settings */}
+									{currentSettings.codebaseIndexEmbedderProvider !== "ollama" && (
+										<>
 									<div className="space-y-2">
 										<label className="text-sm font-medium">
 											{t("settings:codeIndex.qdrantUrlLabel")}
@@ -1068,6 +1084,8 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 											</p>
 										)}
 									</div>
+									</>
+									)}
 								</div>
 							)}
 						</div>
diff --git a/webview-ui/src/components/chat/CommandExecution.tsx b/webview-ui/src/components/chat/CommandExecution.tsx
index 23d60a7a..cf25b1d9 100644
--- a/webview-ui/src/components/chat/CommandExecution.tsx
+++ b/webview-ui/src/components/chat/CommandExecution.tsx
@@ -211,9 +211,15 @@ const OutputContainerInternal = ({ isExpanded, output }: { isExpanded: boolean;
 	<div
 		className={cn("overflow-hidden", {
 			"max-h-0": !isExpanded,
-			"max-h-[100%] mt-1 pt-1 border-t border-border/25": isExpanded,
+			"mt-1 pt-1 border-t border-border/25": isExpanded,
 		})}>
-		{output.length > 0 && <CodeBlock source={output} language="log" />}
+		{output.length > 0 && (
+			<CodeBlock 
+				source={output} 
+				language="log" 
+				preStyle={{ overflowY: 'auto', maxHeight: '300px' }}
+			/>
+		)}
 	</div>
 )
 
diff --git a/webview-ui/src/components/chat/ContextMenu.tsx b/webview-ui/src/components/chat/ContextMenu.tsx
index 86965fcb..0ba83b4b 100644
--- a/webview-ui/src/components/chat/ContextMenu.tsx
+++ b/webview-ui/src/components/chat/ContextMenu.tsx
@@ -271,7 +271,7 @@ const ContextMenu: React.FC<ContextMenuProps> = ({
 					zIndex: 1000,
 					display: "flex",
 					flexDirection: "column",
-					maxHeight: "300px",
+					maxHeight: "405px", // 增加可以显示更多选项
 					overflowY: "auto",
 					overflowX: "hidden",
 				}}>
diff --git a/webview-ui/src/components/chat/__tests__/ChatTextArea.spec.tsx b/webview-ui/src/components/chat/__tests__/ChatTextArea.spec.tsx
index c056640c..e69de29b 100644
--- a/webview-ui/src/components/chat/__tests__/ChatTextArea.spec.tsx
+++ b/webview-ui/src/components/chat/__tests__/ChatTextArea.spec.tsx
@@ -1,1111 +0,0 @@
-import { render, fireEvent, screen } from "@/utils/test-utils"
-
-import { defaultModeSlug } from "@roo/modes"
-
-import { useExtensionState } from "@src/context/ExtensionStateContext"
-import { vscode } from "@src/utils/vscode"
-import * as pathMentions from "@src/utils/path-mentions"
-
-import ChatTextArea from "../ChatTextArea"
-
-vi.mock("@src/utils/vscode", () => ({
-	vscode: {
-		postMessage: vi.fn(),
-	},
-}))
-
-vi.mock("@src/components/common/CodeBlock")
-vi.mock("@src/components/common/MarkdownBlock")
-vi.mock("@src/utils/path-mentions", () => ({
-	convertToMentionPath: vi.fn((path, cwd) => {
-		// Simple mock implementation that mimics the real function's behavior
-		if (cwd && path.toLowerCase().startsWith(cwd.toLowerCase())) {
-			const relativePath = path.substring(cwd.length)
-			return "@" + (relativePath.startsWith("/") ? relativePath : "/" + relativePath)
-		}
-		return path
-	}),
-}))
-
-// Get the mocked postMessage function
-const mockPostMessage = vscode.postMessage as ReturnType<typeof vi.fn>
-const mockConvertToMentionPath = pathMentions.convertToMentionPath as ReturnType<typeof vi.fn>
-
-// Mock ExtensionStateContext
-vi.mock("@src/context/ExtensionStateContext")
-
-// Custom query function to get the enhance prompt button
-const getEnhancePromptButton = () => {
-	return screen.getByRole("button", {
-		name: (_, element) => {
-			// Find the button with the wand sparkles icon (Lucide React)
-			return element.querySelector(".lucide-wand-sparkles") !== null
-		},
-	})
-}
-
-describe("ChatTextArea", () => {
-	const defaultProps = {
-		inputValue: "",
-		setInputValue: vi.fn(),
-		onSend: vi.fn(),
-		sendingDisabled: false,
-		selectApiConfigDisabled: false,
-		onSelectImages: vi.fn(),
-		shouldDisableImages: false,
-		placeholderText: "Type a message...",
-		selectedImages: [],
-		setSelectedImages: vi.fn(),
-		onHeightChange: vi.fn(),
-		mode: defaultModeSlug,
-		setMode: vi.fn(),
-		modeShortcutText: "(⌘. for next mode)",
-	}
-
-	beforeEach(() => {
-		vi.clearAllMocks()
-		// Default mock implementation for useExtensionState
-		;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-			filePaths: [],
-			openedTabs: [],
-			apiConfiguration: {
-				apiProvider: "anthropic",
-			},
-			taskHistory: [],
-			cwd: "/test/workspace",
-		})
-	})
-
-	describe("enhance prompt button", () => {
-		it("should be enabled even when sendingDisabled is true (for message queueing)", () => {
-			;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-				filePaths: [],
-				openedTabs: [],
-				taskHistory: [],
-				cwd: "/test/workspace",
-			})
-			render(<ChatTextArea {...defaultProps} sendingDisabled={true} />)
-			const enhanceButton = getEnhancePromptButton()
-			expect(enhanceButton).toHaveClass("cursor-pointer")
-		})
-	})
-
-	describe("handleEnhancePrompt", () => {
-		it("should send message with correct configuration when clicked", () => {
-			const apiConfiguration = {
-				apiProvider: "openrouter",
-				apiKey: "test-key",
-			}
-
-			;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-				filePaths: [],
-				openedTabs: [],
-				apiConfiguration,
-				taskHistory: [],
-				cwd: "/test/workspace",
-			})
-
-			render(<ChatTextArea {...defaultProps} inputValue="Test prompt" />)
-
-			const enhanceButton = getEnhancePromptButton()
-			fireEvent.click(enhanceButton)
-
-			expect(mockPostMessage).toHaveBeenCalledWith({
-				type: "enhancePrompt",
-				text: "Test prompt",
-			})
-		})
-
-		it("should not send message when input is empty", () => {
-			;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-				filePaths: [],
-				openedTabs: [],
-				apiConfiguration: {
-					apiProvider: "openrouter",
-				},
-				taskHistory: [],
-				cwd: "/test/workspace",
-			})
-
-			render(<ChatTextArea {...defaultProps} inputValue="" />)
-
-			// Clear any calls from component initialization (e.g., IndexingStatusBadge)
-			mockPostMessage.mockClear()
-
-			const enhanceButton = getEnhancePromptButton()
-			fireEvent.click(enhanceButton)
-
-			expect(mockPostMessage).not.toHaveBeenCalled()
-		})
-
-		it("should show loading state while enhancing", () => {
-			;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-				filePaths: [],
-				openedTabs: [],
-				apiConfiguration: {
-					apiProvider: "openrouter",
-				},
-				taskHistory: [],
-				cwd: "/test/workspace",
-			})
-
-			render(<ChatTextArea {...defaultProps} inputValue="Test prompt" />)
-
-			const enhanceButton = getEnhancePromptButton()
-			fireEvent.click(enhanceButton)
-
-			// Check if the WandSparkles icon has the animate-spin class
-			const animatingIcon = enhanceButton.querySelector(".animate-spin")
-			expect(animatingIcon).toBeInTheDocument()
-		})
-	})
-
-	describe("effect dependencies", () => {
-		it("should update when apiConfiguration changes", () => {
-			const { rerender } = render(<ChatTextArea {...defaultProps} />)
-
-			// Update apiConfiguration
-			;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-				filePaths: [],
-				openedTabs: [],
-				apiConfiguration: {
-					apiProvider: "openrouter",
-					newSetting: "test",
-				},
-				taskHistory: [],
-				cwd: "/test/workspace",
-			})
-
-			rerender(<ChatTextArea {...defaultProps} />)
-
-			// Verify the enhance button appears after apiConfiguration changes
-			expect(getEnhancePromptButton()).toBeInTheDocument()
-		})
-	})
-
-	describe("enhanced prompt response", () => {
-		it("should update input value using native browser methods when receiving enhanced prompt", () => {
-			const setInputValue = vi.fn()
-
-			// Mock document.execCommand
-			const mockExecCommand = vi.fn().mockReturnValue(true)
-			Object.defineProperty(document, "execCommand", {
-				value: mockExecCommand,
-				writable: true,
-			})
-
-			const { container } = render(
-				<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Original prompt" />,
-			)
-
-			const textarea = container.querySelector("textarea")!
-
-			// Mock textarea methods
-			const mockSelect = vi.fn()
-			const mockFocus = vi.fn()
-			textarea.select = mockSelect
-			textarea.focus = mockFocus
-
-			// Simulate receiving enhanced prompt message
-			window.dispatchEvent(
-				new MessageEvent("message", {
-					data: {
-						type: "enhancedPrompt",
-						text: "Enhanced test prompt",
-					},
-				}),
-			)
-
-			// Verify native browser methods were used
-			expect(mockFocus).toHaveBeenCalled()
-			expect(mockSelect).toHaveBeenCalled()
-			expect(mockExecCommand).toHaveBeenCalledWith("insertText", false, "Enhanced test prompt")
-		})
-
-		it("should fallback to setInputValue when execCommand is not available", () => {
-			const setInputValue = vi.fn()
-
-			// Mock document.execCommand to be undefined (not available)
-			Object.defineProperty(document, "execCommand", {
-				value: undefined,
-				writable: true,
-			})
-
-			render(<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Original prompt" />)
-
-			// Simulate receiving enhanced prompt message
-			window.dispatchEvent(
-				new MessageEvent("message", {
-					data: {
-						type: "enhancedPrompt",
-						text: "Enhanced test prompt",
-					},
-				}),
-			)
-
-			// Verify fallback to setInputValue was used
-			expect(setInputValue).toHaveBeenCalledWith("Enhanced test prompt")
-		})
-
-		it("should not crash when textarea ref is not available", () => {
-			const setInputValue = vi.fn()
-
-			render(<ChatTextArea {...defaultProps} setInputValue={setInputValue} />)
-
-			// Simulate receiving enhanced prompt message when textarea ref might not be ready
-			expect(() => {
-				window.dispatchEvent(
-					new MessageEvent("message", {
-						data: {
-							type: "enhancedPrompt",
-							text: "Enhanced test prompt",
-						},
-					}),
-				)
-			}).not.toThrow()
-		})
-	})
-
-	describe("multi-file drag and drop", () => {
-		const mockCwd = "/Users/test/project"
-
-		beforeEach(() => {
-			vi.clearAllMocks()
-			;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-				filePaths: [],
-				openedTabs: [],
-				cwd: mockCwd,
-			})
-			mockConvertToMentionPath.mockClear()
-		})
-
-		it("should process multiple file paths separated by newlines", () => {
-			const setInputValue = vi.fn()
-
-			const { container } = render(
-				<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Initial text" />,
-			)
-
-			// Create a mock dataTransfer object with text data containing multiple file paths
-			const dataTransfer = {
-				getData: vi.fn().mockReturnValue("/Users/test/project/file1.js\n/Users/test/project/file2.js"),
-				files: [],
-			}
-
-			// Simulate drop event
-			fireEvent.drop(container.querySelector(".chat-text-area")!, {
-				dataTransfer,
-				preventDefault: vi.fn(),
-			})
-
-			// Verify convertToMentionPath was called for each file path
-			expect(mockConvertToMentionPath).toHaveBeenCalledTimes(2)
-			expect(mockConvertToMentionPath).toHaveBeenCalledWith("/Users/test/project/file1.js", mockCwd)
-			expect(mockConvertToMentionPath).toHaveBeenCalledWith("/Users/test/project/file2.js", mockCwd)
-
-			// Verify setInputValue was called with the correct value
-			// The mock implementation of convertToMentionPath will convert the paths to @/file1.js and @/file2.js
-			expect(setInputValue).toHaveBeenCalledWith("@/file1.js @/file2.js Initial text")
-		})
-
-		it("should filter out empty lines in the dragged text", () => {
-			const setInputValue = vi.fn()
-
-			const { container } = render(
-				<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Initial text" />,
-			)
-
-			// Create a mock dataTransfer object with text data containing empty lines
-			const dataTransfer = {
-				getData: vi.fn().mockReturnValue("/Users/test/project/file1.js\n\n/Users/test/project/file2.js\n\n"),
-				files: [],
-			}
-
-			// Simulate drop event
-			fireEvent.drop(container.querySelector(".chat-text-area")!, {
-				dataTransfer,
-				preventDefault: vi.fn(),
-			})
-
-			// Verify convertToMentionPath was called only for non-empty lines
-			expect(mockConvertToMentionPath).toHaveBeenCalledTimes(2)
-
-			// Verify setInputValue was called with the correct value
-			expect(setInputValue).toHaveBeenCalledWith("@/file1.js @/file2.js Initial text")
-		})
-
-		it("should correctly update cursor position after adding multiple mentions", () => {
-			const setInputValue = vi.fn()
-			const initialCursorPosition = 5
-
-			const { container } = render(
-				<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Hello world" />,
-			)
-
-			// Set the cursor position manually
-			const textArea = container.querySelector("textarea")
-			if (textArea) {
-				textArea.selectionStart = initialCursorPosition
-				textArea.selectionEnd = initialCursorPosition
-			}
-
-			// Create a mock dataTransfer object with text data
-			const dataTransfer = {
-				getData: vi.fn().mockReturnValue("/Users/test/project/file1.js\n/Users/test/project/file2.js"),
-				files: [],
-			}
-
-			// Simulate drop event
-			fireEvent.drop(container.querySelector(".chat-text-area")!, {
-				dataTransfer,
-				preventDefault: vi.fn(),
-			})
-
-			// The cursor position should be updated based on the implementation in the component
-			expect(setInputValue).toHaveBeenCalledWith("@/file1.js @/file2.js Hello world")
-		})
-
-		it("should handle very long file paths correctly", () => {
-			const setInputValue = vi.fn()
-
-			const { container } = render(<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />)
-
-			// Create a very long file path
-			const longPath =
-				"/Users/test/project/very/long/path/with/many/nested/directories/and/a/very/long/filename/with/extension.typescript"
-
-			// Create a mock dataTransfer object with the long path
-			const dataTransfer = {
-				getData: vi.fn().mockReturnValue(longPath),
-				files: [],
-			}
-
-			// Simulate drop event
-			fireEvent.drop(container.querySelector(".chat-text-area")!, {
-				dataTransfer,
-				preventDefault: vi.fn(),
-			})
-
-			// Verify convertToMentionPath was called with the long path
-			expect(mockConvertToMentionPath).toHaveBeenCalledWith(longPath, mockCwd)
-
-			// The mock implementation will convert it to @/very/long/path/...
-			expect(setInputValue).toHaveBeenCalledWith(
-				"@/very/long/path/with/many/nested/directories/and/a/very/long/filename/with/extension.typescript ",
-			)
-		})
-
-		it("should handle paths with special characters correctly", () => {
-			const setInputValue = vi.fn()
-
-			const { container } = render(<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />)
-
-			// Create paths with special characters
-			const specialPath1 = "/Users/test/project/file with spaces.js"
-			const specialPath2 = "/Users/test/project/file-with-dashes.js"
-			const specialPath3 = "/Users/test/project/file_with_underscores.js"
-			const specialPath4 = "/Users/test/project/file.with.dots.js"
-
-			// Create a mock dataTransfer object with the special paths
-			const dataTransfer = {
-				getData: vi.fn().mockReturnValue(`${specialPath1}\n${specialPath2}\n${specialPath3}\n${specialPath4}`),
-				files: [],
-			}
-
-			// Simulate drop event
-			fireEvent.drop(container.querySelector(".chat-text-area")!, {
-				dataTransfer,
-				preventDefault: vi.fn(),
-			})
-
-			// Verify convertToMentionPath was called for each path
-			expect(mockConvertToMentionPath).toHaveBeenCalledTimes(4)
-			expect(mockConvertToMentionPath).toHaveBeenCalledWith(specialPath1, mockCwd)
-			expect(mockConvertToMentionPath).toHaveBeenCalledWith(specialPath2, mockCwd)
-			expect(mockConvertToMentionPath).toHaveBeenCalledWith(specialPath3, mockCwd)
-			expect(mockConvertToMentionPath).toHaveBeenCalledWith(specialPath4, mockCwd)
-
-			// Verify setInputValue was called with the correct value
-			expect(setInputValue).toHaveBeenCalledWith(
-				"@/file with spaces.js @/file-with-dashes.js @/file_with_underscores.js @/file.with.dots.js ",
-			)
-		})
-
-		it("should handle paths outside the current working directory", () => {
-			const setInputValue = vi.fn()
-
-			const { container } = render(<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />)
-
-			// Create paths outside the current working directory
-			const outsidePath = "/Users/other/project/file.js"
-
-			// Mock the convertToMentionPath function to return the original path for paths outside cwd
-			mockConvertToMentionPath.mockImplementationOnce((path, _cwd) => {
-				return path // Return original path for this test
-			})
-
-			// Create a mock dataTransfer object with the outside path
-			const dataTransfer = {
-				getData: vi.fn().mockReturnValue(outsidePath),
-				files: [],
-			}
-
-			// Simulate drop event
-			fireEvent.drop(container.querySelector(".chat-text-area")!, {
-				dataTransfer,
-				preventDefault: vi.fn(),
-			})
-
-			// Verify convertToMentionPath was called with the outside path
-			expect(mockConvertToMentionPath).toHaveBeenCalledWith(outsidePath, mockCwd)
-
-			// Verify setInputValue was called with the original path
-			expect(setInputValue).toHaveBeenCalledWith("/Users/other/project/file.js ")
-		})
-
-		it("should do nothing when dropped text is empty", () => {
-			const setInputValue = vi.fn()
-
-			const { container } = render(
-				<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Initial text" />,
-			)
-
-			// Create a mock dataTransfer object with empty text
-			const dataTransfer = {
-				getData: vi.fn().mockReturnValue(""),
-				files: [],
-			}
-
-			// Simulate drop event
-			fireEvent.drop(container.querySelector(".chat-text-area")!, {
-				dataTransfer,
-				preventDefault: vi.fn(),
-			})
-
-			// Verify convertToMentionPath was not called
-			expect(mockConvertToMentionPath).not.toHaveBeenCalled()
-
-			// Verify setInputValue was not called
-			expect(setInputValue).not.toHaveBeenCalled()
-		})
-
-		describe("prompt history navigation", () => {
-			const mockClineMessages = [
-				{ type: "say", say: "user_feedback", text: "First prompt", ts: 1000 },
-				{ type: "say", say: "user_feedback", text: "Second prompt", ts: 2000 },
-				{ type: "say", say: "user_feedback", text: "Third prompt", ts: 3000 },
-			]
-
-			beforeEach(() => {
-				;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-					filePaths: [],
-					openedTabs: [],
-					apiConfiguration: {
-						apiProvider: "anthropic",
-					},
-					taskHistory: [],
-					clineMessages: mockClineMessages,
-					cwd: "/test/workspace",
-				})
-			})
-
-			it("should navigate to previous prompt on arrow up when cursor is at beginning", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-				// Ensure cursor is at the beginning
-				textarea.setSelectionRange(0, 0)
-
-				// Simulate arrow up key press
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-
-				// Should set the newest conversation message (first in reversed array)
-				expect(setInputValue).toHaveBeenCalledWith("Third prompt")
-			})
-
-			it("should navigate through history with multiple arrow up presses", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// First arrow up - newest conversation message
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Third prompt")
-
-				// Update input value to simulate the state change
-				setInputValue.mockClear()
-
-				// Second arrow up - previous conversation message
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Second prompt")
-			})
-
-			it("should navigate forward with arrow down", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Go back in history first (index 0 -> "Third prompt", then index 1 -> "Second prompt")
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				setInputValue.mockClear()
-
-				// Navigate forward (from index 1 back to index 0)
-				fireEvent.keyDown(textarea, { key: "ArrowDown" })
-				expect(setInputValue).toHaveBeenCalledWith("Third prompt")
-			})
-
-			it("should preserve current input when starting navigation", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Current input" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Navigate to history
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Third prompt")
-
-				setInputValue.mockClear()
-
-				// Navigate back to current input
-				fireEvent.keyDown(textarea, { key: "ArrowDown" })
-				expect(setInputValue).toHaveBeenCalledWith("Current input")
-			})
-
-			it("should reset history navigation when user types", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Navigate to history
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				setInputValue.mockClear()
-
-				// Type something
-				fireEvent.change(textarea, { target: { value: "New input", selectionStart: 9 } })
-
-				// Should reset history navigation
-				expect(setInputValue).toHaveBeenCalledWith("New input")
-			})
-
-			it("should reset history navigation when sending message", () => {
-				const onSend = vi.fn()
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea
-						{...defaultProps}
-						onSend={onSend}
-						setInputValue={setInputValue}
-						inputValue="Test message"
-					/>,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Navigate to history first
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				setInputValue.mockClear()
-
-				// Send message
-				fireEvent.keyDown(textarea, { key: "Enter" })
-
-				expect(onSend).toHaveBeenCalled()
-			})
-
-			it("should navigate history when cursor is at first line", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Clear any calls from initial render
-				setInputValue.mockClear()
-
-				// With empty input, cursor is at first line by default
-				// Arrow up should navigate history
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Third prompt")
-			})
-
-			it("should filter history by current workspace", () => {
-				const mixedClineMessages = [
-					{ type: "say", say: "user_feedback", text: "Workspace 1 prompt", ts: 1000 },
-					{ type: "say", say: "user_feedback", text: "Other workspace prompt", ts: 2000 },
-					{ type: "say", say: "user_feedback", text: "Workspace 1 prompt 2", ts: 3000 },
-				]
-
-				;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-					filePaths: [],
-					openedTabs: [],
-					apiConfiguration: {
-						apiProvider: "anthropic",
-					},
-					taskHistory: [],
-					clineMessages: mixedClineMessages,
-					cwd: "/test/workspace",
-				})
-
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Should show conversation messages newest first (after reverse)
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Workspace 1 prompt 2")
-
-				setInputValue.mockClear()
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Other workspace prompt")
-			})
-
-			it("should handle empty conversation history gracefully", () => {
-				;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-					filePaths: [],
-					openedTabs: [],
-					apiConfiguration: {
-						apiProvider: "anthropic",
-					},
-					taskHistory: [],
-					clineMessages: [],
-					cwd: "/test/workspace",
-				})
-
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Should not crash or call setInputValue
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).not.toHaveBeenCalled()
-			})
-
-			it("should ignore empty or whitespace-only messages", () => {
-				const clineMessagesWithEmpty = [
-					{ type: "say", say: "user_feedback", text: "Valid prompt", ts: 1000 },
-					{ type: "say", say: "user_feedback", text: "", ts: 2000 },
-					{ type: "say", say: "user_feedback", text: "   ", ts: 3000 },
-					{ type: "say", say: "user_feedback", text: "Another valid prompt", ts: 4000 },
-				]
-
-				;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-					filePaths: [],
-					openedTabs: [],
-					apiConfiguration: {
-						apiProvider: "anthropic",
-					},
-					taskHistory: [],
-					clineMessages: clineMessagesWithEmpty,
-					cwd: "/test/workspace",
-				})
-
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Should skip empty messages, newest first for conversation
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Another valid prompt")
-
-				setInputValue.mockClear()
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Valid prompt")
-			})
-
-			it("should use task history (oldest first) when no conversation messages exist", () => {
-				const mockTaskHistory = [
-					{ task: "First task", workspace: "/test/workspace" },
-					{ task: "Second task", workspace: "/test/workspace" },
-					{ task: "Third task", workspace: "/test/workspace" },
-				]
-
-				;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-					filePaths: [],
-					openedTabs: [],
-					apiConfiguration: {
-						apiProvider: "anthropic",
-					},
-					taskHistory: mockTaskHistory,
-					clineMessages: [], // No conversation messages
-					cwd: "/test/workspace",
-				})
-
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Should show task history oldest first (chronological order)
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("First task")
-
-				setInputValue.mockClear()
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Second task")
-			})
-
-			it("should reset navigation position when switching between history sources", () => {
-				const setInputValue = vi.fn()
-				const { rerender } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				// Start with task history
-				;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-					filePaths: [],
-					openedTabs: [],
-					apiConfiguration: {
-						apiProvider: "anthropic",
-					},
-					taskHistory: [
-						{ task: "Task 1", workspace: "/test/workspace" },
-						{ task: "Task 2", workspace: "/test/workspace" },
-					],
-					clineMessages: [],
-					cwd: "/test/workspace",
-				})
-
-				rerender(<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />)
-
-				const textarea = document.querySelector("textarea")!
-
-				// Navigate in task history
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Task 1")
-
-				// Switch to conversation messages
-				;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-					filePaths: [],
-					openedTabs: [],
-					apiConfiguration: {
-						apiProvider: "anthropic",
-					},
-					taskHistory: [],
-					clineMessages: [
-						{ type: "say", say: "user_feedback", text: "Message 1", ts: 1000 },
-						{ type: "say", say: "user_feedback", text: "Message 2", ts: 2000 },
-					],
-					cwd: "/test/workspace",
-				})
-
-				setInputValue.mockClear()
-				rerender(<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />)
-
-				// Should start from beginning of conversation history (newest first)
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Message 2")
-			})
-
-			it("should not navigate history with arrow up when cursor is not at beginning", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Some text here" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-				// Set cursor to middle of text (not at beginning)
-				textarea.setSelectionRange(5, 5)
-
-				// Clear any calls from initial render
-				setInputValue.mockClear()
-
-				// Simulate arrow up key press
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-
-				// Should not navigate history, allowing default behavior (move cursor to start)
-				expect(setInputValue).not.toHaveBeenCalled()
-			})
-
-			it("should navigate history with arrow up when cursor is at beginning", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Some text here" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-				// Set cursor to beginning of text
-				textarea.setSelectionRange(0, 0)
-
-				// Clear any calls from initial render
-				setInputValue.mockClear()
-
-				// Simulate arrow up key press
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-
-				// Should navigate to history since cursor is at beginning
-				expect(setInputValue).toHaveBeenCalledWith("Third prompt")
-			})
-
-			it("should navigate history with Command+Up when cursor is at beginning", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Some text here" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-				// Set cursor to beginning of text
-				textarea.setSelectionRange(0, 0)
-
-				// Clear any calls from initial render
-				setInputValue.mockClear()
-
-				// Simulate Command+Up key press
-				fireEvent.keyDown(textarea, { key: "ArrowUp", metaKey: true })
-
-				// Should navigate to history since cursor is at beginning (same as regular Up)
-				expect(setInputValue).toHaveBeenCalledWith("Third prompt")
-			})
-
-			it("should not navigate history with Command+Up when cursor is not at beginning", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Some text here" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-				// Set cursor to middle of text (not at beginning)
-				textarea.setSelectionRange(5, 5)
-
-				// Clear any calls from initial render
-				setInputValue.mockClear()
-
-				// Simulate Command+Up key press
-				fireEvent.keyDown(textarea, { key: "ArrowUp", metaKey: true })
-
-				// Should not navigate history, allowing default behavior (same as regular Up)
-				expect(setInputValue).not.toHaveBeenCalled()
-			})
-		})
-	})
-
-	describe("slash command highlighting", () => {
-		const mockCommands = [
-			{ name: "setup", source: "project", description: "Setup the project" },
-			{ name: "deploy", source: "global", description: "Deploy the application" },
-			{ name: "test-command", source: "project", description: "Test command with dash" },
-		]
-
-		beforeEach(() => {
-			;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-				filePaths: [],
-				openedTabs: [],
-				taskHistory: [],
-				cwd: "/test/workspace",
-				commands: mockCommands,
-			})
-		})
-
-		it("should highlight valid slash commands", () => {
-			const { getByTestId } = render(<ChatTextArea {...defaultProps} inputValue="/setup the project" />)
-
-			const highlightLayer = getByTestId("highlight-layer")
-			expect(highlightLayer).toBeInTheDocument()
-
-			// The highlighting is applied via innerHTML, so we need to check the content
-			// The valid command "/setup" should be highlighted
-			expect(highlightLayer.innerHTML).toContain('<mark class="mention-context-textarea-highlight">/setup</mark>')
-		})
-
-		it("should not highlight invalid slash commands", () => {
-			const { getByTestId } = render(<ChatTextArea {...defaultProps} inputValue="/invalid command" />)
-
-			const highlightLayer = getByTestId("highlight-layer")
-			expect(highlightLayer).toBeInTheDocument()
-
-			// The invalid command "/invalid" should not be highlighted
-			expect(highlightLayer.innerHTML).not.toContain(
-				'<mark class="mention-context-textarea-highlight">/invalid</mark>',
-			)
-			// But it should still contain the text without highlighting
-			expect(highlightLayer.innerHTML).toContain("/invalid")
-		})
-
-		it("should highlight only the command portion, not arguments", () => {
-			const { getByTestId } = render(<ChatTextArea {...defaultProps} inputValue="/deploy to production" />)
-
-			const highlightLayer = getByTestId("highlight-layer")
-			expect(highlightLayer).toBeInTheDocument()
-
-			// Only "/deploy" should be highlighted, not "to production"
-			expect(highlightLayer.innerHTML).toContain(
-				'<mark class="mention-context-textarea-highlight">/deploy</mark>',
-			)
-			expect(highlightLayer.innerHTML).not.toContain(
-				'<mark class="mention-context-textarea-highlight">/deploy to production</mark>',
-			)
-		})
-
-		it("should handle commands with dashes and underscores", () => {
-			const { getByTestId } = render(<ChatTextArea {...defaultProps} inputValue="/test-command with args" />)
-
-			const highlightLayer = getByTestId("highlight-layer")
-			expect(highlightLayer).toBeInTheDocument()
-
-			// The command with dash should be highlighted
-			expect(highlightLayer.innerHTML).toContain(
-				'<mark class="mention-context-textarea-highlight">/test-command</mark>',
-			)
-		})
-
-		it("should be case-sensitive when matching commands", () => {
-			const { getByTestId } = render(<ChatTextArea {...defaultProps} inputValue="/Setup the project" />)
-
-			const highlightLayer = getByTestId("highlight-layer")
-			expect(highlightLayer).toBeInTheDocument()
-
-			// "/Setup" (capital S) should not be highlighted since the command is "setup" (lowercase)
-			expect(highlightLayer.innerHTML).not.toContain(
-				'<mark class="mention-context-textarea-highlight">/Setup</mark>',
-			)
-			expect(highlightLayer.innerHTML).toContain("/Setup")
-		})
-
-		it("should highlight multiple valid commands in the same text", () => {
-			const { getByTestId } = render(<ChatTextArea {...defaultProps} inputValue="/setup first then /deploy" />)
-
-			const highlightLayer = getByTestId("highlight-layer")
-			expect(highlightLayer).toBeInTheDocument()
-
-			// Both valid commands should be highlighted
-			expect(highlightLayer.innerHTML).toContain('<mark class="mention-context-textarea-highlight">/setup</mark>')
-			expect(highlightLayer.innerHTML).toContain(
-				'<mark class="mention-context-textarea-highlight">/deploy</mark>',
-			)
-		})
-
-		it("should handle mixed valid and invalid commands", () => {
-			const { getByTestId } = render(
-				<ChatTextArea {...defaultProps} inputValue="/setup first then /invalid then /deploy" />,
-			)
-
-			const highlightLayer = getByTestId("highlight-layer")
-			expect(highlightLayer).toBeInTheDocument()
-
-			// Valid commands should be highlighted
-			expect(highlightLayer.innerHTML).toContain('<mark class="mention-context-textarea-highlight">/setup</mark>')
-			expect(highlightLayer.innerHTML).toContain(
-				'<mark class="mention-context-textarea-highlight">/deploy</mark>',
-			)
-
-			// Invalid command should not be highlighted
-			expect(highlightLayer.innerHTML).not.toContain(
-				'<mark class="mention-context-textarea-highlight">/invalid</mark>',
-			)
-			expect(highlightLayer.innerHTML).toContain("/invalid")
-		})
-
-		it("should work when no commands are available", () => {
-			;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-				filePaths: [],
-				openedTabs: [],
-				taskHistory: [],
-				cwd: "/test/workspace",
-				commands: undefined,
-			})
-
-			const { getByTestId } = render(<ChatTextArea {...defaultProps} inputValue="/setup the project" />)
-
-			const highlightLayer = getByTestId("highlight-layer")
-			expect(highlightLayer).toBeInTheDocument()
-
-			// No commands should be highlighted when commands array is undefined
-			expect(highlightLayer.innerHTML).not.toContain(
-				'<mark class="mention-context-textarea-highlight">/setup</mark>',
-			)
-			expect(highlightLayer.innerHTML).toContain("/setup")
-		})
-	})
-
-	describe("selectApiConfig", () => {
-		// Helper function to get the API config dropdown
-		const getApiConfigDropdown = () => {
-			return screen.getByTestId("dropdown-trigger")
-		}
-		it("should be enabled independently of sendingDisabled", () => {
-			render(<ChatTextArea {...defaultProps} sendingDisabled={true} selectApiConfigDisabled={false} />)
-			const apiConfigDropdown = getApiConfigDropdown()
-			expect(apiConfigDropdown).not.toHaveAttribute("disabled")
-		})
-		it("should be disabled when selectApiConfigDisabled is true", () => {
-			render(<ChatTextArea {...defaultProps} sendingDisabled={true} selectApiConfigDisabled={true} />)
-			const apiConfigDropdown = getApiConfigDropdown()
-			expect(apiConfigDropdown).toHaveAttribute("disabled")
-		})
-	})
-	describe("edit mode integration", () => {
-		it("should render edit mode UI when isEditMode is true", () => {
-			;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-				filePaths: [],
-				openedTabs: [],
-				taskHistory: [],
-				cwd: "/test/workspace",
-				customModes: [],
-				customModePrompts: {},
-			})
-
-			render(<ChatTextArea {...defaultProps} isEditMode={true} />)
-
-			// The edit mode UI should be rendered
-			// We can verify this by checking for the presence of elements that are unique to edit mode
-			const cancelButton = screen.getByRole("button", { name: /cancel/i })
-			expect(cancelButton).toBeInTheDocument()
-
-			// Should show save button instead of send button
-			const saveButton = screen.getByRole("button", { name: /save/i })
-			expect(saveButton).toBeInTheDocument()
-
-			// Should not show send button in edit mode
-			const sendButton = screen.queryByRole("button", { name: /send.*message/i })
-			expect(sendButton).not.toBeInTheDocument()
-		})
-
-		it("should not render edit mode UI when isEditMode is false", () => {
-			;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-				filePaths: [],
-				openedTabs: [],
-				taskHistory: [],
-				cwd: "/test/workspace",
-			})
-
-			render(<ChatTextArea {...defaultProps} isEditMode={false} />)
-
-			// The edit mode UI should not be rendered
-			const cancelButton = screen.queryByRole("button", { name: /cancel/i })
-			expect(cancelButton).not.toBeInTheDocument()
-
-			// Should show send button when not in edit mode
-			const sendButton = screen.getByRole("button", { name: /send.*message/i })
-			expect(sendButton).toBeInTheDocument()
-
-			// Should not show save button when not in edit mode
-			const saveButton = screen.queryByRole("button", { name: /save/i })
-			expect(saveButton).not.toBeInTheDocument()
-		})
-	})
-})
diff --git a/webview-ui/src/components/common/CodeAccordian.tsx b/webview-ui/src/components/common/CodeAccordian.tsx
index 7dcef11e..73f6ed51 100644
--- a/webview-ui/src/components/common/CodeAccordian.tsx
+++ b/webview-ui/src/components/common/CodeAccordian.tsx
@@ -35,6 +35,7 @@ const CodeAccordian = ({
 	const inferredLanguage = useMemo(() => language ?? (path ? getLanguageFromPath(path) : "txt"), [path, language])
 	const source = useMemo(() => code.trim(), [code])
 	const hasHeader = Boolean(path || isFeedback || header)
+	const isAgentEdits = Boolean(source.startsWith('# agentEdits'))
 
 	return (
 		<ToolUseBlock>
@@ -48,9 +49,9 @@ const CodeAccordian = ({
 						</div>
 					) : isFeedback ? (
 						<div className="flex items-center">
-							<span className={`codicon codicon-${isFeedback ? "feedback" : "codicon-output"} mr-1.5`} />
+							<span className={`codicon codicon-${isAgentEdits ? "hubot" : isFeedback ? "feedback" : "output"} mr-1.5`} />
 							<span className="whitespace-nowrap overflow-hidden text-ellipsis mr-2 rtl">
-								{isFeedback ? "User Edits" : "Console Logs"}
+								{isAgentEdits ? "Roo Edits" : isFeedback ? "User Edits" : "Console Logs"}
 							</span>
 						</div>
 					) : (
diff --git a/webview-ui/src/components/common/CodeBlock.tsx b/webview-ui/src/components/common/CodeBlock.tsx
index 28492acd..458fa3c2 100644
--- a/webview-ui/src/components/common/CodeBlock.tsx
+++ b/webview-ui/src/components/common/CodeBlock.tsx
@@ -145,7 +145,16 @@ export const StyledPre = styled.div<{
 		word-break: ${({ wordwrap }) => (wordwrap === "false" ? "normal" : "normal")};
 		overflow-wrap: ${({ wordwrap }) => (wordwrap === "false" ? "normal" : "break-word")};
 		font-size: var(--vscode-editor-font-size, var(--vscode-font-size, 12px));
-		font-family: var(--vscode-editor-font-family);
+		font-family: 'JetBrains Mono', 'Fira Code', Consolas, 'Courier New', monospace;
+    
+		@font-face {
+			font-family: 'code-chinese';
+			src: local('Microsoft YaHei'), local('PingFang SC'), local('SimHei');
+			unicode-range: U+4E00-9FFF, U+3400-4DBF, U+20000-2A6DF, U+2A700-2B73F, U+2B740-2B81F, U+2B820-2CEAF;
+			size-adjust: 90%;
+		}
+		
+		font-family: 'JetBrains Mono', 'Fira Code', Consolas, 'code-chinese', 'Courier New', monospace, var(--vscode-font-family);
 	}
 
 	pre > code {
diff --git a/webview-ui/src/components/common/MarkdownBlock.tsx b/webview-ui/src/components/common/MarkdownBlock.tsx
index cae609d9..57bf7bf7 100644
--- a/webview-ui/src/components/common/MarkdownBlock.tsx
+++ b/webview-ui/src/components/common/MarkdownBlock.tsx
@@ -17,7 +17,16 @@ interface MarkdownBlockProps {
 
 const StyledMarkdown = styled.div`
 	code:not(pre > code) {
-		font-family: var(--vscode-editor-font-family, monospace);
+		font-family: 'JetBrains Mono', 'Fira Code', Consolas, 'Courier New', monospace;
+	   
+		@font-face {
+			font-family: 'code-chinese';
+			src: local('Microsoft YaHei'), local('PingFang SC'), local('SimHei');
+			unicode-range: U+4E00-9FFF, U+3400-4DBF, U+20000-2A6DF, U+2A700-2B73F, U+2B740-2B81F, U+2B820-2CEAF;
+			size-adjust: 90%;
+		}
+		
+		font-family: 'JetBrains Mono', 'Fira Code', Consolas, 'code-chinese', 'Courier New', monospace, var(--vscode-font-family);
 		filter: saturation(110%) brightness(95%);
 		color: var(--vscode-textPreformat-foreground) !important;
 		background-color: var(--vscode-textPreformat-background) !important;
@@ -201,12 +210,12 @@ const MarkdownBlock = memo(({ markdown }: MarkdownBlockProps) => {
 					if (!isLocalPath) {
 						return
 					}
-
+					console.log("Requesting to open file:", href, children)
 					e.preventDefault()
 
 					// Handle absolute vs project-relative paths
 					let filePath = href.replace("file://", "")
-
+					
 					// Extract line number if present
 					const match = filePath.match(/(.*):(\d+)(-\d+)?$/)
 					let values = undefined
@@ -214,12 +223,12 @@ const MarkdownBlock = memo(({ markdown }: MarkdownBlockProps) => {
 						filePath = match[1]
 						values = { line: parseInt(match[2]) }
 					}
-
+					console.log("Requesting to open file:", filePath, values)
 					// Add ./ prefix if needed
 					if (!filePath.startsWith("/") && !filePath.startsWith("./")) {
 						filePath = "./" + filePath
 					}
-
+					console.log("Requesting to open file:", filePath, values)
 					vscode.postMessage({
 						type: "openFile",
 						text: filePath,
diff --git a/webview-ui/src/components/settings/ApiOptions.tsx b/webview-ui/src/components/settings/ApiOptions.tsx
index 5b9d90b3..4c5994be 100644
--- a/webview-ui/src/components/settings/ApiOptions.tsx
+++ b/webview-ui/src/components/settings/ApiOptions.tsx
@@ -19,6 +19,7 @@ import {
 	claudeCodeDefaultModelId,
 	geminiDefaultModelId,
 	deepSeekDefaultModelId,
+	modelScopeDefaultModelId,
 	moonshotDefaultModelId,
 	mistralDefaultModelId,
 	xaiDefaultModelId,
@@ -58,6 +59,7 @@ import {
 	Chutes,
 	ClaudeCode,
 	DeepSeek,
+	ModelScope,
 	Doubao,
 	Gemini,
 	Glama,
@@ -294,6 +296,7 @@ const ApiOptions = ({
 				"openai-native": { field: "apiModelId", default: openAiNativeDefaultModelId },
 				gemini: { field: "apiModelId", default: geminiDefaultModelId },
 				deepseek: { field: "apiModelId", default: deepSeekDefaultModelId },
+				modelscope: { field: "apiModelId", default: modelScopeDefaultModelId },
 				doubao: { field: "apiModelId", default: doubaoDefaultModelId },
 				moonshot: { field: "apiModelId", default: moonshotDefaultModelId },
 				mistral: { field: "apiModelId", default: mistralDefaultModelId },
@@ -474,6 +477,10 @@ const ApiOptions = ({
 				<LMStudio apiConfiguration={apiConfiguration} setApiConfigurationField={setApiConfigurationField} />
 			)}
 
+			{selectedProvider === "modelscope" && (
+				<ModelScope apiConfiguration={apiConfiguration} setApiConfigurationField={setApiConfigurationField} />
+			)}
+
 			{selectedProvider === "deepseek" && (
 				<DeepSeek apiConfiguration={apiConfiguration} setApiConfigurationField={setApiConfigurationField} />
 			)}
diff --git a/webview-ui/src/components/settings/AutoApproveToggle.tsx b/webview-ui/src/components/settings/AutoApproveToggle.tsx
index e8b51b01..6f44b241 100644
--- a/webview-ui/src/components/settings/AutoApproveToggle.tsx
+++ b/webview-ui/src/components/settings/AutoApproveToggle.tsx
@@ -109,13 +109,7 @@ export const AutoApproveToggle = ({ onToggle, ...props }: AutoApproveToggleProps
 	const { t } = useAppTranslation()
 
 	return (
-		<div
-			className={cn(
-				"flex flex-row flex-wrap justify-center gap-2 max-w-[600px] mx-auto my-2 ",
-				"[@media(min-width:600px)]:gap-4",
-				"[@media(min-width:800px)]:max-w-[900px]",
-				"[@media(min-width:1200px)]:max-w-[1800px]",
-			)}>
+		<div className={cn("flex flex-row flex-wrap gap-2 py-2")}>
 			{Object.values(autoApproveSettingsConfig).map(({ key, descriptionKey, labelKey, icon, testId }) => (
 				<StandardTooltip key={key} content={t(descriptionKey || "")}>
 					<Button
@@ -124,14 +118,16 @@ export const AutoApproveToggle = ({ onToggle, ...props }: AutoApproveToggleProps
 						aria-label={t(labelKey)}
 						aria-pressed={!!props[key]}
 						data-testid={testId}
-						className={cn(" aspect-square h-[80px]", !props[key] && "opacity-50")}>
-						<span className={cn("flex flex-col items-center gap-1")}>
-							<span className={`codicon codicon-${icon}`} />
-							<span className="text-sm text-center">{t(labelKey)}</span>
-						</span>
+						className={cn(
+							"h-7 px-2 rounded-md flex items-center gap-1.5 text-xs whitespace-nowrap",
+							!props[key] && "opacity-50",
+						)}>
+						<span className={`codicon codicon-${icon} text-sm`} />
+						<span>{t(labelKey)}</span>
 					</Button>
 				</StandardTooltip>
 			))}
 		</div>
 	)
 }
+
diff --git a/webview-ui/src/components/settings/ExperimentalSettings.tsx b/webview-ui/src/components/settings/ExperimentalSettings.tsx
index 53801232..9d2558ec 100644
--- a/webview-ui/src/components/settings/ExperimentalSettings.tsx
+++ b/webview-ui/src/components/settings/ExperimentalSettings.tsx
@@ -39,29 +39,15 @@ export const ExperimentalSettings = ({
 				{Object.entries(experimentConfigsMap)
 					.filter(([key]) => key in EXPERIMENT_IDS)
 					.map((config) => {
-						if (config[0] === "MULTI_FILE_APPLY_DIFF") {
-							return (
-								<ExperimentalFeature
-									key={config[0]}
-									experimentKey={config[0]}
-									enabled={experiments[EXPERIMENT_IDS.MULTI_FILE_APPLY_DIFF] ?? false}
-									onChange={(enabled) =>
-										setExperimentEnabled(EXPERIMENT_IDS.MULTI_FILE_APPLY_DIFF, enabled)
-									}
-								/>
-							)
-						}
+						const experimentKey = config[0] as keyof typeof EXPERIMENT_IDS;
+						const experimentId = EXPERIMENT_IDS[experimentKey];
+						
 						return (
 							<ExperimentalFeature
-								key={config[0]}
-								experimentKey={config[0]}
-								enabled={experiments[EXPERIMENT_IDS[config[0] as keyof typeof EXPERIMENT_IDS]] ?? false}
-								onChange={(enabled) =>
-									setExperimentEnabled(
-										EXPERIMENT_IDS[config[0] as keyof typeof EXPERIMENT_IDS],
-										enabled,
-									)
-								}
+								key={experimentKey}
+								experimentKey={experimentKey}
+								enabled={experiments[experimentId] ?? false}
+								onChange={(enabled) => setExperimentEnabled(experimentId, enabled)}
 							/>
 						)
 					})}
diff --git a/webview-ui/src/components/settings/SettingsView.tsx b/webview-ui/src/components/settings/SettingsView.tsx
index 9cfd9b64..71cf6e5e 100644
--- a/webview-ui/src/components/settings/SettingsView.tsx
+++ b/webview-ui/src/components/settings/SettingsView.tsx
@@ -226,6 +226,14 @@ const SettingsView = forwardRef<SettingsViewRef, SettingsViewProps>(({ onDone, t
 
 				const previousValue = prevState.apiConfiguration?.[field]
 
+				if (Object.keys(previousValue || {}).length === 0 && Object.keys(value || {}).length === 0) {
+					return prevState
+				}
+
+				if (value === "__undefined__") {
+					return prevState
+				}
+
 				// Don't treat initial sync from undefined to a defined value as a user change
 				// This prevents the dirty state when the component initializes and auto-syncs the model ID
 				const isInitialSync = previousValue === undefined && value !== undefined
diff --git a/webview-ui/src/components/settings/constants.ts b/webview-ui/src/components/settings/constants.ts
index b8aa84cb..a6e08354 100644
--- a/webview-ui/src/components/settings/constants.ts
+++ b/webview-ui/src/components/settings/constants.ts
@@ -5,6 +5,7 @@ import {
 	bedrockModels,
 	claudeCodeModels,
 	deepSeekModels,
+	modelScopeModels,
 	moonshotModels,
 	geminiModels,
 	mistralModels,
@@ -22,7 +23,7 @@ export const MODELS_BY_PROVIDER: Partial<Record<ProviderName, Record<string, Mod
 	"claude-code": claudeCodeModels,
 	bedrock: bedrockModels,
 	deepseek: deepSeekModels,
-	doubao: doubaoModels,
+	modelscope: modelScopeModels,
 	moonshot: moonshotModels,
 	gemini: geminiModels,
 	mistral: mistralModels,
@@ -36,28 +37,58 @@ export const MODELS_BY_PROVIDER: Partial<Record<ProviderName, Record<string, Mod
 
 export const PROVIDERS = [
 	{ value: "openrouter", label: "OpenRouter" },
-	{ value: "anthropic", label: "Anthropic" },
-	{ value: "claude-code", label: "Claude Code" },
+	// { value: "anthropic", label: "Anthropic" },
+	// { value: "claude-code", label: "Claude Code" },
 	{ value: "gemini", label: "Google Gemini" },
-	{ value: "doubao", label: "Doubao" },
+	// { value: "doubao", label: "Doubao" },
 	{ value: "deepseek", label: "DeepSeek" },
-	{ value: "moonshot", label: "Moonshot" },
+	// { value: "moonshot", label: "Moonshot" },
 	{ value: "openai-native", label: "OpenAI" },
 	{ value: "openai", label: "OpenAI Compatible" },
-	{ value: "vertex", label: "GCP Vertex AI" },
-	{ value: "bedrock", label: "Amazon Bedrock" },
-	{ value: "glama", label: "Glama" },
-	{ value: "vscode-lm", label: "VS Code LM API" },
-	{ value: "mistral", label: "Mistral" },
-	{ value: "lmstudio", label: "LM Studio" },
-	{ value: "ollama", label: "Ollama" },
-	{ value: "unbound", label: "Unbound" },
-	{ value: "requesty", label: "Requesty" },
-	{ value: "human-relay", label: "Human Relay" },
-	{ value: "xai", label: "xAI (Grok)" },
-	{ value: "groq", label: "Groq" },
-	{ value: "huggingface", label: "Hugging Face" },
-	{ value: "chutes", label: "Chutes AI" },
-	{ value: "litellm", label: "LiteLLM" },
-	{ value: "sambanova", label: "SambaNova" },
+	// { value: "vertex", label: "GCP Vertex AI" },
+	// { value: "bedrock", label: "Amazon Bedrock" },
+	// { value: "glama", label: "Glama" },
+	// { value: "vscode-lm", label: "VS Code LM API" },
+	// { value: "mistral", label: "Mistral" },
+	// { value: "lmstudio", label: "LM Studio" },
+	// { value: "ollama", label: "Ollama" },
+	// { value: "unbound", label: "Unbound" },
+	// { value: "requesty", label: "Requesty" },
+	// { value: "human-relay", label: "Human Relay" },
+	// { value: "xai", label: "xAI (Grok)" },
+	// { value: "groq", label: "Groq" },
+	// { value: "huggingface", label: "Hugging Face" },
+	// { value: "chutes", label: "Chutes AI" },
+	// { value: "litellm", label: "LiteLLM" },
+	// { value: "sambanova", label: "SambaNova" },
+	{ value: "modelscope", label: "ModelScope" },
 ].sort((a, b) => a.label.localeCompare(b.label))
+
+// export const PROVIDERS = [
+// 	// { value: "openrouter", label: "OpenRouter" },
+// 	// { value: "anthropic", label: "Anthropic" },
+// 	// { value: "claude-code", label: "Claude Code" },
+// 	// { value: "gemini", label: "Google Gemini" },
+// 	// { value: "doubao", label: "Doubao" },
+// 	// { value: "deepseek", label: "DeepSeek" },
+// 	// { value: "moonshot", label: "Moonshot" },
+// 	// { value: "openai-native", label: "OpenAI" },
+// 	{ value: "openai", label: "OpenAI Compatible" },
+// 	// { value: "vertex", label: "GCP Vertex AI" },
+// 	// { value: "bedrock", label: "Amazon Bedrock" },
+// 	// { value: "glama", label: "Glama" },
+// 	// { value: "vscode-lm", label: "VS Code LM API" },
+// 	// { value: "mistral", label: "Mistral" },
+// 	// { value: "lmstudio", label: "LM Studio" },
+// 	// { value: "ollama", label: "Ollama" },
+// 	// { value: "unbound", label: "Unbound" },
+// 	// { value: "requesty", label: "Requesty" },
+// 	// { value: "human-relay", label: "Human Relay" },
+// 	// { value: "xai", label: "xAI (Grok)" },
+// 	// { value: "groq", label: "Groq" },
+// 	// { value: "huggingface", label: "Hugging Face" },
+// 	// { value: "chutes", label: "Chutes AI" },
+// 	// { value: "litellm", label: "LiteLLM" },
+// 	// { value: "sambanova", label: "SambaNova" },
+// 	// { value: "modelscope", label: "ModelScope" },
+// ].sort((a, b) => a.label.localeCompare(b.label))
diff --git a/webview-ui/src/components/settings/providers/index.ts b/webview-ui/src/components/settings/providers/index.ts
index 13420b26..dd730dbf 100644
--- a/webview-ui/src/components/settings/providers/index.ts
+++ b/webview-ui/src/components/settings/providers/index.ts
@@ -9,6 +9,7 @@ export { Glama } from "./Glama"
 export { Groq } from "./Groq"
 export { HuggingFace } from "./HuggingFace"
 export { LMStudio } from "./LMStudio"
+export { ModelScope } from "./ModelScope"
 export { Mistral } from "./Mistral"
 export { Moonshot } from "./Moonshot"
 export { Ollama } from "./Ollama"
diff --git a/webview-ui/src/components/ui/hooks/useOpenRouterModelProviders.ts b/webview-ui/src/components/ui/hooks/useOpenRouterModelProviders.ts
index dc50c0f6..0c9ad9a7 100644
--- a/webview-ui/src/components/ui/hooks/useOpenRouterModelProviders.ts
+++ b/webview-ui/src/components/ui/hooks/useOpenRouterModelProviders.ts
@@ -43,7 +43,7 @@ async function getOpenRouterProvidersForModel(modelId: string) {
 	const models: Record<string, OpenRouterModelProvider> = {}
 
 	try {
-		const response = await axios.get(`https://openrouter.ai/api/v1/models/${modelId}/endpoints`)
+		const response = await axios.get(`https://riddler.mynatapp.cc/llm/openrouter/v1/models/${modelId}/endpoints`)
 		const result = openRouterEndpointsSchema.safeParse(response.data)
 
 		if (!result.success) {
diff --git a/webview-ui/src/components/ui/hooks/useSelectedModel.ts b/webview-ui/src/components/ui/hooks/useSelectedModel.ts
index 6bda83ab..e4a3f77f 100644
--- a/webview-ui/src/components/ui/hooks/useSelectedModel.ts
+++ b/webview-ui/src/components/ui/hooks/useSelectedModel.ts
@@ -8,6 +8,8 @@ import {
 	bedrockModels,
 	deepSeekDefaultModelId,
 	deepSeekModels,
+	modelScopeDefaultModelId,
+	modelScopeModels,
 	moonshotDefaultModelId,
 	moonshotModels,
 	geminiDefaultModelId,
@@ -62,7 +64,7 @@ export const useSelectedModel = (apiConfiguration?: ProviderSettings) => {
 					routerModels: routerModels.data,
 					openRouterModelProviders: openRouterModelProviders.data,
 				})
-			: { id: anthropicDefaultModelId, info: undefined }
+			: { id: "__undefined__", info: undefined }
 
 	return {
 		provider,
@@ -178,6 +180,11 @@ function getSelectedModel({
 			const info = deepSeekModels[id as keyof typeof deepSeekModels]
 			return { id, info }
 		}
+		case "modelscope": {
+			const id = apiConfiguration.apiModelId ?? modelScopeDefaultModelId
+			const info = modelScopeModels[id as keyof typeof modelScopeModels]
+			return { id, info }
+		}
 		case "doubao": {
 			const id = apiConfiguration.apiModelId ?? doubaoDefaultModelId
 			const info = doubaoModels[id as keyof typeof doubaoModels]
diff --git a/webview-ui/src/context/__tests__/ExtensionStateContext.spec.tsx b/webview-ui/src/context/__tests__/ExtensionStateContext.spec.tsx
index 7c69f39c..bae96fdf 100644
--- a/webview-ui/src/context/__tests__/ExtensionStateContext.spec.tsx
+++ b/webview-ui/src/context/__tests__/ExtensionStateContext.spec.tsx
@@ -229,6 +229,7 @@ describe("mergeExtensionState", () => {
 				concurrentFileReads: true,
 				multiFileApplyDiff: true,
 				preventFocusDisruption: false,
+				allowedMultiCall:false
 			} as Record<ExperimentId, boolean>,
 		}
 
@@ -246,6 +247,7 @@ describe("mergeExtensionState", () => {
 			concurrentFileReads: true,
 			multiFileApplyDiff: true,
 			preventFocusDisruption: false,
+			allowedMultiCall:false
 		})
 	})
 })
diff --git a/webview-ui/src/i18n/locales/ca/settings.json b/webview-ui/src/i18n/locales/ca/settings.json
index 3da1f5e5..add8b356 100644
--- a/webview-ui/src/i18n/locales/ca/settings.json
+++ b/webview-ui/src/i18n/locales/ca/settings.json
@@ -677,6 +677,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "Edició en segon pla",
 			"description": "Quan s'activa, evita la interrupció del focus de l'editor. Les edicions de fitxers es produeixen en segon pla sense obrir la vista diff o robar el focus. Pots continuar treballant sense interrupcions mentre Roo fa canvis. Els fitxers poden obrir-se sense focus per capturar diagnòstics o romandre completament tancats."
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "Permetre múltiples crides d'eines per missatge",
+			"description": "Quan està activat, Roo pot utilitzar múltiples eines en un sol missatge. Quan està desactivat, Roo ha d'utilitzar una eina per missatge. Activar això pot fer que les interaccions siguin més ràpides, però pot causar comportaments inesperats amb alguns models."
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/de/settings.json b/webview-ui/src/i18n/locales/de/settings.json
index a96b41f0..9b4647a4 100644
--- a/webview-ui/src/i18n/locales/de/settings.json
+++ b/webview-ui/src/i18n/locales/de/settings.json
@@ -677,6 +677,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "Hintergrundbearbeitung",
 			"description": "Verhindert Editor-Fokus-Störungen wenn aktiviert. Dateibearbeitungen erfolgen im Hintergrund ohne Öffnung von Diff-Ansichten oder Fokus-Diebstahl. Du kannst ungestört weiterarbeiten, während Roo Änderungen vornimmt. Dateien können ohne Fokus geöffnet werden, um Diagnosen zu erfassen oder vollständig geschlossen bleiben."
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "Mehrere Tool-Aufrufe pro Nachricht erlauben",
+			"description": "Wenn aktiviert, kann Roo mehrere Tools in einer einzigen Nachricht verwenden. Wenn deaktiviert, muss Roo ein Tool pro Nachricht verwenden. Das Aktivieren kann die Interaktionen beschleunigen, aber bei einigen Modellen zu unerwartetem Verhalten führen."
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/en/settings.json b/webview-ui/src/i18n/locales/en/settings.json
index 5f67268b..58aa4201 100644
--- a/webview-ui/src/i18n/locales/en/settings.json
+++ b/webview-ui/src/i18n/locales/en/settings.json
@@ -676,6 +676,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "Background editing",
 			"description": "Prevent editor focus disruption when enabled. File edits happen in the background without opening diff views or stealing focus. You can continue working uninterrupted while Roo makes changes. Files can be opened without focus to capture diagnostics or kept closed entirely."
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "Allow multiple tool calls per message",
+			"description": "When enabled, Roo can use multiple tools in a single message. When disabled, Roo must use one tool per message. Enabling this can make interactions faster but may cause unexpected behavior with some models."
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/es/settings.json b/webview-ui/src/i18n/locales/es/settings.json
index c4c7fb39..0f69497e 100644
--- a/webview-ui/src/i18n/locales/es/settings.json
+++ b/webview-ui/src/i18n/locales/es/settings.json
@@ -677,6 +677,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "Edición en segundo plano",
 			"description": "Previene la interrupción del foco del editor cuando está habilitado. Las ediciones de archivos ocurren en segundo plano sin abrir vistas de diferencias o robar el foco. Puedes continuar trabajando sin interrupciones mientras Roo realiza cambios. Los archivos pueden abrirse sin foco para capturar diagnósticos o mantenerse completamente cerrados."
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "Permitir múltiples llamadas a herramientas por mensaje",
+			"description": "Cuando está habilitado, Roo puede usar múltiples herramientas en un solo mensaje. Cuando está deshabilitado, Roo debe usar una herramienta por mensaje. Habilitar esto puede hacer que las interacciones sean más rápidas pero puede causar comportamientos inesperados con algunos modelos."
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/fr/settings.json b/webview-ui/src/i18n/locales/fr/settings.json
index e00b0c05..01a863af 100644
--- a/webview-ui/src/i18n/locales/fr/settings.json
+++ b/webview-ui/src/i18n/locales/fr/settings.json
@@ -677,6 +677,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "Édition en arrière-plan",
 			"description": "Empêche la perturbation du focus de l'éditeur lorsqu'activé. Les modifications de fichiers se font en arrière-plan sans ouvrir de vues de différences ou voler le focus. Vous pouvez continuer à travailler sans interruption pendant que Roo effectue des changements. Les fichiers peuvent être ouverts sans focus pour capturer les diagnostics ou rester complètement fermés."
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "Autoriser plusieurs appels d'outils par message",
+			"description": "Lorsque cette option est activée, Roo peut utiliser plusieurs outils dans un seul message. Lorsque désactivé, Roo doit utiliser un outil par message. Activer cette option peut accélérer les interactions mais peut provoquer un comportement inattendu avec certains modèles."
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/hi/settings.json b/webview-ui/src/i18n/locales/hi/settings.json
index 2497ffe6..395b6b6f 100644
--- a/webview-ui/src/i18n/locales/hi/settings.json
+++ b/webview-ui/src/i18n/locales/hi/settings.json
@@ -678,6 +678,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "बैकग्राउंड संपादन",
 			"description": "सक्षम होने पर एडिटर फोकस व्यवधान को रोकता है। फ़ाइल संपादन diff व्यू खोले बिना या फोकस चुराए बिना बैकग्राउंड में होता है। आप Roo के बदलाव करते समय बिना किसी बाधा के काम जारी रख सकते हैं। फ़ाइलें डायग्नोस्टिक्स कैप्चर करने के लिए बिना फोकस के खुल सकती हैं या पूरी तरह बंद रह सकती हैं।"
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "प्रति संदेश कई उपकरण कॉल की अनुमति दें",
+			"description": "सक्षम होने पर, Roo एक ही संदेश में कई उपकरणों का उपयोग कर सकता है। अक्षम होने पर, Roo को प्रति संदेश एक उपकरण का उपयोग करना होगा। इसे सक्षम करने से इंटरैक्शन तेज़ हो सकते हैं लेकिन कुछ मॉडल के साथ अप्रत्याशित व्यवहार हो सकता है।"
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/id/settings.json b/webview-ui/src/i18n/locales/id/settings.json
index 3665c99c..3271eb96 100644
--- a/webview-ui/src/i18n/locales/id/settings.json
+++ b/webview-ui/src/i18n/locales/id/settings.json
@@ -707,6 +707,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "Pengeditan Latar Belakang",
 			"description": "Ketika diaktifkan, mencegah gangguan fokus editor. Pengeditan file terjadi di latar belakang tanpa membuka tampilan diff atau mencuri fokus. Anda dapat terus bekerja tanpa gangguan saat Roo melakukan perubahan. File mungkin dibuka tanpa fokus untuk menangkap diagnostik atau tetap tertutup sepenuhnya."
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "Izinkan beberapa panggilan alat per pesan",
+			"description": "Ketika diaktifkan, Roo dapat menggunakan beberapa alat dalam satu pesan. Ketika dinonaktifkan, Roo harus menggunakan satu alat per pesan. Mengaktifkan ini dapat mempercepat interaksi tetapi dapat menyebabkan perilaku yang tidak terduga dengan beberapa model."
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/it/settings.json b/webview-ui/src/i18n/locales/it/settings.json
index 056b0f91..553d8436 100644
--- a/webview-ui/src/i18n/locales/it/settings.json
+++ b/webview-ui/src/i18n/locales/it/settings.json
@@ -678,6 +678,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "Modifica in background",
 			"description": "Previene l'interruzione del focus dell'editor quando abilitato. Le modifiche ai file avvengono in background senza aprire viste di differenze o rubare il focus. Puoi continuare a lavorare senza interruzioni mentre Roo effettua modifiche. I file possono essere aperti senza focus per catturare diagnostiche o rimanere completamente chiusi."
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "Consenti chiamate multiple di strumenti per messaggio",
+			"description": "Quando abilitato, Roo può utilizzare più strumenti in un unico messaggio. Quando disabilitato, Roo deve utilizzare uno strumento per messaggio. Abilitare questa opzione può rendere le interazioni più veloci, ma può causare comportamenti imprevisti con alcuni modelli."
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/ja/settings.json b/webview-ui/src/i18n/locales/ja/settings.json
index 3b38277b..ea3a4c61 100644
--- a/webview-ui/src/i18n/locales/ja/settings.json
+++ b/webview-ui/src/i18n/locales/ja/settings.json
@@ -678,6 +678,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "バックグラウンド編集",
 			"description": "有効にすると、エディターのフォーカス中断を防ぎます。ファイル編集は差分ビューを開いたりフォーカスを奪ったりすることなく、バックグラウンドで行われます。Rooが変更を行っている間も中断されることなく作業を続けることができます。ファイルは診断をキャプチャするためにフォーカスなしで開くか、完全に閉じたままにできます。"
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "メッセージごとに複数のツール呼び出しを許可",
+			"description": "有効にすると、Rooは単一のメッセージで複数のツールを使用できます。無効にすると、Rooはメッセージごとに1つのツールを使用する必要があります。これを有効にすることでインタラクションが高速化されますが、一部のモデルでは予期しない動作が発生する可能性があります。"
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/ko/settings.json b/webview-ui/src/i18n/locales/ko/settings.json
index 89739f4c..cdc2387e 100644
--- a/webview-ui/src/i18n/locales/ko/settings.json
+++ b/webview-ui/src/i18n/locales/ko/settings.json
@@ -678,6 +678,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "백그라운드 편집",
 			"description": "활성화하면 편집기 포커스 방해를 방지합니다. 파일 편집이 diff 뷰를 열거나 포커스를 빼앗지 않고 백그라운드에서 수행됩니다. Roo가 변경사항을 적용하는 동안 방해받지 않고 계속 작업할 수 있습니다. 파일은 진단을 캡처하기 위해 포커스 없이 열거나 완전히 닫힌 상태로 유지할 수 있습니다."
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "메시지당 여러 도구 호출 허용",
+			"description": "활성화하면 Roo가 단일 메시지에서 여러 도구를 사용할 수 있습니다. 비활성화하면 Roo는 메시지당 하나의 도구만 사용해야 합니다. 이를 활성화하면 상호작용이 빨라지지만 일부 모델에서는 예기치 않은 동작이 발생할 수 있습니다."
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/nl/settings.json b/webview-ui/src/i18n/locales/nl/settings.json
index 7d8721ff..00084a18 100644
--- a/webview-ui/src/i18n/locales/nl/settings.json
+++ b/webview-ui/src/i18n/locales/nl/settings.json
@@ -678,6 +678,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "Achtergrondbewerking",
 			"description": "Voorkomt editor focus verstoring wanneer ingeschakeld. Bestandsbewerkingen gebeuren op de achtergrond zonder diff-weergaven te openen of focus te stelen. Je kunt ononderbroken doorwerken terwijl Roo wijzigingen aanbrengt. Bestanden kunnen zonder focus worden geopend om diagnostiek vast te leggen of volledig gesloten blijven."
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "Meerdere tool-aanroepen per bericht toestaan",
+			"description": "Indien ingeschakeld, kan Roo meerdere tools in één enkel bericht gebruiken. Indien uitgeschakeld, moet Roo één tool per bericht gebruiken. Het inschakelen hiervan kan interacties sneller maken, maar kan onverwacht gedrag veroorzaken met sommige modellen."
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/pl/settings.json b/webview-ui/src/i18n/locales/pl/settings.json
index 5a23d213..5ac09310 100644
--- a/webview-ui/src/i18n/locales/pl/settings.json
+++ b/webview-ui/src/i18n/locales/pl/settings.json
@@ -678,6 +678,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "Edycja w tle",
 			"description": "Zapobiega zakłócaniu fokusa edytora gdy włączone. Edycje plików odbywają się w tle bez otwierania widoków różnic lub kradzieży fokusa. Możesz kontynuować pracę bez przeszkód podczas gdy Roo wprowadza zmiany. Pliki mogą być otwierane bez fokusa aby przechwycić diagnostykę lub pozostać całkowicie zamknięte."
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "Zezwalaj na wiele wywołań narzędzi na wiadomość",
+			"description": "Po włączeniu, Roo może używać wielu narzędzi w jednej wiadomości. Po wyłączeniu, Roo musi używać jednego narzędzia na wiadomość. Włączenie tej opcji może przyspieszyć interakcje, ale może powodować nieoczekiwane zachowania z niektórymi modelami."
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/pt-BR/settings.json b/webview-ui/src/i18n/locales/pt-BR/settings.json
index 2e39982d..53fd9d43 100644
--- a/webview-ui/src/i18n/locales/pt-BR/settings.json
+++ b/webview-ui/src/i18n/locales/pt-BR/settings.json
@@ -678,6 +678,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "Edição em segundo plano",
 			"description": "Previne a interrupção do foco do editor quando habilitado. As edições de arquivos acontecem em segundo plano sem abrir visualizações de diferenças ou roubar o foco. Você pode continuar trabalhando sem interrupções enquanto o Roo faz alterações. Os arquivos podem ser abertos sem foco para capturar diagnósticos ou permanecer completamente fechados."
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "Permitir múltiplas chamadas de ferramentas por mensagem",
+			"description": "Quando habilitado, o Roo pode usar múltiplas ferramentas em uma única mensagem. Quando desabilitado, o Roo deve usar uma ferramenta por mensagem. Habilitar isso pode tornar as interações mais rápidas, mas pode causar comportamentos inesperados com alguns modelos."
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/ru/settings.json b/webview-ui/src/i18n/locales/ru/settings.json
index 76c3877e..2c4a55c9 100644
--- a/webview-ui/src/i18n/locales/ru/settings.json
+++ b/webview-ui/src/i18n/locales/ru/settings.json
@@ -678,6 +678,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "Фоновое редактирование",
 			"description": "Предотвращает нарушение фокуса редактора при включении. Редактирование файлов происходит в фоновом режиме без открытия представлений различий или кражи фокуса. Вы можете продолжать работать без перерывов, пока Roo вносит изменения. Файлы могут открываться без фокуса для захвата диагностики или оставаться полностью закрытыми."
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "Разрешить несколько вызовов инструментов в одном сообщении",
+			"description": "Если включено, Roo может использовать несколько инструментов в одном сообщении. Если отключено, Roo должен использовать один инструмент в одном сообщении. Включение этой функции может ускорить взаимодействие, но может вызвать непредвиденное поведение с некоторыми моделями."
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/tr/settings.json b/webview-ui/src/i18n/locales/tr/settings.json
index 26a295ff..d185d163 100644
--- a/webview-ui/src/i18n/locales/tr/settings.json
+++ b/webview-ui/src/i18n/locales/tr/settings.json
@@ -678,6 +678,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "Arka plan düzenleme",
 			"description": "Etkinleştirildiğinde editör odak kesintisini önler. Dosya düzenlemeleri diff görünümlerini açmadan veya odağı çalmadan arka planda gerçekleşir. Roo değişiklikler yaparken kesintisiz çalışmaya devam edebilirsiniz. Dosyalar tanılamayı yakalamak için odaksız açılabilir veya tamamen kapalı kalabilir."
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "Mesaj başına birden fazla araç çağrısına izin ver",
+			"description": "Etkinleştirildiğinde, Roo tek bir mesajda birden fazla aracı kullanabilir. Devre dışı bırakıldığında, Roo mesaj başına bir araç kullanmak zorundadır. Bunun etkinleştirilmesi etkileşimleri hızlandırabilir, ancak bazı modellerde beklenmeyen davranışlara neden olabilir."
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/vi/settings.json b/webview-ui/src/i18n/locales/vi/settings.json
index 48f63bdf..557383e6 100644
--- a/webview-ui/src/i18n/locales/vi/settings.json
+++ b/webview-ui/src/i18n/locales/vi/settings.json
@@ -678,6 +678,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "Chỉnh sửa nền",
 			"description": "Khi được bật, ngăn chặn gián đoạn tiêu điểm trình soạn thảo. Việc chỉnh sửa tệp diễn ra ở nền mà không mở chế độ xem diff hoặc chiếm tiêu điểm. Bạn có thể tiếp tục làm việc không bị gián đoạn trong khi Roo thực hiện thay đổi. Các tệp có thể được mở mà không có tiêu điểm để thu thập chẩn đoán hoặc giữ hoàn toàn đóng."
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "Cho phép nhiều lần gọi công cụ trên mỗi tin nhắn",
+			"description": "Khi được bật, Roo có thể sử dụng nhiều công cụ trong một tin nhắn duy nhất. Khi bị tắt, Roo phải sử dụng một công cụ trên mỗi tin nhắn. Bật tính năng này có thể làm cho các tương tác nhanh hơn nhưng có thể gây ra hành vi không mong muốn với một số mô hình."
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/zh-CN/chat.json b/webview-ui/src/i18n/locales/zh-CN/chat.json
index dc21acee..c3d91009 100644
--- a/webview-ui/src/i18n/locales/zh-CN/chat.json
+++ b/webview-ui/src/i18n/locales/zh-CN/chat.json
@@ -108,7 +108,7 @@
 	"stopTts": "停止文本转语音",
 	"typeMessage": "输入消息...",
 	"typeTask": "在此处输入您的任务...",
-	"addContext": "@添加上下文，/输入命令",
+	"addContext": "@添加上下文, /输入命令, !执行控制台命令, #持久化记忆",
 	"dragFiles": "Shift+拖拽文件",
 	"dragFilesImages": "Shift+拖拽文件/图片",
 	"enhancePromptDescription": "'增强提示'按钮通过提供额外上下文、澄清或重新表述来帮助改进您的请求。尝试在此处输入请求，然后再次点击按钮查看其工作原理。",
@@ -267,7 +267,7 @@
 		"whatsNew": "新特性",
 		"feature1": "<bold>消息队列</bold>: 在 Roo 工作时将多个消息排队，让你可以不间断地继续规划工作流程。",
 		"feature2": "<bold>自定义斜杠命令</bold>: 创建个性化斜杠命令，快速访问常用提示词和工作流程，具备完整的 UI 管理功能。",
-		"feature3": "<bold>增强的 Gemini 工具</bold>: 新的 URL 上下文和 Google 搜索基础功能为 Gemini 模型提供实时网络信息和增强的研究能力。",
+		"feature3": "<bold>编排器（Orchestrator）上线</bold>: 优化并上线了原版中的Orchestrator模式，相比原版拥有更充分的子智能体上下文，使得它有更强的可用性",
 		"hideButton": "隐藏公告",
 		"detailsDiscussLinks": "在 <discordLink>Discord</discordLink> 和 <redditLink>Reddit</redditLink> 获取更多详情并参与讨论 🚀"
 	},
diff --git a/webview-ui/src/i18n/locales/zh-CN/settings.json b/webview-ui/src/i18n/locales/zh-CN/settings.json
index 6d46e337..85e17bed 100644
--- a/webview-ui/src/i18n/locales/zh-CN/settings.json
+++ b/webview-ui/src/i18n/locales/zh-CN/settings.json
@@ -678,6 +678,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "后台编辑",
 			"description": "启用后防止编辑器焦点干扰。文件编辑在后台进行，不会打开差异视图或抢夺焦点。你可以在 Roo 进行更改时继续不受干扰地工作。文件可以在不获取焦点的情况下打开以捕获诊断信息，或保持完全关闭状态。"
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "允许每条消息调用多个工具",
+			"description": "启用后，Roo 可以在单条消息中使用多个工具。禁用时，Roo 必须每条消息使用一个工具。启用此功能可以加快交互速度，但可能导致某些模型出现意外行为。"
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/i18n/locales/zh-TW/settings.json b/webview-ui/src/i18n/locales/zh-TW/settings.json
index ffd85239..06e002c0 100644
--- a/webview-ui/src/i18n/locales/zh-TW/settings.json
+++ b/webview-ui/src/i18n/locales/zh-TW/settings.json
@@ -678,6 +678,10 @@
 		"PREVENT_FOCUS_DISRUPTION": {
 			"name": "背景編輯",
 			"description": "啟用後可防止編輯器焦點中斷。檔案編輯會在背景進行，不會開啟 diff 檢視或搶奪焦點。您可以在 Roo 進行變更時繼續不受干擾地工作。檔案可能會在不獲得焦點的情況下開啟以捕獲診斷，或保持完全關閉。"
+		},
+		"ALLOWED_MULTI_CALL": {
+			"name": "允許每則訊息呼叫多個工具",
+			"description": "啟用後，Roo 可以在單則訊息中使用多個工具。停用時，Roo 必須每則訊息使用一個工具。啟用此功能可以加快互動速度，但可能導致某些模型出現非預期行為。"
 		}
 	},
 	"promptCaching": {
diff --git a/webview-ui/src/index.css b/webview-ui/src/index.css
index fbb362ca..050c8bcf 100644
--- a/webview-ui/src/index.css
+++ b/webview-ui/src/index.css
@@ -386,6 +386,14 @@ vscode-dropdown::part(listbox) {
 	box-shadow: 0 0 0 0.5px color-mix(in srgb, var(--vscode-badge-foreground) 30%, transparent);
 }
 
+/* Execute command highlight - distinct style for ! commands */
+.execute-command-textarea-highlight {
+	background-color: color-mix(in srgb, var(--vscode-charts-red, #ff6b6b) 25%, transparent);
+	border-radius: 2px;
+	box-shadow: 0 0 0 1px color-mix(in srgb, var(--vscode-charts-red, #ff6b6b) 40%, transparent);
+	color: transparent;
+}
+
 /**
  * vscrui Overrides / Hacks
  */
diff --git a/webview-ui/src/utils/context-mentions.ts b/webview-ui/src/utils/context-mentions.ts
index 217373c2..4d54f92f 100644
--- a/webview-ui/src/utils/context-mentions.ts
+++ b/webview-ui/src/utils/context-mentions.ts
@@ -228,6 +228,7 @@ export function getContextMenuOptions(
 			const files = queryItems
 				.filter(
 					(item) =>
+						// item.type === ContextMenuOptionType.OpenedFile,
 						item.type === ContextMenuOptionType.File || item.type === ContextMenuOptionType.OpenedFile,
 				)
 				.map((item) => ({
@@ -249,13 +250,21 @@ export function getContextMenuOptions(
 			return commits.length > 0 ? [workingChanges, ...commits] : [workingChanges]
 		}
 
+		// return [
+		// 	{ type: ContextMenuOptionType.Problems },
+		// 	{ type: ContextMenuOptionType.Terminal },
+		// 	{ type: ContextMenuOptionType.URL },
+		// 	{ type: ContextMenuOptionType.Folder },
+		// 	{ type: ContextMenuOptionType.File },
+		// 	{ type: ContextMenuOptionType.Git },
+		// ]
 		return [
+			// { type: ContextMenuOptionType.URL },
+			{ type: ContextMenuOptionType.Git },
 			{ type: ContextMenuOptionType.Problems },
 			{ type: ContextMenuOptionType.Terminal },
-			{ type: ContextMenuOptionType.URL },
-			{ type: ContextMenuOptionType.Folder },
+			// { type: ContextMenuOptionType.Folder },
 			{ type: ContextMenuOptionType.File },
-			{ type: ContextMenuOptionType.Git },
 		]
 	}
 
