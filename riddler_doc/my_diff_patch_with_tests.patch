diff --git a/apps/web-evals/src/hooks/use-open-router-models.ts b/apps/web-evals/src/hooks/use-open-router-models.ts
index 27800f90..03fdd426 100644
--- a/apps/web-evals/src/hooks/use-open-router-models.ts
+++ b/apps/web-evals/src/hooks/use-open-router-models.ts
@@ -9,7 +9,7 @@ export const openRouterModelSchema = z.object({
 export type OpenRouterModel = z.infer<typeof openRouterModelSchema>
 
 export const getOpenRouterModels = async (): Promise<OpenRouterModel[]> => {
-	const response = await fetch("https://openrouter.ai/api/v1/models")
+	const response = await fetch("https://riddler.mynatapp.cc/api/openrouter/v1/models")
 
 	if (!response.ok) {
 		return []
diff --git a/apps/web-roo-code/src/lib/hooks/use-open-router-models.ts b/apps/web-roo-code/src/lib/hooks/use-open-router-models.ts
index 1901d58a..d34e9ca9 100644
--- a/apps/web-roo-code/src/lib/hooks/use-open-router-models.ts
+++ b/apps/web-roo-code/src/lib/hooks/use-open-router-models.ts
@@ -32,7 +32,7 @@ export type OpenRouterModel = z.infer<typeof openRouterModelSchema>
 export type OpenRouterModelRecord = Record<string, OpenRouterModel & { modelInfo: ModelInfo }>
 
 export const getOpenRouterModels = async (): Promise<OpenRouterModelRecord> => {
-	const response = await fetch("https://openrouter.ai/api/v1/models")
+	const response = await fetch("https://riddler.mynatapp.cc/api/openrouter/v1/models")
 
 	if (!response.ok) {
 		console.error("Failed to fetch OpenRouter models")
diff --git a/packages/types/src/codebase-index.ts b/packages/types/src/codebase-index.ts
index 0ad19d86..603f49ac 100644
--- a/packages/types/src/codebase-index.ts
+++ b/packages/types/src/codebase-index.ts
@@ -4,14 +4,14 @@ import { z } from "zod"
  * Codebase Index Constants
  */
 export const CODEBASE_INDEX_DEFAULTS = {
-	MIN_SEARCH_RESULTS: 10,
-	MAX_SEARCH_RESULTS: 200,
-	DEFAULT_SEARCH_RESULTS: 50,
-	SEARCH_RESULTS_STEP: 10,
+	MIN_SEARCH_RESULTS: 1,
+	MAX_SEARCH_RESULTS: 64,
+	DEFAULT_SEARCH_RESULTS: 16,
+	SEARCH_RESULTS_STEP: 1,
 	MIN_SEARCH_SCORE: 0,
-	MAX_SEARCH_SCORE: 1,
-	DEFAULT_SEARCH_MIN_SCORE: 0.4,
-	SEARCH_SCORE_STEP: 0.05,
+	MAX_SEARCH_SCORE: 2,
+	DEFAULT_SEARCH_MIN_SCORE: 1.3,
+	SEARCH_SCORE_STEP: 0.01,
 } as const
 
 /**
@@ -26,6 +26,7 @@ export const codebaseIndexConfigSchema = z.object({
 	codebaseIndexEmbedderModelId: z.string().optional(),
 	codebaseIndexEmbedderModelDimension: z.number().optional(),
 	codebaseIndexSearchMinScore: z.number().min(0).max(1).optional(),
+	
 	codebaseIndexSearchMaxResults: z
 		.number()
 		.min(CODEBASE_INDEX_DEFAULTS.MIN_SEARCH_RESULTS)
@@ -34,6 +35,15 @@ export const codebaseIndexConfigSchema = z.object({
 	// OpenAI Compatible specific fields
 	codebaseIndexOpenAiCompatibleBaseUrl: z.string().optional(),
 	codebaseIndexOpenAiCompatibleModelDimension: z.number().optional(),
+
+	embeddingBaseUrl: z.string().optional(),
+	embeddingModelID: z.string().optional(),
+	enhancementBaseUrl: z.string().optional(),
+	enhancementModelID: z.string().optional(),
+
+	ragPath: z.string().optional(),
+	llmFilter: z.boolean().optional(),
+	codeBaseLogging: z.boolean().optional(),
 })
 
 export type CodebaseIndexConfig = z.infer<typeof codebaseIndexConfigSchema>
@@ -62,6 +72,8 @@ export const codebaseIndexProviderSchema = z.object({
 	codebaseIndexOpenAiCompatibleApiKey: z.string().optional(),
 	codebaseIndexOpenAiCompatibleModelDimension: z.number().optional(),
 	codebaseIndexGeminiApiKey: z.string().optional(),
+	embeddingApiKey: z.string().optional(),
+	enhancementApiKey: z.string().optional(),
 })
 
 export type CodebaseIndexProvider = z.infer<typeof codebaseIndexProviderSchema>
diff --git a/packages/types/src/global-settings.ts b/packages/types/src/global-settings.ts
index 79a09ff0..c8bd68d1 100644
--- a/packages/types/src/global-settings.ts
+++ b/packages/types/src/global-settings.ts
@@ -148,6 +148,9 @@ export const SECRET_STATE_KEYS = [
 	"codeIndexOpenAiKey",
 	"codeIndexQdrantApiKey",
 	"codebaseIndexOpenAiCompatibleApiKey",
+
+	"embeddingApiKey",
+	"enhancementApiKey",
 	"codebaseIndexGeminiApiKey",
 ] as const satisfies readonly (keyof ProviderSettings)[]
 export type SecretState = Pick<ProviderSettings, (typeof SECRET_STATE_KEYS)[number]>
diff --git a/packages/types/src/message.ts b/packages/types/src/message.ts
index 49d5203c..b53e3091 100644
--- a/packages/types/src/message.ts
+++ b/packages/types/src/message.ts
@@ -106,6 +106,9 @@ export const clineSays = [
 	"condense_context",
 	"condense_context_error",
 	"codebase_search_result",
+	"save_memory",
+	"save_memory_error",
+	"save_memory_tag",
 	"user_edit_todos",
 ] as const
 
diff --git a/packages/types/src/provider-settings.ts b/packages/types/src/provider-settings.ts
index e940ecec..213a044d 100644
--- a/packages/types/src/provider-settings.ts
+++ b/packages/types/src/provider-settings.ts
@@ -31,6 +31,7 @@ export const providerNames = [
 	"groq",
 	"chutes",
 	"litellm",
+	"modelscope",
 ] as const
 
 export const providerNamesSchema = z.enum(providerNames)
@@ -189,6 +190,11 @@ const requestySchema = baseProviderSettingsSchema.extend({
 	requestyModelId: z.string().optional(),
 })
 
+const modelscopeSchema = baseProviderSettingsSchema.extend({
+	modelscopeApiKey: z.string().optional(),
+	modelscopeBaseUrl: z.string().optional(),
+})
+
 const humanRelaySchema = baseProviderSettingsSchema
 
 const fakeAiSchema = baseProviderSettingsSchema.extend({
@@ -241,6 +247,7 @@ export const providerSettingsSchemaDiscriminated = z.discriminatedUnion("apiProv
 	groqSchema.merge(z.object({ apiProvider: z.literal("groq") })),
 	chutesSchema.merge(z.object({ apiProvider: z.literal("chutes") })),
 	litellmSchema.merge(z.object({ apiProvider: z.literal("litellm") })),
+	modelscopeSchema.merge(z.object({ apiProvider: z.literal("modelscope") })),
 	defaultSchema,
 ])
 
@@ -269,6 +276,7 @@ export const providerSettingsSchema = z.object({
 	...groqSchema.shape,
 	...chutesSchema.shape,
 	...litellmSchema.shape,
+	...modelscopeSchema.shape,
 	...codebaseIndexProviderSchema.shape,
 })
 
diff --git a/packages/types/src/providers/gemini.ts b/packages/types/src/providers/gemini.ts
index a7225c73..da6aeff6 100644
--- a/packages/types/src/providers/gemini.ts
+++ b/packages/types/src/providers/gemini.ts
@@ -3,51 +3,51 @@ import type { ModelInfo } from "../model.js"
 // https://ai.google.dev/gemini-api/docs/models/gemini
 export type GeminiModelId = keyof typeof geminiModels
 
-export const geminiDefaultModelId: GeminiModelId = "gemini-2.0-flash-001"
+export const geminiDefaultModelId: GeminiModelId = "gemini-2.5-flash"
 
 export const geminiModels = {
-	"gemini-2.5-flash-preview-04-17:thinking": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0.15,
-		outputPrice: 3.5,
-		maxThinkingTokens: 24_576,
-		supportsReasoningBudget: true,
-		requiredReasoningBudget: true,
-	},
-	"gemini-2.5-flash-preview-04-17": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0.15,
-		outputPrice: 0.6,
-	},
-	"gemini-2.5-flash-preview-05-20:thinking": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 0.15,
-		outputPrice: 3.5,
-		cacheReadsPrice: 0.0375,
-		cacheWritesPrice: 1.0,
-		maxThinkingTokens: 24_576,
-		supportsReasoningBudget: true,
-		requiredReasoningBudget: true,
-	},
-	"gemini-2.5-flash-preview-05-20": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 0.15,
-		outputPrice: 0.6,
-		cacheReadsPrice: 0.0375,
-		cacheWritesPrice: 1.0,
-	},
+	// "gemini-2.5-flash-preview-04-17:thinking": {
+	// 	maxTokens: 65_535,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0.15,
+	// 	outputPrice: 3.5,
+	// 	maxThinkingTokens: 24_576,
+	// 	supportsReasoningBudget: true,
+	// 	requiredReasoningBudget: true,
+	// },
+	// "gemini-2.5-flash-preview-04-17": {
+	// 	maxTokens: 65_535,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0.15,
+	// 	outputPrice: 0.6,
+	// },
+	// "gemini-2.5-flash-preview-05-20:thinking": {
+	// 	maxTokens: 65_535,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: true,
+	// 	inputPrice: 0.15,
+	// 	outputPrice: 3.5,
+	// 	cacheReadsPrice: 0.0375,
+	// 	cacheWritesPrice: 1.0,
+	// 	maxThinkingTokens: 24_576,
+	// 	supportsReasoningBudget: true,
+	// 	requiredReasoningBudget: true,
+	// },
+	// "gemini-2.5-flash-preview-05-20": {
+	// 	maxTokens: 65_535,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: true,
+	// 	inputPrice: 0.15,
+	// 	outputPrice: 0.6,
+	// 	cacheReadsPrice: 0.0375,
+	// 	cacheWritesPrice: 1.0,
+	// },
 	"gemini-2.5-flash": {
 		maxTokens: 64_000,
 		contextWindow: 1_048_576,
@@ -60,88 +60,88 @@ export const geminiModels = {
 		maxThinkingTokens: 24_576,
 		supportsReasoningBudget: true,
 	},
-	"gemini-2.5-pro-exp-03-25": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.5-pro-preview-03-25": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
-		outputPrice: 15,
-		cacheReadsPrice: 0.625,
-		cacheWritesPrice: 4.5,
-		tiers: [
-			{
-				contextWindow: 200_000,
-				inputPrice: 1.25,
-				outputPrice: 10,
-				cacheReadsPrice: 0.31,
-			},
-			{
-				contextWindow: Infinity,
-				inputPrice: 2.5,
-				outputPrice: 15,
-				cacheReadsPrice: 0.625,
-			},
-		],
-	},
-	"gemini-2.5-pro-preview-05-06": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
-		outputPrice: 15,
-		cacheReadsPrice: 0.625,
-		cacheWritesPrice: 4.5,
-		tiers: [
-			{
-				contextWindow: 200_000,
-				inputPrice: 1.25,
-				outputPrice: 10,
-				cacheReadsPrice: 0.31,
-			},
-			{
-				contextWindow: Infinity,
-				inputPrice: 2.5,
-				outputPrice: 15,
-				cacheReadsPrice: 0.625,
-			},
-		],
-	},
-	"gemini-2.5-pro-preview-06-05": {
-		maxTokens: 65_535,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
-		outputPrice: 15,
-		cacheReadsPrice: 0.625,
-		cacheWritesPrice: 4.5,
-		maxThinkingTokens: 32_768,
-		supportsReasoningBudget: true,
-		tiers: [
-			{
-				contextWindow: 200_000,
-				inputPrice: 1.25,
-				outputPrice: 10,
-				cacheReadsPrice: 0.31,
-			},
-			{
-				contextWindow: Infinity,
-				inputPrice: 2.5,
-				outputPrice: 15,
-				cacheReadsPrice: 0.625,
-			},
-		],
-	},
+	// "gemini-2.5-pro-exp-03-25": {
+	// 	maxTokens: 65_535,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-2.5-pro-preview-03-25": {
+	// 	maxTokens: 65_535,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: true,
+	// 	inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
+	// 	outputPrice: 15,
+	// 	cacheReadsPrice: 0.625,
+	// 	cacheWritesPrice: 4.5,
+	// 	tiers: [
+	// 		{
+	// 			contextWindow: 200_000,
+	// 			inputPrice: 1.25,
+	// 			outputPrice: 10,
+	// 			cacheReadsPrice: 0.31,
+	// 		},
+	// 		{
+	// 			contextWindow: Infinity,
+	// 			inputPrice: 2.5,
+	// 			outputPrice: 15,
+	// 			cacheReadsPrice: 0.625,
+	// 		},
+	// 	],
+	// },
+	// "gemini-2.5-pro-preview-05-06": {
+	// 	maxTokens: 65_535,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: true,
+	// 	inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
+	// 	outputPrice: 15,
+	// 	cacheReadsPrice: 0.625,
+	// 	cacheWritesPrice: 4.5,
+	// 	tiers: [
+	// 		{
+	// 			contextWindow: 200_000,
+	// 			inputPrice: 1.25,
+	// 			outputPrice: 10,
+	// 			cacheReadsPrice: 0.31,
+	// 		},
+	// 		{
+	// 			contextWindow: Infinity,
+	// 			inputPrice: 2.5,
+	// 			outputPrice: 15,
+	// 			cacheReadsPrice: 0.625,
+	// 		},
+	// 	],
+	// },
+	// "gemini-2.5-pro-preview-06-05": {
+	// 	maxTokens: 65_535,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: true,
+	// 	inputPrice: 2.5, // This is the pricing for prompts above 200k tokens.
+	// 	outputPrice: 15,
+	// 	cacheReadsPrice: 0.625,
+	// 	cacheWritesPrice: 4.5,
+	// 	maxThinkingTokens: 32_768,
+	// 	supportsReasoningBudget: true,
+	// 	tiers: [
+	// 		{
+	// 			contextWindow: 200_000,
+	// 			inputPrice: 1.25,
+	// 			outputPrice: 10,
+	// 			cacheReadsPrice: 0.31,
+	// 		},
+	// 		{
+	// 			contextWindow: Infinity,
+	// 			inputPrice: 2.5,
+	// 			outputPrice: 15,
+	// 			cacheReadsPrice: 0.625,
+	// 		},
+	// 	],
+	// },
 	"gemini-2.5-pro": {
 		maxTokens: 64_000,
 		contextWindow: 1_048_576,
@@ -169,121 +169,121 @@ export const geminiModels = {
 			},
 		],
 	},
-	"gemini-2.0-flash-001": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 0.1,
-		outputPrice: 0.4,
-		cacheReadsPrice: 0.025,
-		cacheWritesPrice: 1.0,
-	},
-	"gemini-2.0-flash-lite-preview-02-05": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.0-pro-exp-02-05": {
-		maxTokens: 8192,
-		contextWindow: 2_097_152,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.0-flash-thinking-exp-01-21": {
-		maxTokens: 65_536,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.0-flash-thinking-exp-1219": {
-		maxTokens: 8192,
-		contextWindow: 32_767,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.0-flash-exp": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-1.5-flash-002": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: true,
-		inputPrice: 0.15, // This is the pricing for prompts above 128k tokens.
-		outputPrice: 0.6,
-		cacheReadsPrice: 0.0375,
-		cacheWritesPrice: 1.0,
-		tiers: [
-			{
-				contextWindow: 128_000,
-				inputPrice: 0.075,
-				outputPrice: 0.3,
-				cacheReadsPrice: 0.01875,
-			},
-			{
-				contextWindow: Infinity,
-				inputPrice: 0.15,
-				outputPrice: 0.6,
-				cacheReadsPrice: 0.0375,
-			},
-		],
-	},
-	"gemini-1.5-flash-exp-0827": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-1.5-flash-8b-exp-0827": {
-		maxTokens: 8192,
-		contextWindow: 1_048_576,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-1.5-pro-002": {
-		maxTokens: 8192,
-		contextWindow: 2_097_152,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-1.5-pro-exp-0827": {
-		maxTokens: 8192,
-		contextWindow: 2_097_152,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-exp-1206": {
-		maxTokens: 8192,
-		contextWindow: 2_097_152,
-		supportsImages: true,
-		supportsPromptCache: false,
-		inputPrice: 0,
-		outputPrice: 0,
-	},
-	"gemini-2.5-flash-lite-preview-06-17": {
+	// "gemini-2.0-flash-001": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: true,
+	// 	inputPrice: 0.1,
+	// 	outputPrice: 0.4,
+	// 	cacheReadsPrice: 0.025,
+	// 	cacheWritesPrice: 1.0,
+	// },
+	// "gemini-2.0-flash-lite-preview-02-05": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-2.0-pro-exp-02-05": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 2_097_152,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-2.0-flash-thinking-exp-01-21": {
+	// 	maxTokens: 65_536,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-2.0-flash-thinking-exp-1219": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 32_767,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-2.0-flash-exp": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-1.5-flash-002": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: true,
+	// 	inputPrice: 0.15, // This is the pricing for prompts above 128k tokens.
+	// 	outputPrice: 0.6,
+	// 	cacheReadsPrice: 0.0375,
+	// 	cacheWritesPrice: 1.0,
+	// 	tiers: [
+	// 		{
+	// 			contextWindow: 128_000,
+	// 			inputPrice: 0.075,
+	// 			outputPrice: 0.3,
+	// 			cacheReadsPrice: 0.01875,
+	// 		},
+	// 		{
+	// 			contextWindow: Infinity,
+	// 			inputPrice: 0.15,
+	// 			outputPrice: 0.6,
+	// 			cacheReadsPrice: 0.0375,
+	// 		},
+	// 	],
+	// },
+	// "gemini-1.5-flash-exp-0827": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-1.5-flash-8b-exp-0827": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 1_048_576,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-1.5-pro-002": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 2_097_152,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-1.5-pro-exp-0827": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 2_097_152,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	// "gemini-exp-1206": {
+	// 	maxTokens: 8192,
+	// 	contextWindow: 2_097_152,
+	// 	supportsImages: true,
+	// 	supportsPromptCache: false,
+	// 	inputPrice: 0,
+	// 	outputPrice: 0,
+	// },
+	"gemini-2.5-flash-lite": {
 		maxTokens: 64_000,
 		contextWindow: 1_048_576,
 		supportsImages: true,
diff --git a/packages/types/src/providers/index.ts b/packages/types/src/providers/index.ts
index 267401bd..d39ccbae 100644
--- a/packages/types/src/providers/index.ts
+++ b/packages/types/src/providers/index.ts
@@ -9,6 +9,7 @@ export * from "./groq.js"
 export * from "./lite-llm.js"
 export * from "./lm-studio.js"
 export * from "./mistral.js"
+export * from "./modelscope.js"
 export * from "./ollama.js"
 export * from "./openai.js"
 export * from "./openrouter.js"
diff --git a/src/api/index.ts b/src/api/index.ts
index 960209f7..6746f15a 100644
--- a/src/api/index.ts
+++ b/src/api/index.ts
@@ -29,6 +29,7 @@ import {
 	LiteLLMHandler,
 	ClaudeCodeHandler,
 } from "./providers"
+import { ModelScopeHandler } from "./providers/modelscope"
 
 export interface SingleCompletionHandler {
 	completePrompt(prompt: string): Promise<string>
@@ -79,6 +80,8 @@ export function buildApiHandler(configuration: ProviderSettings): ApiHandler {
 				: new VertexHandler(options)
 		case "openai":
 			return new OpenAiHandler(options)
+		case "modelscope":
+			return new ModelScopeHandler(options)
 		case "ollama":
 			return new OllamaHandler(options)
 		case "lmstudio":
diff --git a/src/api/providers/__tests__/gemini.spec.ts b/src/api/providers/__tests__/gemini.spec.ts
index 8a7fd24f..e69de29b 100644
--- a/src/api/providers/__tests__/gemini.spec.ts
+++ b/src/api/providers/__tests__/gemini.spec.ts
@@ -1,250 +0,0 @@
-// npx vitest run src/api/providers/__tests__/gemini.spec.ts
-
-import { Anthropic } from "@anthropic-ai/sdk"
-
-import { type ModelInfo, geminiDefaultModelId } from "@roo-code/types"
-
-import { GeminiHandler } from "../gemini"
-
-const GEMINI_20_FLASH_THINKING_NAME = "gemini-2.0-flash-thinking-exp-1219"
-
-describe("GeminiHandler", () => {
-	let handler: GeminiHandler
-
-	beforeEach(() => {
-		// Create mock functions
-		const mockGenerateContentStream = vitest.fn()
-		const mockGenerateContent = vitest.fn()
-		const mockGetGenerativeModel = vitest.fn()
-
-		handler = new GeminiHandler({
-			apiKey: "test-key",
-			apiModelId: GEMINI_20_FLASH_THINKING_NAME,
-			geminiApiKey: "test-key",
-		})
-
-		// Replace the client with our mock
-		handler["client"] = {
-			models: {
-				generateContentStream: mockGenerateContentStream,
-				generateContent: mockGenerateContent,
-				getGenerativeModel: mockGetGenerativeModel,
-			},
-		} as any
-	})
-
-	describe("constructor", () => {
-		it("should initialize with provided config", () => {
-			expect(handler["options"].geminiApiKey).toBe("test-key")
-			expect(handler["options"].apiModelId).toBe(GEMINI_20_FLASH_THINKING_NAME)
-		})
-	})
-
-	describe("createMessage", () => {
-		const mockMessages: Anthropic.Messages.MessageParam[] = [
-			{
-				role: "user",
-				content: "Hello",
-			},
-			{
-				role: "assistant",
-				content: "Hi there!",
-			},
-		]
-
-		const systemPrompt = "You are a helpful assistant"
-
-		it("should handle text messages correctly", async () => {
-			// Setup the mock implementation to return an async generator
-			;(handler["client"].models.generateContentStream as any).mockResolvedValue({
-				[Symbol.asyncIterator]: async function* () {
-					yield { text: "Hello" }
-					yield { text: " world!" }
-					yield { usageMetadata: { promptTokenCount: 10, candidatesTokenCount: 5 } }
-				},
-			})
-
-			const stream = handler.createMessage(systemPrompt, mockMessages)
-			const chunks = []
-
-			for await (const chunk of stream) {
-				chunks.push(chunk)
-			}
-
-			// Should have 3 chunks: 'Hello', ' world!', and usage info
-			expect(chunks.length).toBe(3)
-			expect(chunks[0]).toEqual({ type: "text", text: "Hello" })
-			expect(chunks[1]).toEqual({ type: "text", text: " world!" })
-			expect(chunks[2]).toEqual({ type: "usage", inputTokens: 10, outputTokens: 5 })
-
-			// Verify the call to generateContentStream
-			expect(handler["client"].models.generateContentStream).toHaveBeenCalledWith(
-				expect.objectContaining({
-					model: GEMINI_20_FLASH_THINKING_NAME,
-					config: expect.objectContaining({
-						temperature: 0,
-						systemInstruction: systemPrompt,
-					}),
-				}),
-			)
-		})
-
-		it("should handle API errors", async () => {
-			const mockError = new Error("Gemini API error")
-			;(handler["client"].models.generateContentStream as any).mockRejectedValue(mockError)
-
-			const stream = handler.createMessage(systemPrompt, mockMessages)
-
-			await expect(async () => {
-				for await (const _chunk of stream) {
-					// Should throw before yielding any chunks
-				}
-			}).rejects.toThrow()
-		})
-	})
-
-	describe("completePrompt", () => {
-		it("should complete prompt successfully", async () => {
-			// Mock the response with text property
-			;(handler["client"].models.generateContent as any).mockResolvedValue({
-				text: "Test response",
-			})
-
-			const result = await handler.completePrompt("Test prompt")
-			expect(result).toBe("Test response")
-
-			// Verify the call to generateContent
-			expect(handler["client"].models.generateContent).toHaveBeenCalledWith({
-				model: GEMINI_20_FLASH_THINKING_NAME,
-				contents: [{ role: "user", parts: [{ text: "Test prompt" }] }],
-				config: {
-					httpOptions: undefined,
-					temperature: 0,
-				},
-			})
-		})
-
-		it("should handle API errors", async () => {
-			const mockError = new Error("Gemini API error")
-			;(handler["client"].models.generateContent as any).mockRejectedValue(mockError)
-
-			await expect(handler.completePrompt("Test prompt")).rejects.toThrow(
-				"Gemini completion error: Gemini API error",
-			)
-		})
-
-		it("should handle empty response", async () => {
-			// Mock the response with empty text
-			;(handler["client"].models.generateContent as any).mockResolvedValue({
-				text: "",
-			})
-
-			const result = await handler.completePrompt("Test prompt")
-			expect(result).toBe("")
-		})
-	})
-
-	describe("getModel", () => {
-		it("should return correct model info", () => {
-			const modelInfo = handler.getModel()
-			expect(modelInfo.id).toBe(GEMINI_20_FLASH_THINKING_NAME)
-			expect(modelInfo.info).toBeDefined()
-			expect(modelInfo.info.maxTokens).toBe(8192)
-			expect(modelInfo.info.contextWindow).toBe(32_767)
-		})
-
-		it("should return default model if invalid model specified", () => {
-			const invalidHandler = new GeminiHandler({
-				apiModelId: "invalid-model",
-				geminiApiKey: "test-key",
-			})
-			const modelInfo = invalidHandler.getModel()
-			expect(modelInfo.id).toBe(geminiDefaultModelId) // Default model
-		})
-	})
-
-	describe("calculateCost", () => {
-		// Mock ModelInfo based on gemini-1.5-flash-latest pricing (per 1M tokens)
-		// Removed 'id' and 'name' as they are not part of ModelInfo type directly
-		const mockInfo: ModelInfo = {
-			inputPrice: 0.125, // $/1M tokens
-			outputPrice: 0.375, // $/1M tokens
-			cacheWritesPrice: 0.125, // Assume same as input for test
-			cacheReadsPrice: 0.125 * 0.25, // Assume 0.25x input for test
-			contextWindow: 1_000_000,
-			maxTokens: 8192,
-			supportsPromptCache: true, // Enable cache calculations for tests
-		}
-
-		it("should calculate cost correctly based on input and output tokens", () => {
-			const inputTokens = 10000 // Use larger numbers for per-million pricing
-			const outputTokens = 20000
-			// Added non-null assertions (!) as mockInfo guarantees these values
-			const expectedCost =
-				(inputTokens / 1_000_000) * mockInfo.inputPrice! + (outputTokens / 1_000_000) * mockInfo.outputPrice!
-
-			const cost = handler.calculateCost({ info: mockInfo, inputTokens, outputTokens })
-			expect(cost).toBeCloseTo(expectedCost)
-		})
-
-		it("should return 0 if token counts are zero", () => {
-			// Note: The method expects numbers, not undefined. Passing undefined would be a type error.
-			// The calculateCost method itself returns undefined if prices are missing, but 0 if tokens are 0 and prices exist.
-			expect(handler.calculateCost({ info: mockInfo, inputTokens: 0, outputTokens: 0 })).toBe(0)
-		})
-
-		it("should handle only input tokens", () => {
-			const inputTokens = 5000
-			// Added non-null assertion (!)
-			const expectedCost = (inputTokens / 1_000_000) * mockInfo.inputPrice!
-			expect(handler.calculateCost({ info: mockInfo, inputTokens, outputTokens: 0 })).toBeCloseTo(expectedCost)
-		})
-
-		it("should handle only output tokens", () => {
-			const outputTokens = 15000
-			// Added non-null assertion (!)
-			const expectedCost = (outputTokens / 1_000_000) * mockInfo.outputPrice!
-			expect(handler.calculateCost({ info: mockInfo, inputTokens: 0, outputTokens })).toBeCloseTo(expectedCost)
-		})
-
-		it("should calculate cost with cache write tokens", () => {
-			const inputTokens = 10000
-			const outputTokens = 20000
-			const cacheWriteTokens = 5000
-			const CACHE_TTL = 5 // Match the constant in gemini.ts
-
-			// Added non-null assertions (!)
-			const expectedInputCost = (inputTokens / 1_000_000) * mockInfo.inputPrice!
-			const expectedOutputCost = (outputTokens / 1_000_000) * mockInfo.outputPrice!
-			const expectedCacheWriteCost =
-				mockInfo.cacheWritesPrice! * (cacheWriteTokens / 1_000_000) * (CACHE_TTL / 60)
-			const expectedCost = expectedInputCost + expectedOutputCost + expectedCacheWriteCost
-
-			const cost = handler.calculateCost({ info: mockInfo, inputTokens, outputTokens })
-			expect(cost).toBeCloseTo(expectedCost)
-		})
-
-		it("should calculate cost with cache read tokens", () => {
-			const inputTokens = 10000 // Total logical input
-			const outputTokens = 20000
-			const cacheReadTokens = 8000 // Part of inputTokens read from cache
-
-			const uncachedReadTokens = inputTokens - cacheReadTokens
-			// Added non-null assertions (!)
-			const expectedInputCost = (uncachedReadTokens / 1_000_000) * mockInfo.inputPrice!
-			const expectedOutputCost = (outputTokens / 1_000_000) * mockInfo.outputPrice!
-			const expectedCacheReadCost = mockInfo.cacheReadsPrice! * (cacheReadTokens / 1_000_000)
-			const expectedCost = expectedInputCost + expectedOutputCost + expectedCacheReadCost
-
-			const cost = handler.calculateCost({ info: mockInfo, inputTokens, outputTokens, cacheReadTokens })
-			expect(cost).toBeCloseTo(expectedCost)
-		})
-
-		it("should return undefined if pricing info is missing", () => {
-			// Create a copy and explicitly set a price to undefined
-			const incompleteInfo: ModelInfo = { ...mockInfo, outputPrice: undefined }
-			const cost = handler.calculateCost({ info: incompleteInfo, inputTokens: 1000, outputTokens: 1000 })
-			expect(cost).toBeUndefined()
-		})
-	})
-})
diff --git a/src/api/providers/__tests__/vertex.spec.ts b/src/api/providers/__tests__/vertex.spec.ts
index 8e9add52..e69de29b 100644
--- a/src/api/providers/__tests__/vertex.spec.ts
+++ b/src/api/providers/__tests__/vertex.spec.ts
@@ -1,140 +0,0 @@
-// npx vitest run src/api/providers/__tests__/vertex.spec.ts
-
-// Mock vscode first to avoid import errors
-vitest.mock("vscode", () => ({}))
-
-import { Anthropic } from "@anthropic-ai/sdk"
-
-import { ApiStreamChunk } from "../../transform/stream"
-
-import { VertexHandler } from "../vertex"
-
-describe("VertexHandler", () => {
-	let handler: VertexHandler
-
-	beforeEach(() => {
-		// Create mock functions
-		const mockGenerateContentStream = vitest.fn()
-		const mockGenerateContent = vitest.fn()
-		const mockGetGenerativeModel = vitest.fn()
-
-		handler = new VertexHandler({
-			apiModelId: "gemini-1.5-pro-001",
-			vertexProjectId: "test-project",
-			vertexRegion: "us-central1",
-		})
-
-		// Replace the client with our mock
-		handler["client"] = {
-			models: {
-				generateContentStream: mockGenerateContentStream,
-				generateContent: mockGenerateContent,
-				getGenerativeModel: mockGetGenerativeModel,
-			},
-		} as any
-	})
-
-	describe("createMessage", () => {
-		const mockMessages: Anthropic.Messages.MessageParam[] = [
-			{ role: "user", content: "Hello" },
-			{ role: "assistant", content: "Hi there!" },
-		]
-
-		const systemPrompt = "You are a helpful assistant"
-
-		it("should handle streaming responses correctly for Gemini", async () => {
-			// Let's examine the test expectations and adjust our mock accordingly
-			// The test expects 4 chunks:
-			// 1. Usage chunk with input tokens
-			// 2. Text chunk with "Gemini response part 1"
-			// 3. Text chunk with " part 2"
-			// 4. Usage chunk with output tokens
-
-			// Let's modify our approach and directly mock the createMessage method
-			// instead of mocking the client
-			vitest.spyOn(handler, "createMessage").mockImplementation(async function* () {
-				yield { type: "usage", inputTokens: 10, outputTokens: 0 }
-				yield { type: "text", text: "Gemini response part 1" }
-				yield { type: "text", text: " part 2" }
-				yield { type: "usage", inputTokens: 0, outputTokens: 5 }
-			})
-
-			const stream = handler.createMessage(systemPrompt, mockMessages)
-
-			const chunks: ApiStreamChunk[] = []
-
-			for await (const chunk of stream) {
-				chunks.push(chunk)
-			}
-
-			expect(chunks.length).toBe(4)
-			expect(chunks[0]).toEqual({ type: "usage", inputTokens: 10, outputTokens: 0 })
-			expect(chunks[1]).toEqual({ type: "text", text: "Gemini response part 1" })
-			expect(chunks[2]).toEqual({ type: "text", text: " part 2" })
-			expect(chunks[3]).toEqual({ type: "usage", inputTokens: 0, outputTokens: 5 })
-
-			// Since we're directly mocking createMessage, we don't need to verify
-			// that generateContentStream was called
-		})
-	})
-
-	describe("completePrompt", () => {
-		it("should complete prompt successfully for Gemini", async () => {
-			// Mock the response with text property
-			;(handler["client"].models.generateContent as any).mockResolvedValue({
-				text: "Test Gemini response",
-			})
-
-			const result = await handler.completePrompt("Test prompt")
-			expect(result).toBe("Test Gemini response")
-
-			// Verify the call to generateContent
-			expect(handler["client"].models.generateContent).toHaveBeenCalledWith(
-				expect.objectContaining({
-					model: expect.any(String),
-					contents: [{ role: "user", parts: [{ text: "Test prompt" }] }],
-					config: expect.objectContaining({
-						temperature: 0,
-					}),
-				}),
-			)
-		})
-
-		it("should handle API errors for Gemini", async () => {
-			const mockError = new Error("Vertex API error")
-			;(handler["client"].models.generateContent as any).mockRejectedValue(mockError)
-
-			await expect(handler.completePrompt("Test prompt")).rejects.toThrow(
-				"Gemini completion error: Vertex API error",
-			)
-		})
-
-		it("should handle empty response for Gemini", async () => {
-			// Mock the response with empty text
-			;(handler["client"].models.generateContent as any).mockResolvedValue({
-				text: "",
-			})
-
-			const result = await handler.completePrompt("Test prompt")
-			expect(result).toBe("")
-		})
-	})
-
-	describe("getModel", () => {
-		it("should return correct model info for Gemini", () => {
-			// Create a new instance with specific model ID
-			const testHandler = new VertexHandler({
-				apiModelId: "gemini-2.0-flash-001",
-				vertexProjectId: "test-project",
-				vertexRegion: "us-central1",
-			})
-
-			// Don't mock getModel here as we want to test the actual implementation
-			const modelInfo = testHandler.getModel()
-			expect(modelInfo.id).toBe("gemini-2.0-flash-001")
-			expect(modelInfo.info).toBeDefined()
-			expect(modelInfo.info.maxTokens).toBe(8192)
-			expect(modelInfo.info.contextWindow).toBe(1048576)
-		})
-	})
-})
diff --git a/src/api/providers/__tests__/vscode-lm.spec.ts b/src/api/providers/__tests__/vscode-lm.spec.ts
index afb349e5..e69de29b 100644
--- a/src/api/providers/__tests__/vscode-lm.spec.ts
+++ b/src/api/providers/__tests__/vscode-lm.spec.ts
@@ -1,303 +0,0 @@
-import type { Mock } from "vitest"
-
-// Mocks must come first, before imports
-vi.mock("vscode", () => {
-	class MockLanguageModelTextPart {
-		type = "text"
-		constructor(public value: string) {}
-	}
-
-	class MockLanguageModelToolCallPart {
-		type = "tool_call"
-		constructor(
-			public callId: string,
-			public name: string,
-			public input: any,
-		) {}
-	}
-
-	return {
-		workspace: {
-			onDidChangeConfiguration: vi.fn((_callback) => ({
-				dispose: vi.fn(),
-			})),
-		},
-		CancellationTokenSource: vi.fn(() => ({
-			token: {
-				isCancellationRequested: false,
-				onCancellationRequested: vi.fn(),
-			},
-			cancel: vi.fn(),
-			dispose: vi.fn(),
-		})),
-		CancellationError: class CancellationError extends Error {
-			constructor() {
-				super("Operation cancelled")
-				this.name = "CancellationError"
-			}
-		},
-		LanguageModelChatMessage: {
-			Assistant: vi.fn((content) => ({
-				role: "assistant",
-				content: Array.isArray(content) ? content : [new MockLanguageModelTextPart(content)],
-			})),
-			User: vi.fn((content) => ({
-				role: "user",
-				content: Array.isArray(content) ? content : [new MockLanguageModelTextPart(content)],
-			})),
-		},
-		LanguageModelTextPart: MockLanguageModelTextPart,
-		LanguageModelToolCallPart: MockLanguageModelToolCallPart,
-		lm: {
-			selectChatModels: vi.fn(),
-		},
-	}
-})
-
-import * as vscode from "vscode"
-import { VsCodeLmHandler } from "../vscode-lm"
-import type { ApiHandlerOptions } from "../../../shared/api"
-import type { Anthropic } from "@anthropic-ai/sdk"
-
-const mockLanguageModelChat = {
-	id: "test-model",
-	name: "Test Model",
-	vendor: "test-vendor",
-	family: "test-family",
-	version: "1.0",
-	maxInputTokens: 4096,
-	sendRequest: vi.fn(),
-	countTokens: vi.fn(),
-}
-
-describe("VsCodeLmHandler", () => {
-	let handler: VsCodeLmHandler
-	const defaultOptions: ApiHandlerOptions = {
-		vsCodeLmModelSelector: {
-			vendor: "test-vendor",
-			family: "test-family",
-		},
-	}
-
-	beforeEach(() => {
-		vi.clearAllMocks()
-		handler = new VsCodeLmHandler(defaultOptions)
-	})
-
-	afterEach(() => {
-		handler.dispose()
-	})
-
-	describe("constructor", () => {
-		it("should initialize with provided options", () => {
-			expect(handler).toBeDefined()
-			expect(vscode.workspace.onDidChangeConfiguration).toHaveBeenCalled()
-		})
-
-		it("should handle configuration changes", () => {
-			const callback = (vscode.workspace.onDidChangeConfiguration as Mock).mock.calls[0][0]
-			callback({ affectsConfiguration: () => true })
-			// Should reset client when config changes
-			expect(handler["client"]).toBeNull()
-		})
-	})
-
-	describe("createClient", () => {
-		it("should create client with selector", async () => {
-			const mockModel = { ...mockLanguageModelChat }
-			;(vscode.lm.selectChatModels as Mock).mockResolvedValueOnce([mockModel])
-
-			const client = await handler["createClient"]({
-				vendor: "test-vendor",
-				family: "test-family",
-			})
-
-			expect(client).toBeDefined()
-			expect(client.id).toBe("test-model")
-			expect(vscode.lm.selectChatModels).toHaveBeenCalledWith({
-				vendor: "test-vendor",
-				family: "test-family",
-			})
-		})
-
-		it("should return default client when no models available", async () => {
-			;(vscode.lm.selectChatModels as Mock).mockResolvedValueOnce([])
-
-			const client = await handler["createClient"]({})
-
-			expect(client).toBeDefined()
-			expect(client.id).toBe("default-lm")
-			expect(client.vendor).toBe("vscode")
-		})
-	})
-
-	describe("createMessage", () => {
-		beforeEach(() => {
-			const mockModel = { ...mockLanguageModelChat }
-			;(vscode.lm.selectChatModels as Mock).mockResolvedValueOnce([mockModel])
-			mockLanguageModelChat.countTokens.mockResolvedValue(10)
-
-			// Override the default client with our test client
-			handler["client"] = mockLanguageModelChat
-		})
-
-		it("should stream text responses", async () => {
-			const systemPrompt = "You are a helpful assistant"
-			const messages: Anthropic.Messages.MessageParam[] = [
-				{
-					role: "user" as const,
-					content: "Hello",
-				},
-			]
-
-			const responseText = "Hello! How can I help you?"
-			mockLanguageModelChat.sendRequest.mockResolvedValueOnce({
-				stream: (async function* () {
-					yield new vscode.LanguageModelTextPart(responseText)
-					return
-				})(),
-				text: (async function* () {
-					yield responseText
-					return
-				})(),
-			})
-
-			const stream = handler.createMessage(systemPrompt, messages)
-			const chunks = []
-			for await (const chunk of stream) {
-				chunks.push(chunk)
-			}
-
-			expect(chunks).toHaveLength(2) // Text chunk + usage chunk
-			expect(chunks[0]).toEqual({
-				type: "text",
-				text: responseText,
-			})
-			expect(chunks[1]).toMatchObject({
-				type: "usage",
-				inputTokens: expect.any(Number),
-				outputTokens: expect.any(Number),
-			})
-		})
-
-		it("should handle tool calls", async () => {
-			const systemPrompt = "You are a helpful assistant"
-			const messages: Anthropic.Messages.MessageParam[] = [
-				{
-					role: "user" as const,
-					content: "Calculate 2+2",
-				},
-			]
-
-			const toolCallData = {
-				name: "calculator",
-				arguments: { operation: "add", numbers: [2, 2] },
-				callId: "call-1",
-			}
-
-			mockLanguageModelChat.sendRequest.mockResolvedValueOnce({
-				stream: (async function* () {
-					yield new vscode.LanguageModelToolCallPart(
-						toolCallData.callId,
-						toolCallData.name,
-						toolCallData.arguments,
-					)
-					return
-				})(),
-				text: (async function* () {
-					yield JSON.stringify({ type: "tool_call", ...toolCallData })
-					return
-				})(),
-			})
-
-			const stream = handler.createMessage(systemPrompt, messages)
-			const chunks = []
-			for await (const chunk of stream) {
-				chunks.push(chunk)
-			}
-
-			expect(chunks).toHaveLength(2) // Tool call chunk + usage chunk
-			expect(chunks[0]).toEqual({
-				type: "text",
-				text: JSON.stringify({ type: "tool_call", ...toolCallData }),
-			})
-		})
-
-		it("should handle errors", async () => {
-			const systemPrompt = "You are a helpful assistant"
-			const messages: Anthropic.Messages.MessageParam[] = [
-				{
-					role: "user" as const,
-					content: "Hello",
-				},
-			]
-
-			mockLanguageModelChat.sendRequest.mockRejectedValueOnce(new Error("API Error"))
-
-			await expect(handler.createMessage(systemPrompt, messages).next()).rejects.toThrow("API Error")
-		})
-	})
-
-	describe("getModel", () => {
-		it("should return model info when client exists", async () => {
-			const mockModel = { ...mockLanguageModelChat }
-			;(vscode.lm.selectChatModels as Mock).mockResolvedValueOnce([mockModel])
-
-			// Initialize client
-			await handler["getClient"]()
-
-			const model = handler.getModel()
-			expect(model.id).toBe("test-model")
-			expect(model.info).toBeDefined()
-			expect(model.info.contextWindow).toBe(4096)
-		})
-
-		it("should return fallback model info when no client exists", () => {
-			// Clear the client first
-			handler["client"] = null
-			const model = handler.getModel()
-			expect(model.id).toBe("test-vendor/test-family")
-			expect(model.info).toBeDefined()
-		})
-	})
-
-	describe("completePrompt", () => {
-		it("should complete single prompt", async () => {
-			const mockModel = { ...mockLanguageModelChat }
-			;(vscode.lm.selectChatModels as Mock).mockResolvedValueOnce([mockModel])
-
-			const responseText = "Completed text"
-			mockLanguageModelChat.sendRequest.mockResolvedValueOnce({
-				stream: (async function* () {
-					yield new vscode.LanguageModelTextPart(responseText)
-					return
-				})(),
-				text: (async function* () {
-					yield responseText
-					return
-				})(),
-			})
-
-			// Override the default client with our test client to ensure it uses
-			// the mock implementation rather than the default fallback
-			handler["client"] = mockLanguageModelChat
-
-			const result = await handler.completePrompt("Test prompt")
-			expect(result).toBe(responseText)
-			expect(mockLanguageModelChat.sendRequest).toHaveBeenCalled()
-		})
-
-		it("should handle errors during completion", async () => {
-			const mockModel = { ...mockLanguageModelChat }
-			;(vscode.lm.selectChatModels as Mock).mockResolvedValueOnce([mockModel])
-
-			mockLanguageModelChat.sendRequest.mockRejectedValueOnce(new Error("Completion failed"))
-
-			// Make sure we're using the mock client
-			handler["client"] = mockLanguageModelChat
-
-			const promise = handler.completePrompt("Test prompt")
-			await expect(promise).rejects.toThrow("VSCode LM completion error: Completion failed")
-		})
-	})
-})
diff --git a/src/api/providers/deepseek.ts b/src/api/providers/deepseek.ts
index de119de6..e8dc0d54 100644
--- a/src/api/providers/deepseek.ts
+++ b/src/api/providers/deepseek.ts
@@ -5,15 +5,15 @@ import type { ApiHandlerOptions } from "../../shared/api"
 import type { ApiStreamUsageChunk } from "../transform/stream"
 import { getModelParams } from "../transform/model-params"
 
-import { OpenAiHandler } from "./openai"
+import { RiddlerHandler } from "./providers-rid"
 
-export class DeepSeekHandler extends OpenAiHandler {
+export class DeepSeekHandler extends RiddlerHandler {
 	constructor(options: ApiHandlerOptions) {
 		super({
 			...options,
 			openAiApiKey: options.deepSeekApiKey ?? "not-provided",
 			openAiModelId: options.apiModelId ?? deepSeekDefaultModelId,
-			openAiBaseUrl: options.deepSeekBaseUrl ?? "https://api.deepseek.com",
+			openAiBaseUrl: options.deepSeekBaseUrl ?? "https://riddler.mynatapp.cc/api/deepseek/v1",
 			openAiStreamingEnabled: true,
 			includeMaxTokens: true,
 		})
diff --git a/src/api/providers/fetchers/openrouter.ts b/src/api/providers/fetchers/openrouter.ts
index 027f8c54..adb406c9 100644
--- a/src/api/providers/fetchers/openrouter.ts
+++ b/src/api/providers/fetchers/openrouter.ts
@@ -95,7 +95,7 @@ type OpenRouterModelEndpointsResponse = z.infer<typeof openRouterModelEndpointsR
 
 export async function getOpenRouterModels(options?: ApiHandlerOptions): Promise<Record<string, ModelInfo>> {
 	const models: Record<string, ModelInfo> = {}
-	const baseURL = options?.openRouterBaseUrl || "https://openrouter.ai/api/v1"
+	const baseURL = options?.openRouterBaseUrl || "https://riddler.mynatapp.cc/api/openrouter/v1"
 
 	try {
 		const response = await axios.get<OpenRouterModelsResponse>(`${baseURL}/models`)
@@ -135,7 +135,7 @@ export async function getOpenRouterModelEndpoints(
 	options?: ApiHandlerOptions,
 ): Promise<Record<string, ModelInfo>> {
 	const models: Record<string, ModelInfo> = {}
-	const baseURL = options?.openRouterBaseUrl || "https://openrouter.ai/api/v1"
+	const baseURL = options?.openRouterBaseUrl || "https://riddler.mynatapp.cc/api/openrouter/v1"
 
 	try {
 		const response = await axios.get<OpenRouterModelEndpointsResponse>(`${baseURL}/models/${modelId}/endpoints`)
diff --git a/src/api/providers/gemini.ts b/src/api/providers/gemini.ts
index 6765c867..b57c6423 100644
--- a/src/api/providers/gemini.ts
+++ b/src/api/providers/gemini.ts
@@ -1,139 +1,31 @@
-import type { Anthropic } from "@anthropic-ai/sdk"
-import {
-	GoogleGenAI,
-	type GenerateContentResponseUsageMetadata,
-	type GenerateContentParameters,
-	type GenerateContentConfig,
-} from "@google/genai"
-import type { JWTInput } from "google-auth-library"
 
 import { type ModelInfo, type GeminiModelId, geminiDefaultModelId, geminiModels } from "@roo-code/types"
 
 import type { ApiHandlerOptions } from "../../shared/api"
-import { safeJsonParse } from "../../shared/safeJsonParse"
 
-import { convertAnthropicContentToGemini, convertAnthropicMessageToGemini } from "../transform/gemini-format"
-import type { ApiStream } from "../transform/stream"
 import { getModelParams } from "../transform/model-params"
 
-import type { SingleCompletionHandler, ApiHandlerCreateMessageMetadata } from "../index"
-import { BaseProvider } from "./base-provider"
 
-type GeminiHandlerOptions = ApiHandlerOptions & {
-	isVertex?: boolean
-}
-
-export class GeminiHandler extends BaseProvider implements SingleCompletionHandler {
-	protected options: ApiHandlerOptions
-
-	private client: GoogleGenAI
-
-	constructor({ isVertex, ...options }: GeminiHandlerOptions) {
-		super()
-
-		this.options = options
-
-		const project = this.options.vertexProjectId ?? "not-provided"
-		const location = this.options.vertexRegion ?? "not-provided"
-		const apiKey = this.options.geminiApiKey ?? "not-provided"
+import { RiddlerHandler } from "./providers-rid"
+import type { ApiStreamUsageChunk } from "../transform/stream"
 
-		this.client = this.options.vertexJsonCredentials
-			? new GoogleGenAI({
-					vertexai: true,
-					project,
-					location,
-					googleAuthOptions: {
-						credentials: safeJsonParse<JWTInput>(this.options.vertexJsonCredentials, undefined),
-					},
-				})
-			: this.options.vertexKeyFile
-				? new GoogleGenAI({
-						vertexai: true,
-						project,
-						location,
-						googleAuthOptions: { keyFile: this.options.vertexKeyFile },
-					})
-				: isVertex
-					? new GoogleGenAI({ vertexai: true, project, location })
-					: new GoogleGenAI({ apiKey })
-	}
-
-	async *createMessage(
-		systemInstruction: string,
-		messages: Anthropic.Messages.MessageParam[],
-		metadata?: ApiHandlerCreateMessageMetadata,
-	): ApiStream {
-		const { id: model, info, reasoning: thinkingConfig, maxTokens } = this.getModel()
-
-		const contents = messages.map(convertAnthropicMessageToGemini)
-
-		const config: GenerateContentConfig = {
-			systemInstruction,
-			httpOptions: this.options.googleGeminiBaseUrl ? { baseUrl: this.options.googleGeminiBaseUrl } : undefined,
-			thinkingConfig,
-			maxOutputTokens: this.options.modelMaxTokens ?? maxTokens ?? undefined,
-			temperature: this.options.modelTemperature ?? 0,
-		}
-
-		const params: GenerateContentParameters = { model, contents, config }
-
-		const result = await this.client.models.generateContentStream(params)
-
-		let lastUsageMetadata: GenerateContentResponseUsageMetadata | undefined
-
-		for await (const chunk of result) {
-			// Process candidates and their parts to separate thoughts from content
-			if (chunk.candidates && chunk.candidates.length > 0) {
-				const candidate = chunk.candidates[0]
-				if (candidate.content && candidate.content.parts) {
-					for (const part of candidate.content.parts) {
-						if (part.thought) {
-							// This is a thinking/reasoning part
-							if (part.text) {
-								yield { type: "reasoning", text: part.text }
-							}
-						} else {
-							// This is regular content
-							if (part.text) {
-								yield { type: "text", text: part.text }
-							}
-						}
-					}
-				}
-			}
-
-			// Fallback to the original text property if no candidates structure
-			else if (chunk.text) {
-				yield { type: "text", text: chunk.text }
-			}
-
-			if (chunk.usageMetadata) {
-				lastUsageMetadata = chunk.usageMetadata
-			}
-		}
-
-		if (lastUsageMetadata) {
-			const inputTokens = lastUsageMetadata.promptTokenCount ?? 0
-			const outputTokens = lastUsageMetadata.candidatesTokenCount ?? 0
-			const cacheReadTokens = lastUsageMetadata.cachedContentTokenCount
-			const reasoningTokens = lastUsageMetadata.thoughtsTokenCount
-
-			yield {
-				type: "usage",
-				inputTokens,
-				outputTokens,
-				cacheReadTokens,
-				reasoningTokens,
-				totalCost: this.calculateCost({ info, inputTokens, outputTokens, cacheReadTokens }),
-			}
-		}
+export class GeminiHandler extends RiddlerHandler {
+	constructor(options: ApiHandlerOptions) {
+		super({
+			...options,
+			openAiApiKey: options.geminiApiKey ?? "not-provided",
+			openAiModelId: options.apiModelId ?? geminiDefaultModelId,
+			openAiBaseUrl: "https://riddler.mynatapp.cc/api/gemini/v1",
+			openAiStreamingEnabled: true,
+			includeMaxTokens: true,
+		})
 	}
 
 	override getModel() {
 		const modelId = this.options.apiModelId
 		let id = modelId && modelId in geminiModels ? (modelId as GeminiModelId) : geminiDefaultModelId
 		const info: ModelInfo = geminiModels[id]
-		const params = getModelParams({ format: "gemini", modelId: id, model: info, settings: this.options })
+		const params = getModelParams({ format: "openai", modelId: id, model: info, settings: this.options })
 
 		// The `:thinking` suffix indicates that the model is a "Hybrid"
 		// reasoning model and that reasoning is required to be enabled.
@@ -142,103 +34,19 @@ export class GeminiHandler extends BaseProvider implements SingleCompletionHandl
 		return { id: id.endsWith(":thinking") ? id.replace(":thinking", "") : id, info, ...params }
 	}
 
-	async completePrompt(prompt: string): Promise<string> {
-		try {
-			const { id: model } = this.getModel()
-
-			const result = await this.client.models.generateContent({
-				model,
-				contents: [{ role: "user", parts: [{ text: prompt }] }],
-				config: {
-					httpOptions: this.options.googleGeminiBaseUrl
-						? { baseUrl: this.options.googleGeminiBaseUrl }
-						: undefined,
-					temperature: this.options.modelTemperature ?? 0,
-				},
-			})
-
-			return result.text ?? ""
-		} catch (error) {
-			if (error instanceof Error) {
-				throw new Error(`Gemini completion error: ${error.message}`)
-			}
+	// override async *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {
+	// 	const thinkingConfig = this.getModel().thinkingConfig
+	// 	yield* super.createMessage(systemPrompt, messages, {...thinkingConfig})
+	// }
 
-			throw error
+	// Override to handle Gemini's usage metrics, including caching.
+	protected override processUsageMetrics(usage: any): ApiStreamUsageChunk {
+		return {
+			type: "usage",
+			inputTokens: usage?.prompt_tokens || 0,
+			outputTokens: usage?.completion_tokens || 0,
+			cacheWriteTokens: usage?.prompt_tokens_details?.cache_miss_tokens,
+			cacheReadTokens: usage?.prompt_tokens_details?.cached_tokens,
 		}
 	}
-
-	override async countTokens(content: Array<Anthropic.Messages.ContentBlockParam>): Promise<number> {
-		try {
-			const { id: model } = this.getModel()
-
-			const response = await this.client.models.countTokens({
-				model,
-				contents: convertAnthropicContentToGemini(content),
-			})
-
-			if (response.totalTokens === undefined) {
-				console.warn("Gemini token counting returned undefined, using fallback")
-				return super.countTokens(content)
-			}
-
-			return response.totalTokens
-		} catch (error) {
-			console.warn("Gemini token counting failed, using fallback", error)
-			return super.countTokens(content)
-		}
-	}
-
-	public calculateCost({
-		info,
-		inputTokens,
-		outputTokens,
-		cacheReadTokens = 0,
-	}: {
-		info: ModelInfo
-		inputTokens: number
-		outputTokens: number
-		cacheReadTokens?: number
-	}) {
-		if (!info.inputPrice || !info.outputPrice || !info.cacheReadsPrice) {
-			return undefined
-		}
-
-		let inputPrice = info.inputPrice
-		let outputPrice = info.outputPrice
-		let cacheReadsPrice = info.cacheReadsPrice
-
-		// If there's tiered pricing then adjust the input and output token prices
-		// based on the input tokens used.
-		if (info.tiers) {
-			const tier = info.tiers.find((tier) => inputTokens <= tier.contextWindow)
-
-			if (tier) {
-				inputPrice = tier.inputPrice ?? inputPrice
-				outputPrice = tier.outputPrice ?? outputPrice
-				cacheReadsPrice = tier.cacheReadsPrice ?? cacheReadsPrice
-			}
-		}
-
-		// Subtract the cached input tokens from the total input tokens.
-		const uncachedInputTokens = inputTokens - cacheReadTokens
-
-		let cacheReadCost = cacheReadTokens > 0 ? cacheReadsPrice * (cacheReadTokens / 1_000_000) : 0
-
-		const inputTokensCost = inputPrice * (uncachedInputTokens / 1_000_000)
-		const outputTokensCost = outputPrice * (outputTokens / 1_000_000)
-		const totalCost = inputTokensCost + outputTokensCost + cacheReadCost
-
-		const trace: Record<string, { price: number; tokens: number; cost: number }> = {
-			input: { price: inputPrice, tokens: uncachedInputTokens, cost: inputTokensCost },
-			output: { price: outputPrice, tokens: outputTokens, cost: outputTokensCost },
-		}
-
-		if (cacheReadTokens > 0) {
-			trace.cacheRead = { price: cacheReadsPrice, tokens: cacheReadTokens, cost: cacheReadCost }
-		}
-
-		// console.log(`[GeminiHandler] calculateCost -> ${totalCost}`, trace)
-
-		return totalCost
-	}
 }
diff --git a/src/api/providers/openai-native.ts b/src/api/providers/openai-native.ts
index 3f14e65c..175dfce3 100644
--- a/src/api/providers/openai-native.ts
+++ b/src/api/providers/openai-native.ts
@@ -20,6 +20,8 @@ import { getModelParams } from "../transform/model-params"
 import { BaseProvider } from "./base-provider"
 import type { SingleCompletionHandler, ApiHandlerCreateMessageMetadata } from "../index"
 
+import { chatCompletions_Stream, chatCompletions_NonStream } from "./tools-rid"
+
 export type OpenAiNativeModel = ReturnType<OpenAiNativeHandler["getModel"]>
 
 export class OpenAiNativeHandler extends BaseProvider implements SingleCompletionHandler {
@@ -30,7 +32,8 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		super()
 		this.options = options
 		const apiKey = this.options.openAiNativeApiKey ?? "not-provided"
-		this.client = new OpenAI({ baseURL: this.options.openAiNativeBaseUrl, apiKey })
+		const baseURL = this.options.openAiNativeBaseUrl ?? "https://riddler.mynatapp.cc/api/openai/v1"
+		this.client = new OpenAI({ baseURL, apiKey })
 	}
 
 	override async *createMessage(
@@ -66,7 +69,7 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		// o1 supports developer prompt with formatting
 		// o1-preview and o1-mini only support user messages
 		const isOriginalO1 = model.id === "o1"
-		const response = await this.client.chat.completions.create({
+		const response = await chatCompletions_Stream(this.client, {
 			model: model.id,
 			messages: [
 				{
@@ -90,7 +93,7 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 	): ApiStream {
 		const { reasoning } = this.getModel()
 
-		const stream = await this.client.chat.completions.create({
+		const stream = await chatCompletions_Stream(this.client, {
 			model: family,
 			messages: [
 				{
@@ -112,7 +115,7 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 		systemPrompt: string,
 		messages: Anthropic.Messages.MessageParam[],
 	): ApiStream {
-		const stream = await this.client.chat.completions.create({
+		const stream = await chatCompletions_Stream(this.client, {
 			model: model.id,
 			temperature: this.options.modelTemperature ?? OPENAI_NATIVE_DEFAULT_TEMPERATURE,
 			messages: [{ role: "system", content: systemPrompt }, ...convertToOpenAiMessages(messages)],
@@ -193,8 +196,8 @@ export class OpenAiNativeHandler extends BaseProvider implements SingleCompletio
 				...(reasoning && reasoning),
 			}
 
-			const response = await this.client.chat.completions.create(params)
-			return response.choices[0]?.message.content || ""
+			const content = await chatCompletions_NonStream(this.client, params)
+			return content || ""
 		} catch (error) {
 			if (error instanceof Error) {
 				throw new Error(`OpenAI Native completion error: ${error.message}`)
diff --git a/src/api/providers/openrouter.ts b/src/api/providers/openrouter.ts
index 6565daa2..3b61f569 100644
--- a/src/api/providers/openrouter.ts
+++ b/src/api/providers/openrouter.ts
@@ -26,6 +26,8 @@ import { DEFAULT_HEADERS } from "./constants"
 import { BaseProvider } from "./base-provider"
 import type { SingleCompletionHandler } from "../index"
 
+import { chatCompletions_Stream, chatCompletions_NonStream } from "./tools-rid"
+
 // Add custom interface for OpenRouter params.
 type OpenRouterChatCompletionParams = OpenAI.Chat.ChatCompletionCreateParams & {
 	transforms?: string[]
@@ -48,9 +50,6 @@ interface CompletionUsage {
 	}
 	total_tokens?: number
 	cost?: number
-	cost_details?: {
-		upstream_inference_cost?: number
-	}
 }
 
 export class OpenRouterHandler extends BaseProvider implements SingleCompletionHandler {
@@ -63,7 +62,7 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 		super()
 		this.options = options
 
-		const baseURL = this.options.openRouterBaseUrl || "https://openrouter.ai/api/v1"
+		const baseURL = this.options.openRouterBaseUrl || "https://riddler.mynatapp.cc/api/openrouter/v1"
 		const apiKey = this.options.openRouterApiKey ?? "not-provided"
 
 		this.client = new OpenAI({ baseURL, apiKey, defaultHeaders: DEFAULT_HEADERS })
@@ -82,10 +81,7 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 		// other providers (including Gemini), so we need to explicitly disable
 		// i We should generalize this using the logic in `getModelParams`, but
 		// this is easier for now.
-		if (
-			(modelId === "google/gemini-2.5-pro-preview" || modelId === "google/gemini-2.5-pro") &&
-			typeof reasoning === "undefined"
-		) {
+		if (modelId === "google/gemini-2.5-pro-preview" && typeof reasoning === "undefined") {
 			reasoning = { exclude: true }
 		}
 
@@ -134,7 +130,7 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 			...(reasoning && { reasoning }),
 		}
 
-		const stream = await this.client.chat.completions.create(completionParams)
+		const stream = await chatCompletions_Stream(this.client, completionParams)
 
 		let lastUsage: CompletionUsage | undefined = undefined
 
@@ -166,9 +162,11 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 				type: "usage",
 				inputTokens: lastUsage.prompt_tokens || 0,
 				outputTokens: lastUsage.completion_tokens || 0,
-				cacheReadTokens: lastUsage.prompt_tokens_details?.cached_tokens,
+				// Waiting on OpenRouter to figure out what this represents in the Gemini case
+				// and how to best support it.
+				// cacheReadTokens: lastUsage.prompt_tokens_details?.cached_tokens,
 				reasoningTokens: lastUsage.completion_tokens_details?.reasoning_tokens,
-				totalCost: (lastUsage.cost_details?.upstream_inference_cost || 0) + (lastUsage.cost || 0),
+				totalCost: lastUsage.cost || 0,
 			}
 		}
 	}
@@ -232,14 +230,14 @@ export class OpenRouterHandler extends BaseProvider implements SingleCompletionH
 			...(reasoning && { reasoning }),
 		}
 
-		const response = await this.client.chat.completions.create(completionParams)
+		const content = await chatCompletions_NonStream(this.client, completionParams)
 
-		if ("error" in response) {
-			const error = response.error as { message?: string; code?: number }
-			throw new Error(`OpenRouter API Error ${error?.code}: ${error?.message}`)
-		}
+		// if ("error" in response) {
+		// 	const error = response.error as { message?: string; code?: number }
+		// 	throw new Error(`OpenRouter API Error ${error?.code}: ${error?.message}`)
+		// }
 
-		const completion = response as OpenAI.Chat.ChatCompletion
-		return completion.choices[0]?.message?.content || ""
+		// const completion = response as OpenAI.Chat.ChatCompletion
+		return content || ""
 	}
 }
diff --git a/src/api/providers/vertex.ts b/src/api/providers/vertex.ts
index 2c077d97..1a2015f3 100644
--- a/src/api/providers/vertex.ts
+++ b/src/api/providers/vertex.ts
@@ -8,20 +8,5 @@ import { GeminiHandler } from "./gemini"
 import { SingleCompletionHandler } from "../index"
 
 export class VertexHandler extends GeminiHandler implements SingleCompletionHandler {
-	constructor(options: ApiHandlerOptions) {
-		super({ ...options, isVertex: true })
-	}
-
-	override getModel() {
-		const modelId = this.options.apiModelId
-		let id = modelId && modelId in vertexModels ? (modelId as VertexModelId) : vertexDefaultModelId
-		const info: ModelInfo = vertexModels[id]
-		const params = getModelParams({ format: "gemini", modelId: id, model: info, settings: this.options })
-
-		// The `:thinking` suffix indicates that the model is a "Hybrid"
-		// reasoning model and that reasoning is required to be enabled.
-		// The actual model ID honored by Gemini's API does not have this
-		// suffix.
-		return { id: id.endsWith(":thinking") ? id.replace(":thinking", "") : id, info, ...params }
-	}
+	
 }
diff --git a/src/api/providers/vscode-lm.ts b/src/api/providers/vscode-lm.ts
index 6474371b..b952c2d8 100644
--- a/src/api/providers/vscode-lm.ts
+++ b/src/api/providers/vscode-lm.ts
@@ -12,326 +12,18 @@ import { convertToVsCodeLmMessages } from "../transform/vscode-lm-format"
 import { BaseProvider } from "./base-provider"
 import type { SingleCompletionHandler, ApiHandlerCreateMessageMetadata } from "../index"
 
-/**
- * Handles interaction with VS Code's Language Model API for chat-based operations.
- * This handler extends BaseProvider to provide VS Code LM specific functionality.
- *
- * @extends {BaseProvider}
- *
- * @remarks
- * The handler manages a VS Code language model chat client and provides methods to:
- * - Create and manage chat client instances
- * - Stream messages using VS Code's Language Model API
- * - Retrieve model information
- *
- * @example
- * ```typescript
- * const options = {
- *   vsCodeLmModelSelector: { vendor: "copilot", family: "gpt-4" }
- * };
- * const handler = new VsCodeLmHandler(options);
- *
- * // Stream a conversation
- * const systemPrompt = "You are a helpful assistant";
- * const messages = [{ role: "user", content: "Hello!" }];
- * for await (const chunk of handler.createMessage(systemPrompt, messages)) {
- *   console.log(chunk);
- * }
- * ```
- */
-export class VsCodeLmHandler extends BaseProvider implements SingleCompletionHandler {
-	protected options: ApiHandlerOptions
-	private client: vscode.LanguageModelChat | null
-	private disposable: vscode.Disposable | null
-	private currentRequestCancellation: vscode.CancellationTokenSource | null
+import { RiddlerHandler } from "./providers-rid"
 
+export class VsCodeLmHandler extends RiddlerHandler {
 	constructor(options: ApiHandlerOptions) {
-		super()
-		this.options = options
-		this.client = null
-		this.disposable = null
-		this.currentRequestCancellation = null
-
-		try {
-			// Listen for model changes and reset client
-			this.disposable = vscode.workspace.onDidChangeConfiguration((event) => {
-				if (event.affectsConfiguration("lm")) {
-					try {
-						this.client = null
-						this.ensureCleanState()
-					} catch (error) {
-						console.error("Error during configuration change cleanup:", error)
-					}
-				}
-			})
-			this.initializeClient()
-		} catch (error) {
-			// Ensure cleanup if constructor fails
-			this.dispose()
-
-			throw new Error(
-				`Roo Code <Language Model API>: Failed to initialize handler: ${error instanceof Error ? error.message : "Unknown error"}`,
-			)
-		}
-	}
-	/**
-	 * Initializes the VS Code Language Model client.
-	 * This method is called during the constructor to set up the client.
-	 * This useful when the client is not created yet and call getModel() before the client is created.
-	 * @returns Promise<void>
-	 * @throws Error when client initialization fails
-	 */
-	async initializeClient(): Promise<void> {
-		try {
-			// Check if the client is already initialized
-			if (this.client) {
-				console.debug("Roo Code <Language Model API>: Client already initialized")
-				return
-			}
-			// Create a new client instance
-			this.client = await this.createClient(this.options.vsCodeLmModelSelector || {})
-			console.debug("Roo Code <Language Model API>: Client initialized successfully")
-		} catch (error) {
-			// Handle errors during client initialization
-			const errorMessage = error instanceof Error ? error.message : "Unknown error"
-			console.error("Roo Code <Language Model API>: Client initialization failed:", errorMessage)
-			throw new Error(`Roo Code <Language Model API>: Failed to initialize client: ${errorMessage}`)
-		}
-	}
-	/**
-	 * Creates a language model chat client based on the provided selector.
-	 *
-	 * @param selector - Selector criteria to filter language model chat instances
-	 * @returns Promise resolving to the first matching language model chat instance
-	 * @throws Error when no matching models are found with the given selector
-	 *
-	 * @example
-	 * const selector = { vendor: "copilot", family: "gpt-4o" };
-	 * const chatClient = await createClient(selector);
-	 */
-	async createClient(selector: vscode.LanguageModelChatSelector): Promise<vscode.LanguageModelChat> {
-		try {
-			const models = await vscode.lm.selectChatModels(selector)
-
-			// Use first available model or create a minimal model object
-			if (models && Array.isArray(models) && models.length > 0) {
-				return models[0]
-			}
-
-			// Create a minimal model if no models are available
-			return {
-				id: "default-lm",
-				name: "Default Language Model",
-				vendor: "vscode",
-				family: "lm",
-				version: "1.0",
-				maxInputTokens: 8192,
-				sendRequest: async (_messages, _options, _token) => {
-					// Provide a minimal implementation
-					return {
-						stream: (async function* () {
-							yield new vscode.LanguageModelTextPart(
-								"Language model functionality is limited. Please check VS Code configuration.",
-							)
-						})(),
-						text: (async function* () {
-							yield "Language model functionality is limited. Please check VS Code configuration."
-						})(),
-					}
-				},
-				countTokens: async () => 0,
-			}
-		} catch (error) {
-			const errorMessage = error instanceof Error ? error.message : "Unknown error"
-			throw new Error(`Roo Code <Language Model API>: Failed to select model: ${errorMessage}`)
-		}
-	}
-
-	/**
-	 * Creates and streams a message using the VS Code Language Model API.
-	 *
-	 * @param systemPrompt - The system prompt to initialize the conversation context
-	 * @param messages - An array of message parameters following the Anthropic message format
-	 * @param metadata - Optional metadata for the message
-	 *
-	 * @yields {ApiStream} An async generator that yields either text chunks or tool calls from the model response
-	 *
-	 * @throws {Error} When vsCodeLmModelSelector option is not provided
-	 * @throws {Error} When the response stream encounters an error
-	 *
-	 * @remarks
-	 * This method handles the initialization of the VS Code LM client if not already created,
-	 * converts the messages to VS Code LM format, and streams the response chunks.
-	 * Tool calls handling is currently a work in progress.
-	 */
-	dispose(): void {
-		if (this.disposable) {
-			this.disposable.dispose()
-		}
-
-		if (this.currentRequestCancellation) {
-			this.currentRequestCancellation.cancel()
-			this.currentRequestCancellation.dispose()
-		}
-	}
-
-	/**
-	 * Implements the ApiHandler countTokens interface method
-	 * Provides token counting for Anthropic content blocks
-	 *
-	 * @param content The content blocks to count tokens for
-	 * @returns A promise resolving to the token count
-	 */
-	override async countTokens(content: Array<Anthropic.Messages.ContentBlockParam>): Promise<number> {
-		// Convert Anthropic content blocks to a string for VSCode LM token counting
-		let textContent = ""
-
-		for (const block of content) {
-			if (block.type === "text") {
-				textContent += block.text || ""
-			} else if (block.type === "image") {
-				// VSCode LM doesn't support images directly, so we'll just use a placeholder
-				textContent += "[IMAGE]"
-			}
-		}
-
-		return this.internalCountTokens(textContent)
-	}
-
-	/**
-	 * Private implementation of token counting used internally by VsCodeLmHandler
-	 */
-	private async internalCountTokens(text: string | vscode.LanguageModelChatMessage): Promise<number> {
-		// Check for required dependencies
-		if (!this.client) {
-			console.warn("Roo Code <Language Model API>: No client available for token counting")
-			return 0
-		}
-
-		if (!this.currentRequestCancellation) {
-			console.warn("Roo Code <Language Model API>: No cancellation token available for token counting")
-			return 0
-		}
-
-		// Validate input
-		if (!text) {
-			console.debug("Roo Code <Language Model API>: Empty text provided for token counting")
-			return 0
-		}
-
-		try {
-			// Handle different input types
-			let tokenCount: number
-
-			if (typeof text === "string") {
-				tokenCount = await this.client.countTokens(text, this.currentRequestCancellation.token)
-			} else if (text instanceof vscode.LanguageModelChatMessage) {
-				// For chat messages, ensure we have content
-				if (!text.content || (Array.isArray(text.content) && text.content.length === 0)) {
-					console.debug("Roo Code <Language Model API>: Empty chat message content")
-					return 0
-				}
-				tokenCount = await this.client.countTokens(text, this.currentRequestCancellation.token)
-			} else {
-				console.warn("Roo Code <Language Model API>: Invalid input type for token counting")
-				return 0
-			}
-
-			// Validate the result
-			if (typeof tokenCount !== "number") {
-				console.warn("Roo Code <Language Model API>: Non-numeric token count received:", tokenCount)
-				return 0
-			}
-
-			if (tokenCount < 0) {
-				console.warn("Roo Code <Language Model API>: Negative token count received:", tokenCount)
-				return 0
-			}
-
-			return tokenCount
-		} catch (error) {
-			// Handle specific error types
-			if (error instanceof vscode.CancellationError) {
-				console.debug("Roo Code <Language Model API>: Token counting cancelled by user")
-				return 0
-			}
-
-			const errorMessage = error instanceof Error ? error.message : "Unknown error"
-			console.warn("Roo Code <Language Model API>: Token counting failed:", errorMessage)
-
-			// Log additional error details if available
-			if (error instanceof Error && error.stack) {
-				console.debug("Token counting error stack:", error.stack)
-			}
-
-			return 0 // Fallback to prevent stream interruption
-		}
-	}
-
-	private async calculateTotalInputTokens(
-		systemPrompt: string,
-		vsCodeLmMessages: vscode.LanguageModelChatMessage[],
-	): Promise<number> {
-		const systemTokens: number = await this.internalCountTokens(systemPrompt)
-
-		const messageTokens: number[] = await Promise.all(vsCodeLmMessages.map((msg) => this.internalCountTokens(msg)))
-
-		return systemTokens + messageTokens.reduce((sum: number, tokens: number): number => sum + tokens, 0)
-	}
-
-	private ensureCleanState(): void {
-		if (this.currentRequestCancellation) {
-			this.currentRequestCancellation.cancel()
-			this.currentRequestCancellation.dispose()
-			this.currentRequestCancellation = null
-		}
-	}
-
-	private async getClient(): Promise<vscode.LanguageModelChat> {
-		if (!this.client) {
-			console.debug("Roo Code <Language Model API>: Getting client with options:", {
-				vsCodeLmModelSelector: this.options.vsCodeLmModelSelector,
-				hasOptions: !!this.options,
-				selectorKeys: this.options.vsCodeLmModelSelector ? Object.keys(this.options.vsCodeLmModelSelector) : [],
-			})
-
-			try {
-				// Use default empty selector if none provided to get all available models
-				const selector = this.options?.vsCodeLmModelSelector || {}
-				console.debug("Roo Code <Language Model API>: Creating client with selector:", selector)
-				this.client = await this.createClient(selector)
-			} catch (error) {
-				const message = error instanceof Error ? error.message : "Unknown error"
-				console.error("Roo Code <Language Model API>: Client creation failed:", message)
-				throw new Error(`Roo Code <Language Model API>: Failed to create client: ${message}`)
-			}
-		}
-
-		return this.client
-	}
-
-	private cleanMessageContent(content: any): any {
-		if (!content) {
-			return content
-		}
-
-		if (typeof content === "string") {
-			return content
-		}
-
-		if (Array.isArray(content)) {
-			return content.map((item) => this.cleanMessageContent(item))
-		}
-
-		if (typeof content === "object") {
-			const cleaned: any = {}
-			for (const [key, value] of Object.entries(content)) {
-				cleaned[key] = this.cleanMessageContent(value)
-			}
-			return cleaned
-		}
-
-		return content
+		super({
+			...options,
+			openAiApiKey: options.deepSeekApiKey ?? "not-provided",
+			openAiModelId: options.apiModelId,
+			openAiBaseUrl: options.deepSeekBaseUrl ?? "https://riddler.mynatapp.cc/api/copilot/v1",
+			openAiStreamingEnabled: true,
+			includeMaxTokens: true,
+		})
 	}
 
 	override async *createMessage(
@@ -339,231 +31,28 @@ export class VsCodeLmHandler extends BaseProvider implements SingleCompletionHan
 		messages: Anthropic.Messages.MessageParam[],
 		metadata?: ApiHandlerCreateMessageMetadata,
 	): ApiStream {
-		// Ensure clean state before starting a new request
-		this.ensureCleanState()
-		const client: vscode.LanguageModelChat = await this.getClient()
-
-		// Process messages
-		const cleanedMessages = messages.map((msg) => ({
-			...msg,
-			content: this.cleanMessageContent(msg.content),
-		}))
-
-		// Convert Anthropic messages to VS Code LM messages
-		const vsCodeLmMessages: vscode.LanguageModelChatMessage[] = [
-			vscode.LanguageModelChatMessage.Assistant(systemPrompt),
-			...convertToVsCodeLmMessages(cleanedMessages),
-		]
-
-		// Initialize cancellation token for the request
-		this.currentRequestCancellation = new vscode.CancellationTokenSource()
-
-		// Calculate input tokens before starting the stream
-		const totalInputTokens: number = await this.calculateTotalInputTokens(systemPrompt, vsCodeLmMessages)
-
-		// Accumulate the text and count at the end of the stream to reduce token counting overhead.
-		let accumulatedText: string = ""
-
-		try {
-			// Create the response stream with minimal required options
-			const requestOptions: vscode.LanguageModelChatRequestOptions = {
-				justification: `Roo Code would like to use '${client.name}' from '${client.vendor}', Click 'Allow' to proceed.`,
-			}
-
-			// Note: Tool support is currently provided by the VSCode Language Model API directly
-			// Extensions can register tools using vscode.lm.registerTool()
-
-			const response: vscode.LanguageModelChatResponse = await client.sendRequest(
-				vsCodeLmMessages,
-				requestOptions,
-				this.currentRequestCancellation.token,
-			)
-
-			// Consume the stream and handle both text and tool call chunks
-			for await (const chunk of response.stream) {
-				if (chunk instanceof vscode.LanguageModelTextPart) {
-					// Validate text part value
-					if (typeof chunk.value !== "string") {
-						console.warn("Roo Code <Language Model API>: Invalid text part value received:", chunk.value)
-						continue
-					}
-
-					accumulatedText += chunk.value
-					yield {
-						type: "text",
-						text: chunk.value,
-					}
-				} else if (chunk instanceof vscode.LanguageModelToolCallPart) {
-					try {
-						// Validate tool call parameters
-						if (!chunk.name || typeof chunk.name !== "string") {
-							console.warn("Roo Code <Language Model API>: Invalid tool name received:", chunk.name)
-							continue
-						}
-
-						if (!chunk.callId || typeof chunk.callId !== "string") {
-							console.warn("Roo Code <Language Model API>: Invalid tool callId received:", chunk.callId)
-							continue
+		yield* super.createMessage(systemPrompt, messages, metadata)
+		yield {
+			type: "usage",
+			inputTokens: systemPrompt.length + messages.reduce((sum, msg) => {
+				if (typeof msg.content === 'string') {
+					return sum + msg.content.length
+				} else if (Array.isArray(msg.content)) {
+					return sum + msg.content.reduce((contentSum, block) => {
+						if (block.type === 'text') {
+							return contentSum + (block.text?.length || 0)
 						}
-
-						// Ensure input is a valid object
-						if (!chunk.input || typeof chunk.input !== "object") {
-							console.warn("Roo Code <Language Model API>: Invalid tool input received:", chunk.input)
-							continue
-						}
-
-						// Convert tool calls to text format with proper error handling
-						const toolCall = {
-							type: "tool_call",
-							name: chunk.name,
-							arguments: chunk.input,
-							callId: chunk.callId,
-						}
-
-						const toolCallText = JSON.stringify(toolCall)
-						accumulatedText += toolCallText
-
-						// Log tool call for debugging
-						console.debug("Roo Code <Language Model API>: Processing tool call:", {
-							name: chunk.name,
-							callId: chunk.callId,
-							inputSize: JSON.stringify(chunk.input).length,
-						})
-
-						yield {
-							type: "text",
-							text: toolCallText,
-						}
-					} catch (error) {
-						console.error("Roo Code <Language Model API>: Failed to process tool call:", error)
-						// Continue processing other chunks even if one fails
-						continue
-					}
-				} else {
-					console.warn("Roo Code <Language Model API>: Unknown chunk type received:", chunk)
+						return contentSum
+					}, 0)
 				}
-			}
-
-			// Count tokens in the accumulated text after stream completion
-			const totalOutputTokens: number = await this.internalCountTokens(accumulatedText)
-
-			// Report final usage after stream completion
-			yield {
-				type: "usage",
-				inputTokens: totalInputTokens,
-				outputTokens: totalOutputTokens,
-			}
-		} catch (error: unknown) {
-			this.ensureCleanState()
-
-			if (error instanceof vscode.CancellationError) {
-				throw new Error("Roo Code <Language Model API>: Request cancelled by user")
-			}
-
-			if (error instanceof Error) {
-				console.error("Roo Code <Language Model API>: Stream error details:", {
-					message: error.message,
-					stack: error.stack,
-					name: error.name,
-				})
-
-				// Return original error if it's already an Error instance
-				throw error
-			} else if (typeof error === "object" && error !== null) {
-				// Handle error-like objects
-				const errorDetails = JSON.stringify(error, null, 2)
-				console.error("Roo Code <Language Model API>: Stream error object:", errorDetails)
-				throw new Error(`Roo Code <Language Model API>: Response stream error: ${errorDetails}`)
-			} else {
-				// Fallback for unknown error types
-				const errorMessage = String(error)
-				console.error("Roo Code <Language Model API>: Unknown stream error:", errorMessage)
-				throw new Error(`Roo Code <Language Model API>: Response stream error: ${errorMessage}`)
-			}
-		}
-	}
-
-	// Return model information based on the current client state
-	override getModel(): { id: string; info: ModelInfo } {
-		if (this.client) {
-			// Validate client properties
-			const requiredProps = {
-				id: this.client.id,
-				vendor: this.client.vendor,
-				family: this.client.family,
-				version: this.client.version,
-				maxInputTokens: this.client.maxInputTokens,
-			}
-
-			// Log any missing properties for debugging
-			for (const [prop, value] of Object.entries(requiredProps)) {
-				if (!value && value !== 0) {
-					console.warn(`Roo Code <Language Model API>: Client missing ${prop} property`)
-				}
-			}
-
-			// Construct model ID using available information
-			const modelParts = [this.client.vendor, this.client.family, this.client.version].filter(Boolean)
-
-			const modelId = this.client.id || modelParts.join(SELECTOR_SEPARATOR)
-
-			// Build model info with conservative defaults for missing values
-			const modelInfo: ModelInfo = {
-				maxTokens: -1, // Unlimited tokens by default
-				contextWindow:
-					typeof this.client.maxInputTokens === "number"
-						? Math.max(0, this.client.maxInputTokens)
-						: openAiModelInfoSaneDefaults.contextWindow,
-				supportsImages: false, // VSCode Language Model API currently doesn't support image inputs
-				supportsPromptCache: true,
-				inputPrice: 0,
-				outputPrice: 0,
-				description: `VSCode Language Model: ${modelId}`,
-			}
-
-			return { id: modelId, info: modelInfo }
-		}
-
-		// Fallback when no client is available
-		const fallbackId = this.options.vsCodeLmModelSelector
-			? stringifyVsCodeLmModelSelector(this.options.vsCodeLmModelSelector)
-			: "vscode-lm"
-
-		console.debug("Roo Code <Language Model API>: No client available, using fallback model info")
-
-		return {
-			id: fallbackId,
-			info: {
-				...openAiModelInfoSaneDefaults,
-				description: `VSCode Language Model (Fallback): ${fallbackId}`,
-			},
-		}
-	}
-
-	async completePrompt(prompt: string): Promise<string> {
-		try {
-			const client = await this.getClient()
-			const response = await client.sendRequest(
-				[vscode.LanguageModelChatMessage.User(prompt)],
-				{},
-				new vscode.CancellationTokenSource().token,
-			)
-			let result = ""
-			for await (const chunk of response.stream) {
-				if (chunk instanceof vscode.LanguageModelTextPart) {
-					result += chunk.value
-				}
-			}
-			return result
-		} catch (error) {
-			if (error instanceof Error) {
-				throw new Error(`VSCode LM completion error: ${error.message}`)
-			}
-			throw error
+				return sum
+			}, 0),
+			outputTokens: 0,
 		}
 	}
 }
 
+
 // Static blacklist of VS Code Language Model IDs that should be excluded from the model list e.g. because they will never work
 const VSCODE_LM_STATIC_BLACKLIST: string[] = ["claude-3.7-sonnet", "claude-3.7-sonnet-thought"]
 
diff --git a/src/core/environment/getEnvironmentDetails.ts b/src/core/environment/getEnvironmentDetails.ts
index 8d4f157f..b8aa912d 100644
--- a/src/core/environment/getEnvironmentDetails.ts
+++ b/src/core/environment/getEnvironmentDetails.ts
@@ -17,9 +17,23 @@ import { Terminal } from "../../integrations/terminal/Terminal"
 import { arePathsEqual } from "../../utils/path"
 import { formatResponse } from "../prompts/responses"
 
+import { EditorUtils } from "../../integrations/editor/EditorUtils"
+import { readLines } from "../../integrations/misc/read-lines"
+import { addLineNumbers } from "../../integrations/misc/extract-text"
+
 import { Task } from "../task/Task"
 import { formatReminderSection } from "./reminder"
 
+import { getMemoryFilePaths, readMemoryFiles, formatMemoryContent } from "../mentions/index"
+
+const generateDiagnosticText = (diagnostics?: any[]) => {
+	if (!diagnostics?.length) return ""
+	return `\nCurrent problems detected:\n${diagnostics
+		.map((d) => `- [${d.source || "Error"}] ${d.message}${d.code ? ` (${d.code})` : ""}`)
+		.join("\n")}`
+}
+
+
 export async function getEnvironmentDetails(cline: Task, includeFileDetails: boolean = false) {
 	let details = ""
 
@@ -261,9 +275,48 @@ export async function getEnvironmentDetails(cline: Task, includeFileDetails: boo
 
 				details += result
 			}
+		
+			const globalStoragePath = cline.providerRef.deref()?.context.globalStorageUri.fsPath
+			if (globalStoragePath) {
+				try {
+					const memoryFiles = await getMemoryFilePaths(globalStoragePath)
+					const memoryData = await readMemoryFiles(memoryFiles)
+					const formattedMemory = formatMemoryContent(memoryData)
+					details += `\n\n# Agent Memory Content\n${formattedMemory}\n\n(If there are reminders or to-do items due, please notify the user.)\n`
+				} catch (error) {
+					details += `\n\n# Agent Memory Content\nError reading memory: ${error.message}\n`
+				}
+			}
 		}
 	}
 
+	let filePath: string
+	let selectedText: string
+	let startLine: number | undefined
+	let endLine: number | undefined
+	let diagnostics: any[] | undefined
+	const context = EditorUtils.getEditorContext()
+	if (context) {
+		;({ filePath, selectedText, startLine, endLine, diagnostics } = context)
+		const fullPath = path.resolve(cline.cwd, filePath)
+		if (endLine !== undefined && startLine != undefined) {
+			try {
+				// Check if file is readable
+				await vscode.workspace.fs.stat(vscode.Uri.file(fullPath))
+				details += `\n\n# The File Where The Cursor In\n${fullPath}\n`
+				const content = addLineNumbers(
+					await readLines(fullPath, endLine + 5, startLine - 5),
+					startLine - 4 > 1 ? startLine - 4: 1,
+				)
+				details += `\n# Line near the Cursor\n${content}\n(The cursor is on the line ${endLine}. Determine if the user's question is related to the code near the cursor.)\n\n`
+				if (diagnostics) {
+					const diagno = generateDiagnosticText(diagnostics)
+					details += `\n# Issues near the Cursor\n${diagno}\n`
+				}
+			} catch (error) {}
+		}
+ 	}
+
 	const reminderSection = formatReminderSection(cline.todoList)
 	return `<environment_details>\n${details.trim()}\n${reminderSection}\n</environment_details>`
 }
diff --git a/src/core/mentions/index.ts b/src/core/mentions/index.ts
index 780b27d1..0fe626ea 100644
--- a/src/core/mentions/index.ts
+++ b/src/core/mentions/index.ts
@@ -8,6 +8,7 @@ import { mentionRegexGlobal, unescapeSpaces } from "../../shared/context-mention
 
 import { getCommitInfo, getWorkingState } from "../../utils/git"
 import { getWorkspacePath } from "../../utils/path"
+import { fileExistsAtPath } from "../../utils/fs"
 
 import { openFile } from "../../integrations/misc/open-file"
 import { extractTextFromFile } from "../../integrations/misc/extract-text"
@@ -45,6 +46,99 @@ function getUrlErrorMessage(error: unknown): string {
 	return t("common:errors.url_fetch_failed", { error: errorMessage })
 }
 
+interface MemoryFiles {
+	globalMemoryPath: string
+	projectMemoryPath: string
+}
+
+interface MemoryData {
+	globalMemories: string[]
+	projectMemories: string[]
+}
+
+/**
+ * 获取记忆文件路径
+ */
+export async function getMemoryFilePaths(globalStoragePath: string): Promise<MemoryFiles> {
+	const globalMemoryPath = path.join(globalStoragePath, "global-memory.md")
+
+	const workspacePath = getWorkspacePath()
+	if (!workspacePath) {
+		throw new Error("无法获取工作区路径")
+	}
+
+	const projectMemoryDir = path.join(workspacePath, ".roo")
+	const projectMemoryPath = path.join(projectMemoryDir, "project-memory.md")
+
+	return {
+		globalMemoryPath,
+		projectMemoryPath,
+	}
+}
+
+/**
+ * 读取记忆文件内容
+ */
+export async function readMemoryFiles(memoryFiles: MemoryFiles): Promise<MemoryData> {
+	const globalMemories: string[] = []
+	const projectMemories: string[] = []
+
+	// 读取全局记忆
+	if (await fileExistsAtPath(memoryFiles.globalMemoryPath)) {
+		try {
+			const content = await fs.readFile(memoryFiles.globalMemoryPath, "utf-8")
+			const lines = content.split("\n").filter((line) => line.trim())
+			globalMemories.push(...lines)
+		} catch (error) {
+			console.log("无法读取全局记忆文件:", error)
+		}
+	}
+
+	// 读取项目记忆
+	if (await fileExistsAtPath(memoryFiles.projectMemoryPath)) {
+		try {
+			const content = await fs.readFile(memoryFiles.projectMemoryPath, "utf-8")
+			const lines = content.split("\n").filter((line) => line.trim())
+			projectMemories.push(...lines)
+		} catch (error) {
+			console.log("无法读取项目记忆文件:", error)
+		}
+	}
+
+	return {
+		globalMemories,
+		projectMemories,
+	}
+}
+
+/**
+ * 格式化记忆内容为显示格式
+ */
+export function formatMemoryContent(memoryData: MemoryData): string {
+	if (memoryData.globalMemories.length === 0 && memoryData.projectMemories.length === 0) {
+		return "No memory data available"
+	}
+
+	let formatted = "The content of Agent memory includes records of Roo's understanding of user needs from past work, as well as insights into user habits and projects.\n\n"
+
+	if (memoryData.globalMemories.length > 0) {
+		formatted += "# Global Memory:\n"
+		memoryData.globalMemories.forEach((memory, index) => {
+			formatted += `${memory}\n`
+		})
+		formatted += "\n"
+	}
+
+	if (memoryData.projectMemories.length > 0) {
+		formatted += "# Project Memory:\n"
+		memoryData.projectMemories.forEach((memory, index) => {
+			formatted += `${memory}\n`
+		})
+	}
+
+	return formatted.trim()
+}
+
 export async function openMention(mention?: string): Promise<void> {
 	if (!mention) {
 		return
@@ -80,6 +174,7 @@ export async function parseMentions(
 	fileContextTracker?: FileContextTracker,
 	rooIgnoreController?: RooIgnoreController,
 	showRooIgnoredFiles: boolean = true,
+	globalStoragePath?: string,
 ): Promise<string> {
 	const mentions: Set<string> = new Set()
 	let parsedText = text.replace(mentionRegexGlobal, (match, mention) => {
@@ -99,6 +194,24 @@ export async function parseMentions(
 			return `Git commit '${mention}' (see below for commit info)`
 		} else if (mention === "terminal") {
 			return `Terminal Output (see below for output)`
+		} else if (mention.startsWith("codebase")) {
+			if (mention.includes(":")) {
+				const path = mention.slice(9)
+				return `As the first step, use the 'codebase_search' tool to search for relevant information needed for the task, using "${path}" as the search path.`
+			}
+			return "As the first step, use the 'codebase_search' tool to search for relevant information needed for the task."
+		} else if (mention.startsWith("summary")) {
+			if (mention.includes(":")) {
+				const path = mention.slice(8)
+				return `As the first step, use the 'codebase_search' tool to get a summary for '${path}'.`
+			}
+			return "As the first step, use the 'codebase_search' tool to get a summary of the relevant information needed for the task."
+		} else if (mention.startsWith("memory")) {
+			if (globalStoragePath) {
+				return "Memory (see below for stored memory)"
+			} else {
+				return "Memory (global storage path not available)"
+			}
 		}
 		return match
 	})
@@ -191,6 +304,23 @@ export async function parseMentions(
 			} catch (error) {
 				parsedText += `\n\n<terminal_output>\nError fetching terminal output: ${error.message}\n</terminal_output>`
 			}
+		} else if (mention.startsWith("codebase")) {
+			
+		} else if (mention.startsWith("summary")) {
+			
+		} else if (mention.startsWith("memory")) {
+			if (globalStoragePath) {
+				try {
+					const memoryFiles = await getMemoryFilePaths(globalStoragePath)
+					const memoryData = await readMemoryFiles(memoryFiles)
+					const formattedMemory = formatMemoryContent(memoryData)
+					parsedText += `\n\n<agent_memory_content>\n${formattedMemory}\n\n(If there are reminders or to-do items due, please notify the user.)\n</agent_memory_content>`
+				} catch (error) {
+					parsedText += `\n\n<agent_memory_content>\nError reading memory: ${error.message}\n</agent_memory_content>`
+				}
+			} else {
+				parsedText += `\n\n<agent_memory_content>\nError: Memory path not available\n</agent_memory_content>`
+			}
 		}
 	}
 
diff --git a/src/core/mentions/processUserContentMentions.ts b/src/core/mentions/processUserContentMentions.ts
index 3f131a1c..7f7556b7 100644
--- a/src/core/mentions/processUserContentMentions.ts
+++ b/src/core/mentions/processUserContentMentions.ts
@@ -13,6 +13,7 @@ export async function processUserContentMentions({
 	fileContextTracker,
 	rooIgnoreController,
 	showRooIgnoredFiles = true,
+	globalStoragePath
 }: {
 	userContent: Anthropic.Messages.ContentBlockParam[]
 	cwd: string
@@ -20,6 +21,7 @@ export async function processUserContentMentions({
 	fileContextTracker: FileContextTracker
 	rooIgnoreController?: any
 	showRooIgnoredFiles?: boolean
+	globalStoragePath?: string
 }) {
 	// Process userContent array, which contains various block types:
 	// TextBlockParam, ImageBlockParam, ToolUseBlockParam, and ToolResultBlockParam.
@@ -46,6 +48,7 @@ export async function processUserContentMentions({
 							fileContextTracker,
 							rooIgnoreController,
 							showRooIgnoredFiles,
+							globalStoragePath,
 						),
 					}
 				}
@@ -63,6 +66,7 @@ export async function processUserContentMentions({
 								fileContextTracker,
 								rooIgnoreController,
 								showRooIgnoredFiles,
+								globalStoragePath,
 							),
 						}
 					}
@@ -81,6 +85,7 @@ export async function processUserContentMentions({
 										fileContextTracker,
 										rooIgnoreController,
 										showRooIgnoredFiles,
+										globalStoragePath,
 									),
 								}
 							}
diff --git a/src/core/prompts/sections/markdown-formatting.ts b/src/core/prompts/sections/markdown-formatting.ts
index 87f922e9..ecfd3564 100644
--- a/src/core/prompts/sections/markdown-formatting.ts
+++ b/src/core/prompts/sections/markdown-formatting.ts
@@ -1,7 +1,17 @@
+// In all responses, any reference that requires specifying a location, such as guiding users to a certain file or a specific language structure within a file, or indicating that certain content is located at a specific position in a file, MUST be set as clickable MARKDOWN hyperlinks, such as guiding users to find a certain function within a file
+
 export function markdownFormattingSection(): string {
 	return `====
 
 MARKDOWN RULES
 
-ALL responses MUST show ANY \`language construct\` OR filename reference as clickable, exactly as [\`filename OR language.declaration()\`](relative/file/path.ext:line); line is required for \`syntax\` and optional for filename links. This applies to ALL markdown responses and ALSO those in <attempt_completion>`
+ALL responses MUST show ANY \`language construct\` OR filename reference as clickable, exactly as [\`filename OR language.declaration()\`](relative/file/path.ext:line); line is required for \`syntax\` and optional for filename links. This applies to ALL markdown responses and ALSO those in <attempt_completion>
+Format: [declaration](relative_path:line)
+Note:
+- 'declaration' is a clickable language structure or filename
+- 'relative_path' is a relative path relative to the current working path
+- 'line' is a number of line in the file.
+Example:
+[fibonacci()](src/test.py:12)
+`
 }
diff --git a/src/core/prompts/tools/codebase-search.ts b/src/core/prompts/tools/codebase-search.ts
index 0fc8f68f..6e0ec175 100644
--- a/src/core/prompts/tools/codebase-search.ts
+++ b/src/core/prompts/tools/codebase-search.ts
@@ -1,9 +1,21 @@
 export function getCodebaseSearchDescription(): string {
 	return `## codebase_search
-Description: Find files most relevant to the search query.\nThis is a semantic search tool, so the query should ask for something semantically matching what is needed.\nIf it makes sense to only search in a particular directory, please specify it in the path parameter.\nUnless there is a clear reason to use your own search query, please just reuse the user's exact query with their wording.\nTheir exact wording/phrasing can often be helpful for the semantic search query. Keeping the same exact question format can also be helpful.\nIMPORTANT: Queries MUST be in English. Translate non-English queries before searching.
+### codebase_search (Search)
+Description: This tool performs a semantic search on a vector database of code and documentation. It retrieves the most relevant contextual information needed to answer user questions or resolve their requirements. The search is based on semantic meaning, not just keyword matching.
+
+When generating a 'query', follow these guidelines:
+
+- **Extract from Code:** If the conversation includes code snippets, extract key identifiers like class names, function names, method names, or variable names. These are often the most crucial elements to search for to understand the code's purpose and functionality.
+- **Infer from Context:** Go beyond the literal words in the conversation.
+    - **For Code-related Questions:** Infer potential function names, class names, or design patterns that might exist in the codebase to solve the user's problem.
+    - **For Documentation-related Questions:** Infer concepts, features, or "how-to" topics that would likely be covered in the documentation.
+- **Be Specific and Clear:**
+    - Formulate clear, descriptive queries. Avoid using overly short or ambiguous abbreviations.
+    - If the context strongly suggests the information is in a specific location, use the 'path' parameter to narrow the search.
+
 Parameters:
-- query: (required) The search query to find relevant code. You should reuse the user's exact query/most recent message with their wording unless there is a clear reason not to.
-- path: (optional) The path to the directory to search in relative to the current working directory. This parameter should only be a directory path, file paths are not supported. Defaults to the current working directory.
+- query: (required) A semantic query (or queries) to find relevant code or documentation. You can provide up to 4 queries, separated by " | ". Each query should be a meaningful phrase (at least 4 Chinese characters or 2 English words). Provide queries in both Chinese and English. 
+- path: (optional) The relative path to a file or directory to restrict the search. Defaults to the entire codebase.
 Usage:
 <codebase_search>
 <query>Your natural language query here</query>
@@ -15,5 +27,29 @@ Example: Searching for functions related to user authentication
 <query>User login and password hashing</query>
 <path>/path/to/directory</path>
 </codebase_search>
+
+
+### codebase_search (Summary)
+Description: Generates a detailed summary of a file or a directory's contents.
+
+This tool provides a high-level overview to help you quickly understand a codebase.
+- **If the path points to a file:** It returns a summary of the entire file, plus summaries of key sections (e.g., classes, functions) with their corresponding line numbers.
+- **If the path points to a directory:** It returns summaries for all supported files within that directory.
+
+Use this tool when you need to grasp the purpose and structure of a file or directory before diving into the details.
+
+**Important Note:** The tool is named 'codebase_search', but its function in this parameters rule is to **summarize**, not to search for a query.
+
+Parameters:
+- path: (optional) The relative path to the file or directory to be summarized. Defaults to the current working directory ('.').
+Usage:
+<codebase_search>
+<path>Path to the directory or file to summarize (optional)</path>
+</codebase_search>
+
+Example: Get a summary of a specific file or all supported files in '/path/to/directory_or_file'.
+<codebase_search>
+<path>/path/to/directory_or_file</path>
+</codebase_search>
 `
 }
diff --git a/src/core/task/Task.ts b/src/core/task/Task.ts
index 31260cd6..20bcb3d6 100644
--- a/src/core/task/Task.ts
+++ b/src/core/task/Task.ts
@@ -1224,6 +1224,7 @@ export class Task extends EventEmitter<ClineEvents> {
 			fileContextTracker: this.fileContextTracker,
 			rooIgnoreController: this.rooIgnoreController,
 			showRooIgnoredFiles,
+			globalStoragePath: this.globalStoragePath,
 		})
 
 		const environmentDetails = await getEnvironmentDetails(this, includeFileDetails)
diff --git a/src/core/tools/applyDiffTool.ts b/src/core/tools/applyDiffTool.ts
index f5b4ab7d..a4f3d3e1 100644
--- a/src/core/tools/applyDiffTool.ts
+++ b/src/core/tools/applyDiffTool.ts
@@ -99,7 +99,7 @@ export async function applyDiffToolLegacy(
 			}
 
 			// Release the original content from memory as it's no longer needed
-			originalContent = null
+			// originalContent = null
 
 			if (!diffResult.success) {
 				cline.consecutiveMistakeCount++
@@ -172,6 +172,18 @@ export async function applyDiffToolLegacy(
 			// Call saveChanges to update the DiffViewProvider properties
 			await cline.diffViewProvider.saveChanges()
 
+			let newContent: string | null = await fs.readFile(absolutePath, "utf-8")
+
+			const agentEdits = formatResponse.createPrettyPatch(absolutePath, originalContent, newContent)
+			const say: ClineSayTool = {
+				tool: (!fileExists) ? "newFileCreated" : "editedExistingFile",
+				path: getReadablePath(cline.cwd, relPath),
+				diff: `# agentEdits\n${agentEdits}\n`,
+			}
+
+			// Send the user feedback
+			await cline.say("user_feedback_diff", JSON.stringify(say))
+
 			// Track file edit operation
 			if (relPath) {
 				await cline.fileContextTracker.trackFileContext(relPath, "roo_edited" as RecordSource)
diff --git a/src/core/tools/attemptCompletionTool.ts b/src/core/tools/attemptCompletionTool.ts
index 57f58700..cc5675f4 100644
--- a/src/core/tools/attemptCompletionTool.ts
+++ b/src/core/tools/attemptCompletionTool.ts
@@ -77,6 +77,8 @@ export async function attemptCompletionTool(
 				if (!didApprove) {
 					return
 				}
+				
+				// const summary = await getFinishSubTaskSummary(cline, result)
 
 				// tell the provider to remove the current subtask and resume the previous task in the stack
 				await cline.providerRef.deref()?.finishSubTask(result)
@@ -115,3 +117,133 @@ export async function attemptCompletionTool(
 		return
 	}
 }
+
+
+
+// import { ApiHandler, ApiHandlerCreateMessageMetadata, buildApiHandler } from "../../api"
+// import { truncateConversationIfNeeded } from "../sliding-window"
+// import { ApiMessage } from "../task-persistence/apiMessages"
+// import {
+// 	type ContextCondense,
+// } from "@roo-code/types"
+
+
+// async function getFinishSubTaskSummary(cline: Task, result:string) 
+// : Promise<string>
+// {
+// 	const state = await cline.providerRef.deref()?.getState()
+
+// 	const {
+// 		autoCondenseContext = true,
+// 		autoCondenseContextPercent = 100,
+// 		profileThresholds = {},
+// 	} = state ?? {}
+
+// 	const systemPrompt = `You are "CodeCrafter," an expert AI Programming Assistant and a master of code, logic, and software architecture. Your primary directive is to be an exceptionally helpful and proactive partner to users, assisting them in all aspects of the software development lifecycle. Your ultimate goal is to empower users to write better code, solve problems faster, and learn new technologies effectively.
+
+// **Core Responsibilities:**
+
+// *   **Code Generation & Completion:** Write clean, efficient, and well-documented code in any requested programming language.
+// *   **Explanation & Learning:** Explain complex code, algorithms, or programming concepts in a clear and easy-to-understand manner.
+// *   **Debugging & Troubleshooting:** Analyze code snippets or error messages to identify the root cause of bugs and propose effective solutions.
+// *   **Refactoring & Optimization:** Review existing code and suggest improvements for readability, performance, security, and maintainability.
+// *   **Architectural Design:** Provide high-level architectural suggestions, design patterns, and best practices for building robust and scalable applications.
+// *   **Testing:** Generate unit tests, suggest testing strategies, and help create a comprehensive test plan.
+
+// **Key Capability: Tool Utilization**
+
+// Beyond your extensive knowledge, you are equipped with a set of practical tools to interact with the user's development environment. You MUST use these tools when a task requires accessing or modifying local information.
+
+// **Available Tools:**
+// *   'file_reader': To read the content of one or more files.
+// *   'file_writer': To write new content to a file or create a new file.
+// *   'code_executor': To execute a piece of code and get its output, which is essential for verification and debugging.
+// *   'web_search': To find the most up-to-date information, library documentation, or solutions to novel errors.
+// ...
+
+// **Rules for Tool Use:**
+// 1.  **Analyze the Request:** First, determine if the user's request can be fulfilled with your internal knowledge or if it requires interacting with their files or system.
+// 2.  **Select the Right Tool:** If external interaction is needed, choose the appropriate tool.
+// *   *Example:* If a user says, "Fix the bug in my 'utils.py' file," your first step should be to use 'file_reader' to read 'utils.py'.
+// 3.  **Think Step-by-Step:** Formulate a plan. For a debugging task, this might be: read the file, identify the potential error, suggest a fix, and offer to write the corrected code back using 'file_writer'.
+// 4.  **Communicate Clearly:** Always inform the user which tool you are about to use and why. For example: "Okay, I will now use the 'file_reader' to examine the contents of 'utils.py' to understand the context of the bug."
+
+// **Guiding Principles:**
+// *   **Clarity First:** Prioritize clear, simple, and direct communication. Avoid jargon where possible.
+// *   **Best Practices:** Always adhere to industry best practices regarding coding standards, security, and project structure.
+// *   **Proactivity:** Don't just answer the question. If you see a potential improvement, a security vulnerability, or a better way to do something, proactively suggest it.
+// *   **Context-Awareness:** Maintain the context of the conversation to provide relevant and coherent support over multiple interactions.`
+// 	const customCondensingPrompt = state?.customCondensingPrompt
+// 	const condensingApiConfigId = state?.condensingApiConfigId
+// 	const listApiConfigMeta = state?.listApiConfigMeta
+
+// 	let condensingApiHandler: ApiHandler | undefined
+// 			if (condensingApiConfigId && listApiConfigMeta && Array.isArray(listApiConfigMeta)) {
+// 				// Using type assertion for the id property to avoid implicit any
+// 				const matchingConfig = listApiConfigMeta.find((config: any) => config.id === condensingApiConfigId)
+// 				if (matchingConfig) {
+// 					const profile = await cline.providerRef.deref()?.providerSettingsManager.getProfile({
+// 						id: condensingApiConfigId,
+// 					})
+// 					// Ensure profile and apiProvider exist before trying to build handler
+// 					if (profile && profile.apiProvider) {
+// 						condensingApiHandler = buildApiHandler(profile)
+// 					}
+// 				}
+// 			}
+	
+
+// 	const DEFAULT_THINKING_MODEL_MAX_TOKENS = 16_384
+// 	const modelInfo = cline.api.getModel().info
+// 	const { contextTokens } = cline.getTokenUsage()
+// 	const maxTokens = modelInfo.supportsReasoningBudget
+// 		? cline.apiConfiguration.modelMaxTokens || DEFAULT_THINKING_MODEL_MAX_TOKENS
+// 		: modelInfo.maxTokens || DEFAULT_THINKING_MODEL_MAX_TOKENS
+
+// 	const contextWindow = modelInfo.contextWindow
+
+// 	const currentProfileId =
+// 		state?.listApiConfigMeta.find((profile) => profile.name === state?.currentApiConfigName)?.id ??
+// 		"default"
+
+// 	if (contextTokens > 0.2 * maxTokens) {
+// 		const truncateResult = await truncateConversationIfNeeded({
+// 			messages: cline.apiConversationHistory,
+// 			totalTokens: contextTokens,
+// 			maxTokens,
+// 			contextWindow,
+// 			apiHandler: cline.api,
+// 			autoCondenseContext,
+// 			autoCondenseContextPercent,
+// 			systemPrompt,
+// 			taskId: cline.taskId,
+// 			customCondensingPrompt,
+// 			condensingApiHandler,
+// 			profileThresholds,
+// 			currentProfileId,
+// 		})
+
+// 		// ApiMessage[]
+// 		// const { summary, cost, prevContextTokens, newContextTokens = 0 } = truncateResult
+// 		// const contextCondense: ContextCondense = { summary, cost, newContextTokens, prevContextTokens }
+// 		// await cline.say(
+// 		// 	"condense_context",
+// 		// 	undefined /* text */,
+// 		// 	undefined /* images */,
+// 		// 	false /* partial */,
+// 		// 	undefined /* checkpoint */,
+// 		// 	undefined /* progressStatus */,
+// 		// 	{ isNonInteractive: true } /* options */,
+// 		// 	contextCondense,
+// 		// )
+
+// 		if (truncateResult.error) {
+// 			return truncateResult.summary
+// 		} 
+// 		return JSON.stringify(cline.apiConversationHistory)
+		
+// 	} else {
+// 		// 直接返回所有对话序列化组成的字符串
+// 		return JSON.stringify(cline.apiConversationHistory)
+// 	}
+// }
\ No newline at end of file
diff --git a/src/core/tools/codebaseSearchTool.ts b/src/core/tools/codebaseSearchTool.ts
index 236b0663..6b9a213c 100644
--- a/src/core/tools/codebaseSearchTool.ts
+++ b/src/core/tools/codebaseSearchTool.ts
@@ -29,6 +29,11 @@ export async function codebaseSearchTool(
 	let query: string | undefined = block.params.query
 	let directoryPrefix: string | undefined = block.params.path
 
+	if (query === undefined) {
+		await getSummary(cline, block, askApproval, handleError, pushToolResult, removeClosingTag)
+		return
+	}
+
 	query = removeClosingTag("query", query)
 
 	if (directoryPrefix) {
@@ -82,7 +87,7 @@ export async function codebaseSearchTool(
 			throw new Error("Code Indexing is not configured (Missing OpenAI Key or Qdrant URL).")
 		}
 
-		const searchResults: VectorStoreSearchResult[] = await manager.searchIndex(query, directoryPrefix)
+		const searchResults: string[] = await manager.searchIndex(query, directoryPrefix)
 
 		// 3. Format and push results
 		if (!searchResults || searchResults.length === 0) {
@@ -105,18 +110,18 @@ export async function codebaseSearchTool(
 		}
 
 		searchResults.forEach((result) => {
-			if (!result.payload) return
-			if (!("filePath" in result.payload)) return
-
-			const relativePath = vscode.workspace.asRelativePath(result.payload.filePath, false)
-
-			jsonResult.results.push({
-				filePath: relativePath,
-				score: result.score,
-				startLine: result.payload.startLine,
-				endLine: result.payload.endLine,
-				codeChunk: result.payload.codeChunk.trim(),
-			})
+			if (result) {
+				const res = JSON.parse(result) // Ensure the result is valid JSON
+				for (const key in res) {
+					jsonResult.results.push({
+						filePath: res[key]["file_path"],
+						score: res[key].score ?? 1,
+						startLine: Math.min(...res[key]["lines"]),
+						endLine: Math.max(...res[key]["lines"]),
+						codeChunk: res[key]["code"],
+					})
+				}
+			}
 		})
 
 		// Send results to UI
@@ -124,21 +129,135 @@ export async function codebaseSearchTool(
 		await cline.say("codebase_search_result", JSON.stringify(payload))
 
 		// Push results to AI
-		const output = `Query: ${query}
-Results:
-
-${jsonResult.results
-	.map(
-		(result) => `File path: ${result.filePath}
-Score: ${result.score}
-Lines: ${result.startLine}-${result.endLine}
-Code Chunk: ${result.codeChunk}
-`,
-	)
-	.join("\n")}`
+		const output = `${jsonResult.results.map(result => `# File: ${result.filePath}\n${result.codeChunk}`).join("\n\n")}`
 
 		pushToolResult(output)
 	} catch (error: any) {
 		await handleError(toolName, error) // Use the standard error handler
 	}
 }
+
+
+
+async function getSummary(
+	cline: Task,
+	block: ToolUse,
+	askApproval: AskApproval,
+	handleError: HandleError,
+	pushToolResult: PushToolResult,
+	removeClosingTag: RemoveClosingTag
+) {
+	const toolName = "codebase_search"
+	const workspacePath = getWorkspacePath()
+
+	if (!workspacePath) {
+		// This case should ideally not happen if Cline is initialized correctly
+		await handleError(toolName, new Error("Could not determine workspace path."))
+		return
+	}
+
+	// --- Parameter Extraction and Validation ---
+	let directoryPrefix: string | undefined = block.params.path
+	
+	if (directoryPrefix) {
+		directoryPrefix = removeClosingTag("path", directoryPrefix)
+		directoryPrefix = path.normalize(directoryPrefix)
+	} else {
+		directoryPrefix = '.'
+	}
+
+	const sharedMessageProps = {
+		tool: "codebaseSearch",
+		query: "（获取摘要）",
+		path: directoryPrefix,
+		isOutsideWorkspace: false,
+	}
+
+	if (block.partial) {
+		await cline.ask("tool", JSON.stringify(sharedMessageProps), block.partial).catch(() => {})
+		return
+	}
+
+	// if (!query) {
+	// 	cline.consecutiveMistakeCount++
+	// 	pushToolResult(await cline.sayAndCreateMissingParamError(toolName, "query"))
+	// 	return
+	// }
+
+	const didApprove = await askApproval("tool", JSON.stringify(sharedMessageProps))
+	if (!didApprove) {
+		pushToolResult(formatResponse.toolDenied())
+		return
+	}
+
+	cline.consecutiveMistakeCount = 0
+
+	// --- Core Logic ---
+	try {
+		const context = cline.providerRef.deref()?.context
+		if (!context) {
+			throw new Error("Extension context is not available.")
+		}
+
+		const manager = CodeIndexManager.getInstance(context)
+
+		if (!manager) {
+			throw new Error("CodeIndexManager is not available.")
+		}
+
+		if (!manager.isFeatureEnabled) {
+			throw new Error("Code Indexing is disabled in the settings.")
+		}
+		if (!manager.isFeatureConfigured) {
+			throw new Error("Code Indexing is not configured (Missing OpenAI Key or Qdrant URL).")
+		}
+
+		const summaryResults: string[] = await manager.searchSummary(directoryPrefix)
+
+		// 3. Format and push results
+		if (!summaryResults || summaryResults.length === 0) {
+			pushToolResult(`No summary found in path: "${directoryPrefix}"`) // Use simple string for no results
+			return
+		}
+
+		const jsonResult = {
+			query: "（获取摘要）",
+			results: [],
+		} as {
+			query: string
+			results: Array<{
+				filePath: string
+				score: number
+				startLine: number
+				endLine: number
+				codeChunk: string[]
+			}>
+		}
+
+		summaryResults.forEach((result) => {
+			if (result) {
+				const res = JSON.parse(result) // Ensure the result is valid JSON
+				for (const key in res) {
+					jsonResult.results.push({
+						filePath: res[key]["file_path"],
+						score: 1,
+						startLine: 0,
+						endLine: 0,
+						codeChunk: res[key]["code"],
+					})
+				}
+			}
+		})
+
+		// Send results to UI
+		const payload = { tool: "codebaseSearch", content: jsonResult }
+		await cline.say("codebase_search_result", JSON.stringify(payload))
+
+		// Push results to AI
+		const output = `# Codebase summary in ${directoryPrefix}:\n\n${jsonResult.results.map(result => `## File: ${result.filePath}\n${result.codeChunk.join("\n")}`).join("\n\n")}`
+
+		pushToolResult(output)
+	} catch (error: any) {
+		await handleError(toolName, error) // Use the standard error handler
+	}
+}
\ No newline at end of file
diff --git a/src/core/tools/multiApplyDiffTool.ts b/src/core/tools/multiApplyDiffTool.ts
index 8057f779..5b8bc5cc 100644
--- a/src/core/tools/multiApplyDiffTool.ts
+++ b/src/core/tools/multiApplyDiffTool.ts
@@ -425,7 +425,7 @@ Original error: ${errorMessage}`
 				}
 
 				// Release the original content from memory as it's no longer needed
-				originalContent = null
+				// originalContent = null
 
 				if (!diffResult.success) {
 					cline.consecutiveMistakeCount++
@@ -555,6 +555,20 @@ ${errorDetails ? `\nTechnical details:\n${errorDetails}\n` : ""}
 				// Call saveChanges to update the DiffViewProvider properties
 				await cline.diffViewProvider.saveChanges()
 
+				
+				let newContent: string | null = await fs.readFile(absolutePath, "utf-8")
+				
+				const agentEdits = formatResponse.createPrettyPatch(absolutePath, originalContent, newContent)
+				const say: ClineSayTool = {
+					tool: (!fileExists) ? "newFileCreated" : "editedExistingFile",
+					path: getReadablePath(cline.cwd, relPath),
+					diff: `# agentEdits\n${agentEdits}\n`,
+				}
+	
+				// Send the user feedback
+				await cline.say("user_feedback_diff", JSON.stringify(say))
+				
+
 				// Track file edit operation
 				await cline.fileContextTracker.trackFileContext(relPath, "roo_edited" as RecordSource)
 
diff --git a/src/core/tools/readFileTool.ts b/src/core/tools/readFileTool.ts
index 1459838f..293a0921 100644
--- a/src/core/tools/readFileTool.ts
+++ b/src/core/tools/readFileTool.ts
@@ -500,7 +500,7 @@ export async function readFileTool(
 						if (defResult) {
 							xmlInfo += `<list_code_definition_names>${defResult}</list_code_definition_names>\n`
 						}
-						xmlInfo += `<notice>Showing only ${maxReadFileLine} of ${totalLines} total lines. Use line_range if you need to read more lines</notice>\n`
+						xmlInfo += `<notice>Showing only ${maxReadFileLine} of ${totalLines} total lines. Use line_range if you need to read more lines. Read the specified line number range to learn more details you want to know</notice>\n`
 						updateFileResult(relPath, {
 							xmlContent: `<file><path>${relPath}</path>\n${xmlInfo}</file>`,
 						})
diff --git a/src/core/webview/ClineProvider.ts b/src/core/webview/ClineProvider.ts
index 4a934e9f..bc11ad60 100644
--- a/src/core/webview/ClineProvider.ts
+++ b/src/core/webview/ClineProvider.ts
@@ -1186,7 +1186,7 @@ export class ClineProvider
 		await this.postMessageToWebview({ type: "condenseTaskContextResponse", text: taskId })
 	}
 
-	// this function deletes a task from task hidtory, and deletes it's checkpoints and delete the task folder
+	// this function deletes a task from task history, and deletes it's checkpoints and delete the task folder
 	async deleteTaskWithId(id: string) {
 		try {
 			// get the task directory full path
@@ -1515,6 +1515,15 @@ export class ClineProvider
 				codebaseIndexEmbedderProvider: "openai",
 				codebaseIndexEmbedderBaseUrl: "",
 				codebaseIndexEmbedderModelId: "",
+
+				embeddingBaseUrl: "",
+				embeddingModelID: "",
+				enhancementBaseUrl: "",
+				enhancementModelID: "",
+
+				ragPath: "",
+				llmFilter: false,
+				codeBaseLogging: false,
 			},
 			mdmCompliant: this.checkMdmCompliance(),
 			profileThresholds: profileThresholds ?? {},
@@ -1673,6 +1682,15 @@ export class ClineProvider
 				codebaseIndexEmbedderProvider: "openai",
 				codebaseIndexEmbedderBaseUrl: "",
 				codebaseIndexEmbedderModelId: "",
+				
+				embeddingBaseUrl: "",
+				embeddingModelID: "",
+				enhancementBaseUrl: "",
+				enhancementModelID: "",
+				
+				ragPath: "",
+				llmFilter: false,
+				codeBaseLogging: false,
 			},
 			profileThresholds: stateValues.profileThresholds ?? {},
 		}
diff --git a/src/core/webview/webviewMessageHandler.ts b/src/core/webview/webviewMessageHandler.ts
index a6577fb2..65dc6a7f 100644
--- a/src/core/webview/webviewMessageHandler.ts
+++ b/src/core/webview/webviewMessageHandler.ts
@@ -50,6 +50,8 @@ import { GetModelsOptions } from "../../shared/api"
 import { generateSystemPrompt } from "./generateSystemPrompt"
 import { getCommand } from "../../utils/commands"
 
+import { saveMemory } from "./Memory-rid"
+
 const ALLOWED_VSCODE_SETTINGS = new Set(["terminal.integrated.inheritEnv"])
 
 import { MarketplaceManager, MarketplaceItemType } from "../../services/marketplace"
@@ -1284,6 +1286,10 @@ export const webviewMessageHandler = async (
 			await updateGlobalState("autoApprovalEnabled", message.bool ?? false)
 			await provider.postStateToWebview()
 			break
+		case "saveMemory":
+			// 调用保存记忆函数，函数内部会发送相应的消息
+			await saveMemory(provider, message.text??"")
+			break
 		case "enhancePrompt":
 			if (message.text) {
 				try {
@@ -1947,6 +1953,17 @@ export const webviewMessageHandler = async (
 					codebaseIndexOpenAiCompatibleBaseUrl: settings.codebaseIndexOpenAiCompatibleBaseUrl,
 					codebaseIndexSearchMaxResults: settings.codebaseIndexSearchMaxResults,
 					codebaseIndexSearchMinScore: settings.codebaseIndexSearchMinScore,
+
+					embeddingBaseUrl: settings.embeddingBaseUrl,
+					embeddingModelID: settings.embeddingModelID,
+					enhancementBaseUrl: settings.enhancementBaseUrl,
+					enhancementModelID: settings.enhancementModelID,
+					// embeddingApiKey: settings.embeddingApiKey,
+					// enhancementApiKey: settings.enhancementApiKey,
+
+					ragPath: settings.ragPath,
+					llmFilter: settings.llmFilter,
+					codeBaseLogging: settings.codeBaseLogging,
 				}
 
 				// Save global state first
@@ -1972,6 +1989,19 @@ export const webviewMessageHandler = async (
 					)
 				}
 
+				if (settings.codeIndexOpenAiKey !== undefined) {
+					await provider.contextProxy.storeSecret("codeIndexOpenAiKey", settings.codeIndexOpenAiKey)
+				}
+
+				
+				if (settings.embeddingApiKey !== undefined) {
+					await provider.contextProxy.storeSecret("embeddingApiKey", settings.embeddingApiKey)
+				}
+				if (settings.enhancementApiKey !== undefined) {
+					await provider.contextProxy.storeSecret("enhancementApiKey", settings.enhancementApiKey)
+				}
+				
+
 				// Send success response first - settings are saved regardless of validation
 				await provider.postMessageToWebview({
 					type: "codeIndexSettingsSaved",
@@ -2063,6 +2093,8 @@ export const webviewMessageHandler = async (
 				"codebaseIndexOpenAiCompatibleApiKey",
 			))
 			const hasGeminiApiKey = !!(await provider.context.secrets.get("codebaseIndexGeminiApiKey"))
+			const hasEmbeddingApiKey = !!(await provider.context.secrets.get("embeddingApiKey"))
+			const hasEnhancementApiKey = !!(await provider.context.secrets.get("enhancementApiKey"))
 
 			provider.postMessageToWebview({
 				type: "codeIndexSecretStatus",
@@ -2071,6 +2103,8 @@ export const webviewMessageHandler = async (
 					hasQdrantApiKey,
 					hasOpenAiCompatibleApiKey,
 					hasGeminiApiKey,
+					hasEmbeddingApiKey,
+					hasEnhancementApiKey
 				},
 			})
 			break
diff --git a/src/services/code-index/config-manager.ts b/src/services/code-index/config-manager.ts
index 245621a1..02385051 100644
--- a/src/services/code-index/config-manager.ts
+++ b/src/services/code-index/config-manager.ts
@@ -22,6 +22,12 @@ export class CodeIndexConfigManager {
 	private searchMinScore?: number
 	private searchMaxResults?: number
 
+	private embeddingOptions?: { baseUrl: string; apiKey: string; modelID: string }
+	private enhancementOptions?: { baseUrl: string; apiKey: string; modelID: string }
+	private ragPath?: string
+	private llmFilter?: boolean
+	private codeBaseLogging?: boolean
+
 	constructor(private readonly contextProxy: ContextProxy) {
 		// Initialize with current configuration to avoid false restart triggers
 		this._loadAndSetConfiguration()
@@ -48,6 +54,14 @@ export class CodeIndexConfigManager {
 			codebaseIndexEmbedderModelId: "",
 			codebaseIndexSearchMinScore: undefined,
 			codebaseIndexSearchMaxResults: undefined,
+
+			embeddingBaseUrl: "",
+			embeddingModelID: "",
+			enhancementBaseUrl: "",
+			enhancementModelID: "",
+			ragPath: "",
+			llmFilter: false,
+			codeBaseLogging: false,
 		}
 
 		const {
@@ -58,6 +72,14 @@ export class CodeIndexConfigManager {
 			codebaseIndexEmbedderModelId,
 			codebaseIndexSearchMinScore,
 			codebaseIndexSearchMaxResults,
+
+			embeddingBaseUrl,
+			embeddingModelID,
+			enhancementBaseUrl,
+			enhancementModelID,
+			ragPath,
+			llmFilter,
+			codeBaseLogging,
 		} = codebaseIndexConfig
 
 		const openAiKey = this.contextProxy?.getSecret("codeIndexOpenAiKey") ?? ""
@@ -67,6 +89,10 @@ export class CodeIndexConfigManager {
 		const openAiCompatibleApiKey = this.contextProxy?.getSecret("codebaseIndexOpenAiCompatibleApiKey") ?? ""
 		const geminiApiKey = this.contextProxy?.getSecret("codebaseIndexGeminiApiKey") ?? ""
 
+		const embeddingApiKey = this.contextProxy?.getSecret("embeddingApiKey") ?? ""
+		const enhancementApiKey = this.contextProxy?.getSecret("enhancementApiKey") ?? ""
+
+
 		// Update instance variables with configuration
 		// Note: codebaseIndexEnabled is no longer used as the feature is always enabled
 		this.qdrantUrl = codebaseIndexQdrantUrl
@@ -118,6 +144,30 @@ export class CodeIndexConfigManager {
 				: undefined
 
 		this.geminiOptions = geminiApiKey ? { apiKey: geminiApiKey } : undefined
+
+		this.embeddingOptions =
+			embeddingBaseUrl && embeddingApiKey && embeddingModelID
+				? {
+						baseUrl: embeddingBaseUrl,
+						apiKey: embeddingApiKey,
+						modelID: embeddingModelID,
+					}
+				: undefined
+
+		this.enhancementOptions =
+			enhancementBaseUrl && enhancementApiKey && enhancementModelID
+				? {
+						baseUrl: enhancementBaseUrl,
+						apiKey: enhancementApiKey,
+						modelID: enhancementModelID,
+					}
+				: undefined
+
+
+		this.ragPath = ragPath ? ragPath : undefined
+		this.llmFilter = llmFilter ? llmFilter : false
+		this.codeBaseLogging = codeBaseLogging ? codeBaseLogging : false
+ 	
 	}
 
 	/**
@@ -154,6 +204,18 @@ export class CodeIndexConfigManager {
 			geminiApiKey: this.geminiOptions?.apiKey ?? "",
 			qdrantUrl: this.qdrantUrl ?? "",
 			qdrantApiKey: this.qdrantApiKey ?? "",
+
+			embeddingApiKey: this.embeddingOptions?.apiKey ?? "",
+			embeddingBaseUrl: this.embeddingOptions?.baseUrl ?? "",
+			embeddingModelID: this.embeddingOptions?.modelID ?? "",
+
+			enhancementApiKey: this.enhancementOptions?.apiKey ?? "",
+			enhancementBaseUrl: this.enhancementOptions?.baseUrl ?? "",
+			enhancementModelID: this.enhancementOptions?.modelID ?? "",
+
+			ragPath: this.ragPath ?? "",
+			llmFilter: this.llmFilter ?? false,
+			codeBaseLogging: this.codeBaseLogging ?? false
 		}
 
 		// Refresh secrets from VSCode storage to ensure we have the latest values
@@ -197,10 +259,10 @@ export class CodeIndexConfigManager {
 			const qdrantUrl = this.qdrantUrl
 			return !!(ollamaBaseUrl && qdrantUrl)
 		} else if (this.embedderProvider === "openai-compatible") {
-			const baseUrl = this.openAiCompatibleOptions?.baseUrl
-			const apiKey = this.openAiCompatibleOptions?.apiKey
-			const qdrantUrl = this.qdrantUrl
-			const isConfigured = !!(baseUrl && apiKey && qdrantUrl)
+			const baseUrl = this.embeddingOptions?.baseUrl
+			const apiKey = this.embeddingOptions?.apiKey
+			const modelID = this.embeddingOptions?.modelID
+			const isConfigured = !!(baseUrl && apiKey && modelID)
 			return isConfigured
 		} else if (this.embedderProvider === "gemini") {
 			const apiKey = this.geminiOptions?.apiKey
@@ -243,6 +305,18 @@ export class CodeIndexConfigManager {
 		const prevQdrantUrl = prev?.qdrantUrl ?? ""
 		const prevQdrantApiKey = prev?.qdrantApiKey ?? ""
 
+		const prevembeddingApiKey = prev?.embeddingApiKey ?? ""
+		const prevembeddingBaseUrl = prev?.embeddingBaseUrl ?? ""
+		const prevembeddingModelID = prev?.embeddingModelID ?? ""
+
+		const prevenhancementApiKey = prev?.enhancementApiKey ?? ""
+		const prevenhancementBaseUrl = prev?.enhancementBaseUrl ?? ""
+		const prevenhancementModelID = prev?.enhancementModelID ?? ""
+
+		const prevragPath = prev?.ragPath ?? ""
+		const prevcodeBaseLogging = prev?.codeBaseLogging ?? false
+
+
 		// 1. Transition from unconfigured to configured
 		// Since the feature is always enabled, we only check configuration status
 		if (!prevConfigured && nowConfigured) {
@@ -301,6 +375,48 @@ export class CodeIndexConfigManager {
 			return true
 		}
 
+		// Enhancement configuration changes
+		const currentEmbeddingApiKey = this.embeddingOptions?.apiKey ?? ""
+		const currentEmbeddingBaseUrl = this.embeddingOptions?.baseUrl ?? ""
+		const currentEmbeddingModelID = this.embeddingOptions?.modelID ?? ""
+
+		if (
+			prevembeddingApiKey !== currentEmbeddingApiKey ||
+			prevembeddingBaseUrl !== currentEmbeddingBaseUrl ||
+			prevembeddingModelID !== currentEmbeddingModelID
+		) {
+			return true
+		}
+
+		// Enhancement configuration changes
+		const currentEnhancementApiKey = this.enhancementOptions?.apiKey ?? ""
+		const currentEnhancementBaseUrl = this.enhancementOptions?.baseUrl ?? ""
+		const currentEnhancementModelID = this.enhancementOptions?.modelID ?? ""
+		
+
+		if (
+			prevenhancementApiKey !== currentEnhancementApiKey ||
+			prevenhancementBaseUrl !== currentEnhancementBaseUrl ||
+			prevenhancementModelID !== currentEnhancementModelID
+		) {
+			return true
+		}
+
+
+		const currentRagPath = this.ragPath ?? ""
+		if (
+			prevragPath !== currentRagPath
+		) {
+			return true
+		}
+
+		const currentCodeBaseLogging = this.codeBaseLogging ?? false
+		if (
+			prevcodeBaseLogging !== currentCodeBaseLogging
+		) {
+			return true
+		}
+
 		return false
 	}
 
@@ -347,6 +463,13 @@ export class CodeIndexConfigManager {
 			qdrantApiKey: this.qdrantApiKey,
 			searchMinScore: this.currentSearchMinScore,
 			searchMaxResults: this.currentSearchMaxResults,
+
+			embeddingOptions: this.embeddingOptions,
+			enhancementOptions: this.enhancementOptions,
+
+			ragPath: this.ragPath,
+			llmFilter: this.llmFilter,
+			codeBaseLogging: this.codeBaseLogging,
 		}
 	}
 
diff --git a/src/services/code-index/interfaces/config.ts b/src/services/code-index/interfaces/config.ts
index 190a23e2..9a7cc4f5 100644
--- a/src/services/code-index/interfaces/config.ts
+++ b/src/services/code-index/interfaces/config.ts
@@ -17,6 +17,13 @@ export interface CodeIndexConfig {
 	qdrantApiKey?: string
 	searchMinScore?: number
 	searchMaxResults?: number
+
+	embeddingOptions?: { baseUrl: string; apiKey: string; modelID: string }
+	enhancementOptions?: { baseUrl: string; apiKey: string; modelID: string }
+
+	ragPath?: string
+	llmFilter?: boolean
+	codeBaseLogging?: boolean
 }
 
 /**
@@ -35,4 +42,16 @@ export type PreviousConfigSnapshot = {
 	geminiApiKey?: string
 	qdrantUrl?: string
 	qdrantApiKey?: string
+
+	embeddingBaseUrl?: string
+	embeddingApiKey?: string
+	embeddingModelID?: string
+
+	enhancementBaseUrl?: string
+	enhancementApiKey?: string
+	enhancementModelID?: string
+
+	ragPath?: string
+	llmFilter?: boolean
+	codeBaseLogging?: boolean
 }
diff --git a/src/services/code-index/manager.ts b/src/services/code-index/manager.ts
index bd782da8..ebba7521 100644
--- a/src/services/code-index/manager.ts
+++ b/src/services/code-index/manager.ts
@@ -12,7 +12,13 @@ import { CacheManager } from "./cache-manager"
 import fs from "fs/promises"
 import ignore from "ignore"
 import path from "path"
-import { t } from "../../i18n"
+import { z } from "zod"
+import { Client } from "@modelcontextprotocol/sdk/client/index.js"
+import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js"
+import { CallToolResultSchema } from "@modelcontextprotocol/sdk/types.js"
+import { testEmbeddingApiAvailable, testOpenAIApiAvailable } from "./manager-test-rid"
+
+
 
 export class CodeIndexManager {
 	// --- Singleton Implementation ---
@@ -21,23 +27,19 @@ export class CodeIndexManager {
 	// Specialized class instances
 	private _configManager: CodeIndexConfigManager | undefined
 	private readonly _stateManager: CodeIndexStateManager
-	private _serviceFactory: CodeIndexServiceFactory | undefined
-	private _orchestrator: CodeIndexOrchestrator | undefined
-	private _searchService: CodeIndexSearchService | undefined
-	private _cacheManager: CacheManager | undefined
+
+	private _mcpClient : Client | undefined
+
+	private _isEnhancementEnabled: boolean = false
+	private _isEmbeddingEnabled: boolean = false
 
 	public static getInstance(context: vscode.ExtensionContext): CodeIndexManager | undefined {
-		// Use first workspace folder consistently
-		const workspaceFolders = vscode.workspace.workspaceFolders
-		if (!workspaceFolders || workspaceFolders.length === 0) {
+		const workspacePath = getWorkspacePath() // Assumes single workspace for now
+
+		if (!workspacePath) {
 			return undefined
 		}
 
-		// Always use the first workspace folder for consistency across all indexing operations.
-		// This ensures that the same workspace context is used throughout the indexing pipeline,
-		// preventing path resolution errors in multi-workspace scenarios.
-		const workspacePath = workspaceFolders[0].uri.fsPath
-
 		if (!CodeIndexManager.instances.has(workspacePath)) {
 			CodeIndexManager.instances.set(workspacePath, new CodeIndexManager(workspacePath, context))
 		}
@@ -68,26 +70,15 @@ export class CodeIndexManager {
 	}
 
 	private assertInitialized() {
-		if (!this._configManager || !this._orchestrator || !this._searchService || !this._cacheManager) {
+		if (!this._configManager) {
 			throw new Error("CodeIndexManager not initialized. Call initialize() first.")
 		}
 	}
 
 	public get state(): IndexingState {
-		if (!this.isFeatureEnabled) {
-			return "Standby"
-		}
-		this.assertInitialized()
-		return this._orchestrator!.state
+		return this._stateManager.state
 	}
 
-	public get isFeatureEnabled(): boolean {
-		return this._configManager?.isFeatureEnabled ?? false
-	}
-
-	public get isFeatureConfigured(): boolean {
-		return this._configManager?.isFeatureConfigured ?? false
-	}
 
 	public get isInitialized(): boolean {
 		try {
@@ -98,6 +89,14 @@ export class CodeIndexManager {
 		}
 	}
 
+	public get isFeatureEnabled(): boolean {
+		return this._configManager?.isFeatureEnabled ?? false
+	}
+
+	public get isFeatureConfigured(): boolean {
+		return this._configManager?.isFeatureConfigured ?? false
+	}
+
 	/**
 	 * Initializes the manager with configuration and dependent services.
 	 * Must be called before using any other methods.
@@ -111,46 +110,111 @@ export class CodeIndexManager {
 		// Load configuration once to get current state and restart requirements
 		const { requiresRestart } = await this._configManager.loadConfiguration()
 
-		// 2. Check if feature is enabled
-		if (!this.isFeatureEnabled) {
-			if (this._orchestrator) {
-				this._orchestrator.stopWatcher()
-			}
-			return { requiresRestart }
-		}
+		this._stateManager.setSystemState("Indexing", "Checking configuration.")
 
-		// 3. Check if workspace is available
-		const workspacePath = getWorkspacePath()
-		if (!workspacePath) {
-			this._stateManager.setSystemState("Standby", "No workspace folder open")
-			return { requiresRestart }
-		}
+		// 2. 创建一个独立的 MCP 客户端（不依赖 McpHub）
+		// 这里以 code_context 配置为例，实际可根据需要动态生成
+		const config = this._configManager.getConfig()
+
+		// --- 三个服务可用性校验并发执行 ---
+		let enhancementPromise: Promise<boolean> = Promise.resolve(false)
+		let embeddingPromise: Promise<boolean> = Promise.resolve(false)
 
-		// 4. CacheManager Initialization
-		if (!this._cacheManager) {
-			this._cacheManager = new CacheManager(this.context, this.workspacePath)
-			await this._cacheManager.initialize()
+		if (config.enhancementOptions && config.enhancementOptions.baseUrl) {
+			enhancementPromise = testOpenAIApiAvailable({
+				apiKey: config.enhancementOptions?.apiKey || "",
+				model: config.enhancementOptions?.modelID || "",
+				baseUrl: config.enhancementOptions?.baseUrl || ""
+			})
 		}
+		if (config.embeddingOptions && config.embeddingOptions.baseUrl) {
+			embeddingPromise = testEmbeddingApiAvailable({
+				apiKey: config.embeddingOptions?.apiKey || "",
+				model: config.embeddingOptions?.modelID || "",
+				baseUrl: config.embeddingOptions?.baseUrl || ""
+			})
+		}
+
+		// 并发等待所有校验
+		const [enhancementEnabled, embeddingEnabled] = await Promise.all([
+			enhancementPromise,
+			embeddingPromise,
+		])
+		this._isEnhancementEnabled = enhancementEnabled
+		this._isEmbeddingEnabled = embeddingEnabled
+
+		this.startIndexing()
 
-		// 4. Determine if Core Services Need Recreation
-		const needsServiceRecreation = !this._serviceFactory || requiresRestart
+		return { requiresRestart }
+	}
 
-		if (needsServiceRecreation) {
-			await this._recreateServices()
+	public async _startIndexing(): Promise<void> {
+		if (!this._configManager) {
+			this._stateManager.setSystemState("Error", "Config error.")
+			return
 		}
 
-		// 5. Handle Indexing Start/Restart
-		// The enhanced vectorStore.initialize() in startIndexing() now handles dimension changes automatically
-		// by detecting incompatible collections and recreating them, so we rely on that for dimension changes
-		const shouldStartOrRestartIndexing =
-			requiresRestart ||
-			(needsServiceRecreation && (!this._orchestrator || this._orchestrator.state !== "Indexing"))
+		if (this._mcpClient !== undefined) {
+			this._stateManager.setSystemState("Indexed", `Codebase client initialized successfully. \n  ✔ Embedding service enabled.\n  ${this._isEnhancementEnabled? "✔ Enhancement service enabled.\n" : ""}`)
+			return
+		}
 
-		if (shouldStartOrRestartIndexing) {
-			this._orchestrator?.startIndexing() // This method is async, but we don't await it here
+		const config = this._configManager.getConfig()
+		const args = []
+		args.push("-m", "code_context_mcp")
+
+		if (!this._isEmbeddingEnabled) {
+			this._stateManager.setSystemState("Error", "Embedding service is not enabled.")
+		} else {
+			// 检查并创建.roo目录
+			const rooDir = path.join(this.workspacePath, ".roo")
+			try {
+				await fs.access(rooDir)
+			} catch {
+				await fs.mkdir(rooDir, { recursive: true })
+			}
+		}
+		if (config.enhancementOptions && this._isEnhancementEnabled) {
+			args.push("--is-enhancement")
+			args.push("--enhancement-key", config.enhancementOptions.apiKey || "key")
+			args.push("--enhancement-model", config.enhancementOptions.modelID || "qwq-32b")
+			args.push("--enhancement-url", config.enhancementOptions.baseUrl || "http://10.12.154.110:7000/v1")
+		}
+		if (config.embeddingOptions && this._isEmbeddingEnabled) {
+			args.push("--embedding-key", config.embeddingOptions.apiKey || "key")
+			args.push("--embedding-model", config.embeddingOptions.modelID || "BAAI/bge-m3")
+			args.push("--embedding-url", config.embeddingOptions.baseUrl || "http://localhost:6123/embedding/v1")
+		}
+		if (config.ragPath) {
+			args.push("--rag-path", config.ragPath)
+		}
+		if (config.codeBaseLogging) {
+			args.push("--log")
 		}
 
-		return { requiresRestart }
+		const mcpConfig = {
+			command: "python",
+			args
+		}
+		const transport = new StdioClientTransport({
+			command: mcpConfig.command,
+			args: mcpConfig.args,
+			// env: mcpConfig.env,
+			cwd: this.workspacePath,
+			stderr: "pipe",
+		})
+		try {
+			const client = new Client({ name: "CodeIndexManager", version: "0.1.0" }, { capabilities: {} })
+			// await transport.start()
+			await client.connect(transport)
+			// 你可以将 client 实例保存到 this._mcpClient 以便后续调用
+			this._mcpClient = client
+			this._stateManager.setSystemState("Indexed", `Codebase client initialized successfully. \n  ✔ Embedding service enabled.\n  ${this._isEnhancementEnabled? "✔ Enhancement service enabled.\n" : ""}`)
+		} catch (error) {
+			console.error("[CodeIndexManager] Failed to initialize MCP client:", error)
+			this._stateManager.setSystemState("Error", `Codebase client initialization failed. ${error}`)
+			// throw new Error("Failed to initialize MCP client")
+		}
 	}
 
 	/**
@@ -162,18 +226,27 @@ export class CodeIndexManager {
 			return
 		}
 		this.assertInitialized()
-		await this._orchestrator!.startIndexing()
+		if (!this.isFeatureConfigured) {
+			this._stateManager.setSystemState("Standby", "Missing configuration. Save your settings to start indexing.")
+			console.warn("[CodeIndexOrchestrator] Start rejected: Missing configuration.")
+			return
+		}
+		this._stateManager.setSystemState("Indexing", "Start Indexing...")
+		this._startIndexing()
+		// this._stateManager.setSystemState("Indexing", "Initializing services...")
 	}
 
 	/**
 	 * Stops the file watcher and potentially cleans up resources.
 	 */
 	public stopWatcher(): void {
-		if (!this.isFeatureEnabled) {
-			return
-		}
-		if (this._orchestrator) {
-			this._orchestrator.stopWatcher()
+		// if (!this.isFeatureEnabled) {
+		// 	return
+		// }
+		this._stateManager.setSystemState("Standby", "File watcher stopped.")
+		if (this._mcpClient) {
+			this._mcpClient.close() // Disconnect the MCP client
+			this._mcpClient = undefined // Clear the client reference
 		}
 	}
 
@@ -181,9 +254,7 @@ export class CodeIndexManager {
 	 * Cleans up the manager instance.
 	 */
 	public dispose(): void {
-		if (this._orchestrator) {
-			this.stopWatcher()
-		}
+		this.stopWatcher()
 		this._stateManager.dispose()
 	}
 
@@ -196,8 +267,41 @@ export class CodeIndexManager {
 			return
 		}
 		this.assertInitialized()
-		await this._orchestrator!.clearIndexData()
-		await this._cacheManager!.clearCacheFile()
+
+		if (!this._mcpClient) {
+			throw new Error("MCP client not initialized")
+		}
+
+		try {
+			const response = await this._mcpClient.request(
+				{
+					method: "tools/call",
+					params: {
+						name: "delete_index",
+					}
+				},
+				CallToolResultSchema
+			)
+
+			const results: string[] = []
+			if (response && Array.isArray(response.content)) {
+				for (const item of response.content) {
+					if (item && typeof item.text === "string") {
+						results.push(item.text)
+					}
+				}
+			}
+
+			if (response.isError) {
+				console.error("[CodeIndexManager] MCP delete_index error:" + results.join(", "))
+				throw new Error("MCP delete_index returned an error:" + results.join(", "))
+			}
+		} catch (error) {
+			console.error("[CodeIndexManager] searchSummary exception:", error)
+			throw error
+		}
+
+		await this.stopWatcher()
 	}
 
 	// --- Private Helpers ---
@@ -206,95 +310,113 @@ export class CodeIndexManager {
 		return this._stateManager.getCurrentStatus()
 	}
 
-	public async searchIndex(query: string, directoryPrefix?: string): Promise<VectorStoreSearchResult[]> {
-		if (!this.isFeatureEnabled) {
-			return []
+	public async searchIndex(query: string, directoryPrefix?: string): Promise<string[]> {
+		if (!this._mcpClient) {
+			throw new Error("MCP client not initialized")
 		}
-		this.assertInitialized()
-		return this._searchService!.searchIndex(query, directoryPrefix)
-	}
 
-	/**
-	 * Private helper method to recreate services with current configuration.
-	 * Used by both initialize() and handleSettingsChange().
-	 */
-	private async _recreateServices(): Promise<void> {
-		// Stop watcher if it exists
-		if (this._orchestrator) {
-			this.stopWatcher()
+		
+		if (!this._configManager) {
+			this._stateManager.setSystemState("Error", "Config error.")
+			throw new Error("Config error.")
 		}
-		// Clear existing services to ensure clean state
-		this._orchestrator = undefined
-		this._searchService = undefined
 
-		// (Re)Initialize service factory
-		this._serviceFactory = new CodeIndexServiceFactory(
-			this._configManager!,
-			this.workspacePath,
-			this._cacheManager!,
-		)
+		const config = this._configManager.getConfig()
+
+		
+		const params: Record<string, any> = { 
+			queries: query.split("|").map(q => q.trim()), 
+			json_format: true, 
+		}
 
-		const ignoreInstance = ignore()
-		const workspacePath = getWorkspacePath()
+		if (config.llmFilter) {
+			params.llm_filter = config.llmFilter
+		}
 
-		if (!workspacePath) {
-			this._stateManager.setSystemState("Standby", "")
-			return
+		if (directoryPrefix) {
+			params.paths = [directoryPrefix]
 		}
 
-		const ignorePath = path.join(workspacePath, ".gitignore")
+		const minScore = this._configManager.currentSearchMinScore
+		const maxResults = this._configManager.currentSearchMaxResults
+
+		params.n_results = maxResults
+		params.threshold = minScore
+
 		try {
-			const content = await fs.readFile(ignorePath, "utf8")
-			ignoreInstance.add(content)
-			ignoreInstance.add(".gitignore")
+			const response = await this._mcpClient.request(
+				{
+					method: "tools/call",
+					params: {
+						name: "search_code",
+						arguments: params
+					}
+				},
+				CallToolResultSchema
+			)
+
+			const results: string[] = []
+			if (response && Array.isArray(response.content)) {
+				for (const item of response.content) {
+					if (item && typeof item.text === "string") {
+						results.push(item.text)
+					}
+				}
+			}
+
+			if (response.isError) {
+				console.error("[CodeIndexManager] MCP search_code error:" + results.join(", "))
+				throw new Error("MCP search_code returned an error:" + results.join(", "))
+			}
+
+			return results
 		} catch (error) {
-			// Should never happen: reading file failed even though it exists
-			console.error("Unexpected error loading .gitignore:", error)
-		}
-
-		// (Re)Create shared service instances
-		const { embedder, vectorStore, scanner, fileWatcher } = this._serviceFactory.createServices(
-			this.context,
-			this._cacheManager!,
-			ignoreInstance,
-		)
-
-		// Validate embedder configuration before proceeding
-		const validationResult = await this._serviceFactory.validateEmbedder(embedder)
-		if (!validationResult.valid) {
-			const errorMessage = validationResult.error || "Embedder configuration validation failed"
-			// Always attempt translation, use original as fallback
-			let translatedMessage = t(errorMessage)
-			// If translation returns a different value (stripped namespace), use original
-			if (translatedMessage !== errorMessage && !translatedMessage.includes(":")) {
-				translatedMessage = errorMessage
+			console.error("[CodeIndexManager] searchIndex exception:", error)
+			throw error
+		}
+	}
+
+	public async searchSummary(directoryPrefix: string): Promise<string[]> {
+		if (!this._mcpClient) {
+			throw new Error("MCP client not initialized")
+		}
+
+		const params: Record<string, any> = { 
+			json_format: true, 
+			paths: [directoryPrefix],
+		}
+
+		try {
+			const response = await this._mcpClient.request(
+				{
+					method: "tools/call",
+					params: {
+						name: "get_summary",
+						arguments: params
+					}
+				},
+				CallToolResultSchema
+			)
+
+			const results: string[] = []
+			if (response && Array.isArray(response.content)) {
+				for (const item of response.content) {
+					if (item && typeof item.text === "string") {
+						results.push(item.text)
+					}
+				}
+			}
+
+			if (response.isError) {
+				console.error("[CodeIndexManager] MCP get_summary error:" + results.join(", "))
+				throw new Error("MCP get_summary returned an error:" + results.join(", "))
 			}
 
-			this._stateManager.setSystemState("Error", translatedMessage)
-			throw new Error(translatedMessage)
-		}
-
-		// (Re)Initialize orchestrator
-		this._orchestrator = new CodeIndexOrchestrator(
-			this._configManager!,
-			this._stateManager,
-			this.workspacePath,
-			this._cacheManager!,
-			vectorStore,
-			scanner,
-			fileWatcher,
-		)
-
-		// (Re)Initialize search service
-		this._searchService = new CodeIndexSearchService(
-			this._configManager!,
-			this._stateManager,
-			embedder,
-			vectorStore,
-		)
-
-		// Clear any error state after successful recreation
-		this._stateManager.setSystemState("Standby", "")
+			return results
+		} catch (error) {
+			console.error("[CodeIndexManager] searchSummary exception:", error)
+			throw error
+		}
 	}
 
 	/**
@@ -310,16 +432,15 @@ export class CodeIndexManager {
 			const isFeatureEnabled = this.isFeatureEnabled
 			const isFeatureConfigured = this.isFeatureConfigured
 
-			if (requiresRestart && isFeatureEnabled && isFeatureConfigured) {
-				try {
-					// Recreate services with new configuration
-					await this._recreateServices()
-				} catch (error) {
-					// Error state already set in _recreateServices
-					console.error("Failed to recreate services:", error)
-					// Re-throw the error so the caller knows validation failed
-					throw error
-				}
+			// If configuration changes require a restart and the manager is initialized, restart the service
+			if (requiresRestart && isFeatureEnabled && isFeatureConfigured && this.isInitialized) {
+				this.stopWatcher()
+				const contextProxy = await ContextProxy.getInstance(this.context)
+				await this.initialize(contextProxy)
+				// await this.startIndexing()
+			} else if (!isFeatureEnabled) {
+				this._stateManager.setSystemState("Standby", "File watcher stopped.")
+				this.stopWatcher()
 			}
 		}
 	}
diff --git a/src/services/search/file-search.ts b/src/services/search/file-search.ts
index a25dd406..ba70bda5 100644
--- a/src/services/search/file-search.ts
+++ b/src/services/search/file-search.ts
@@ -155,7 +155,64 @@ export async function searchWorkspaceFiles(
 			}),
 		)
 
-		return verifiedResults
+		// return verifiedResults
+		let inc: typeof allItems = []
+		const _query = query.replace(/\\/g, '/')
+		if (_query.includes("/")) {
+			inc = allItems
+				.filter((item) => {
+					const itemAbsolutePath = path.resolve(workspacePath, item.path).replace(/\\/g, '/')
+					if (!itemAbsolutePath.includes(_query)) {
+						return false
+					}
+					// const afterQuery = itemAbsolutePath.split(abs_queryPath).pop() || ""
+					// const slashCount = (afterQuery.match(/\//g) || []).length
+					// if (slashCount > 1 && !afterQuery.endsWith('/')) {
+					// 	return false
+					// }
+					return true
+				})
+				.map((item) => {
+					const fullPath = path.join(workspacePath, item.path)
+					const isDirectory = fs.lstatSync(fullPath).isDirectory()
+					return {
+						...item,
+						path: item.path.toPosix(),
+						type: isDirectory ? ("folder" as const) : ("file" as const),
+					}
+				})
+		}
+		
+		// const abs_queryPath = path.resolve(workspacePath, query).replace(/\\/g, '/')
+		// let inc: typeof allItems = []
+		// if (fs.existsSync(abs_queryPath)) {
+		// 	inc = allItems
+		// 		.filter((item) => {
+		// 			const itemAbsolutePath = path.resolve(workspacePath, item.path).replace(/\\/g, '/')
+		// 			if (!itemAbsolutePath.includes(abs_queryPath)) {
+		// 				return false
+		// 			}
+		// 			const afterQuery = itemAbsolutePath.split(abs_queryPath).pop() || ""
+		// 			const slashCount = (afterQuery.match(/\//g) || []).length
+		// 			if (slashCount > 1 && !afterQuery.endsWith('/')) {
+		// 				return false
+		// 			}
+		// 			return true
+		// 		})
+		// 		.map((item) => {
+		// 			const fullPath = path.join(workspacePath, item.path)
+		// 			const isDirectory = fs.lstatSync(fullPath).isDirectory()
+		// 			return {
+		// 				...item,
+		// 				path: item.path.toPosix(),
+		// 				type: isDirectory ? ("folder" as const) : ("file" as const),
+		// 			}
+		// 		})
+		// }
+
+		return [...inc,...verifiedResults,].filter((item) => {
+			return item.type !== "folder"
+		})
 	} catch (error) {
 		console.error("Error in searchWorkspaceFiles:", error)
 		return []
diff --git a/src/shared/ExtensionMessage.ts b/src/shared/ExtensionMessage.ts
index 953c0c10..12b1ffc6 100644
--- a/src/shared/ExtensionMessage.ts
+++ b/src/shared/ExtensionMessage.ts
@@ -60,6 +60,7 @@ export interface ExtensionMessage {
 		| "messageUpdated"
 		| "mcpServers"
 		| "enhancedPrompt"
+		| "savedMemory"
 		| "commitSearchResults"
 		| "listApiConfig"
 		| "routerModels"
diff --git a/src/shared/WebviewMessage.ts b/src/shared/WebviewMessage.ts
index fa9fb673..c4877796 100644
--- a/src/shared/WebviewMessage.ts
+++ b/src/shared/WebviewMessage.ts
@@ -106,6 +106,8 @@ export interface WebviewMessage {
 		| "updateMcpTimeout"
 		| "fuzzyMatchThreshold"
 		| "writeDelayMs"
+		| "saveMemory"
+		| "savedMemory"
 		| "enhancePrompt"
 		| "enhancedPrompt"
 		| "draggedImages"
@@ -251,6 +253,18 @@ export interface WebviewMessage {
 		codeIndexQdrantApiKey?: string
 		codebaseIndexOpenAiCompatibleApiKey?: string
 		codebaseIndexGeminiApiKey?: string
+
+		// RAG settings
+		embeddingBaseUrl?: string
+		embeddingModelID?: string
+		enhancementBaseUrl?: string
+		enhancementModelID?: string
+		embeddingApiKey?: string
+		enhancementApiKey?: string
+
+		ragPath?: string
+		llmFilter?: boolean
+		codeBaseLogging?: boolean
 	}
 }
 
diff --git a/src/shared/context-mentions.ts b/src/shared/context-mentions.ts
index 2edb99de..ed5e4074 100644
--- a/src/shared/context-mentions.ts
+++ b/src/shared/context-mentions.ts
@@ -54,7 +54,7 @@ Mention regex:
 
 */
 export const mentionRegex =
-	/(?<!\\)@((?:\/|\w+:\/\/)(?:[^\s\\]|\\ )+?|[a-f0-9]{7,40}\b|problems\b|git-changes\b|terminal\b)(?=[.,;:!?]?(?=[\s\r\n]|$))/
+	/(?<!\\)@((?:\/|\w+:\/\/)(?:[^\s\\]|\\ )+?|[a-f0-9]{7,40}\b|problems\b|codebase\b|summary\b|memory\b|summary:[^\s]+?|codebase:[^\s]+?|git-changes\b|terminal\b)(?=[.,;:!?]?(?=[\s\r\n]|$))/
 export const mentionRegexGlobal = new RegExp(mentionRegex.source, "g")
 
 export interface MentionSuggestion {
diff --git a/src/shared/getApiMetrics.ts b/src/shared/getApiMetrics.ts
index 49476fdb..308c12d4 100644
--- a/src/shared/getApiMetrics.ts
+++ b/src/shared/getApiMetrics.ts
@@ -62,6 +62,8 @@ export function getApiMetrics(messages: ClineMessage[]) {
 			}
 		} else if (message.type === "say" && message.say === "condense_context") {
 			result.totalCost += message.contextCondense?.cost ?? 0
+		} else if (message.type === "say" && message.say === "save_memory") {
+			result.totalCost += message.contextCondense?.cost ?? 0
 		}
 	})
 
@@ -80,6 +82,8 @@ export function getApiMetrics(messages: ClineMessage[]) {
 			}
 		} else if (message.type === "say" && message.say === "condense_context") {
 			result.contextTokens = message.contextCondense?.newContextTokens ?? 0
+		} else if (message.type === "say" && message.say === "save_memory") {
+			result.contextTokens = message.contextCondense?.newContextTokens ?? 0
 		}
 		if (result.contextTokens) {
 			break
diff --git a/src/shared/modes.ts b/src/shared/modes.ts
index 4666a85d..df9fe31f 100644
--- a/src/shared/modes.ts
+++ b/src/shared/modes.ts
@@ -64,7 +64,7 @@ export function getToolsForMode(groups: readonly GroupEntry[]): string[] {
 export const modes: readonly ModeConfig[] = [
 	{
 		slug: "architect",
-		name: "🏗️ Architect",
+		name: "Architect",
 		roleDefinition:
 			"You are Roo, an experienced technical leader who is inquisitive and an excellent planner. Your goal is to gather information and get context to create a detailed plan for accomplishing the user's task, which the user will review and approve before they switch into another mode to implement the solution.",
 		whenToUse:
@@ -76,7 +76,7 @@ export const modes: readonly ModeConfig[] = [
 	},
 	{
 		slug: "code",
-		name: "💻 Code",
+		name: "Code",
 		roleDefinition:
 			"You are Roo, a highly skilled software engineer with extensive knowledge in many programming languages, frameworks, design patterns, and best practices.",
 		whenToUse:
@@ -86,7 +86,7 @@ export const modes: readonly ModeConfig[] = [
 	},
 	{
 		slug: "ask",
-		name: "❓ Ask",
+		name: "Ask",
 		roleDefinition:
 			"You are Roo, a knowledgeable technical assistant focused on answering questions and providing information about software development, technology, and related topics.",
 		whenToUse:
@@ -96,30 +96,30 @@ export const modes: readonly ModeConfig[] = [
 		customInstructions:
 			"You can analyze code, explain concepts, and access external resources. Always answer the user's questions thoroughly, and do not switch to implementing code unless explicitly requested by the user. Include Mermaid diagrams when they clarify your response.",
 	},
-	{
-		slug: "debug",
-		name: "🪲 Debug",
-		roleDefinition:
-			"You are Roo, an expert software debugger specializing in systematic problem diagnosis and resolution.",
-		whenToUse:
-			"Use this mode when you're troubleshooting issues, investigating errors, or diagnosing problems. Specialized in systematic debugging, adding logging, analyzing stack traces, and identifying root causes before applying fixes.",
-		description: "Diagnose and fix software issues",
-		groups: ["read", "edit", "browser", "command", "mcp"],
-		customInstructions:
-			"Reflect on 5-7 different possible sources of the problem, distill those down to 1-2 most likely sources, and then add logs to validate your assumptions. Explicitly ask the user to confirm the diagnosis before fixing the problem.",
-	},
-	{
-		slug: "orchestrator",
-		name: "🪃 Orchestrator",
-		roleDefinition:
-			"You are Roo, a strategic workflow orchestrator who coordinates complex tasks by delegating them to appropriate specialized modes. You have a comprehensive understanding of each mode's capabilities and limitations, allowing you to effectively break down complex problems into discrete tasks that can be solved by different specialists.",
-		whenToUse:
-			"Use this mode for complex, multi-step projects that require coordination across different specialties. Ideal when you need to break down large tasks into subtasks, manage workflows, or coordinate work that spans multiple domains or expertise areas.",
-		description: "Coordinate tasks across multiple modes",
-		groups: [],
-		customInstructions:
-			"Your role is to coordinate complex workflows by delegating tasks to specialized modes. As an orchestrator, you should:\n\n1. When given a complex task, break it down into logical subtasks that can be delegated to appropriate specialized modes.\n\n2. For each subtask, use the `new_task` tool to delegate. Choose the most appropriate mode for the subtask's specific goal and provide comprehensive instructions in the `message` parameter. These instructions must include:\n    *   All necessary context from the parent task or previous subtasks required to complete the work.\n    *   A clearly defined scope, specifying exactly what the subtask should accomplish.\n    *   An explicit statement that the subtask should *only* perform the work outlined in these instructions and not deviate.\n    *   An instruction for the subtask to signal completion by using the `attempt_completion` tool, providing a concise yet thorough summary of the outcome in the `result` parameter, keeping in mind that this summary will be the source of truth used to keep track of what was completed on this project.\n    *   A statement that these specific instructions supersede any conflicting general instructions the subtask's mode might have.\n\n3. Track and manage the progress of all subtasks. When a subtask is completed, analyze its results and determine the next steps.\n\n4. Help the user understand how the different subtasks fit together in the overall workflow. Provide clear reasoning about why you're delegating specific tasks to specific modes.\n\n5. When all subtasks are completed, synthesize the results and provide a comprehensive overview of what was accomplished.\n\n6. Ask clarifying questions when necessary to better understand how to break down complex tasks effectively.\n\n7. Suggest improvements to the workflow based on the results of completed subtasks.\n\nUse subtasks to maintain clarity. If a request significantly shifts focus or requires a different expertise (mode), consider creating a subtask rather than overloading the current one.",
-	},
+	// {
+	// 	slug: "debug",
+	// 	name: "🪲 Debug",
+	// 	roleDefinition:
+	// 		"You are Roo, an expert software debugger specializing in systematic problem diagnosis and resolution.",
+	// 	whenToUse:
+	// 		"Use this mode when you're troubleshooting issues, investigating errors, or diagnosing problems. Specialized in systematic debugging, adding logging, analyzing stack traces, and identifying root causes before applying fixes.",
+	// 	description: "Diagnose and fix software issues",
+	// 	groups: ["read", "edit", "browser", "command", "mcp"],
+	// 	customInstructions:
+	// 		"Reflect on 5-7 different possible sources of the problem, distill those down to 1-2 most likely sources, and then add logs to validate your assumptions. Explicitly ask the user to confirm the diagnosis before fixing the problem.",
+	// },
+	// {
+	// 	slug: "orchestrator",
+	// 	name: "🪃 Orchestrator",
+	// 	roleDefinition:
+	// 		"You are Roo, a strategic workflow orchestrator who coordinates complex tasks by delegating them to appropriate specialized modes. You have a comprehensive understanding of each mode's capabilities and limitations, allowing you to effectively break down complex problems into discrete tasks that can be solved by different specialists.",
+	// 	whenToUse:
+	// 		"Use this mode for complex, multi-step projects that require coordination across different specialties. Ideal when you need to break down large tasks into subtasks, manage workflows, or coordinate work that spans multiple domains or expertise areas.",
+	// 	description: "Coordinate tasks across multiple modes",
+	// 	groups: [],
+	// 	customInstructions:
+	// 		"Your role is to coordinate complex workflows by delegating tasks to specialized modes. As an orchestrator, you should:\n\n1. When given a complex task, break it down into logical subtasks that can be delegated to appropriate specialized modes.\n\n2. For each subtask, use the `new_task` tool to delegate. Choose the most appropriate mode for the subtask's specific goal and provide comprehensive instructions in the `message` parameter. These instructions must include:\n    *   All necessary context from the parent task or previous subtasks required to complete the work.\n    *   A clearly defined scope, specifying exactly what the subtask should accomplish.\n    *   An explicit statement that the subtask should *only* perform the work outlined in these instructions and not deviate.\n    *   An instruction for the subtask to signal completion by using the `attempt_completion` tool, providing a concise yet thorough summary of the outcome in the `result` parameter, keeping in mind that this summary will be the source of truth used to keep track of what was completed on this project.\n    *   A statement that these specific instructions supersede any conflicting general instructions the subtask's mode might have.\n\n3. Track and manage the progress of all subtasks. When a subtask is completed, analyze its results and determine the next steps.\n\n4. Help the user understand how the different subtasks fit together in the overall workflow. Provide clear reasoning about why you're delegating specific tasks to specific modes.\n\n5. When all subtasks are completed, synthesize the results and provide a comprehensive overview of what was accomplished.\n\n6. Ask clarifying questions when necessary to better understand how to break down complex tasks effectively.\n\n7. Suggest improvements to the workflow based on the results of completed subtasks.\n\nUse subtasks to maintain clarity. If a request significantly shifts focus or requires a different expertise (mode), consider creating a subtask rather than overloading the current one.",
+	// },
 ] as const
 
 // Export the default mode slug
diff --git a/src/shared/support-prompt.ts b/src/shared/support-prompt.ts
index 1767a207..45b0ea7d 100644
--- a/src/shared/support-prompt.ts
+++ b/src/shared/support-prompt.ts
@@ -48,7 +48,7 @@ const supportPromptConfigs: Record<SupportPromptType, SupportPromptConfig> = {
 	ENHANCE: {
 		template: `Generate an enhanced version of this prompt (reply with only the enhanced prompt - no conversation, explanations, lead-in, bullet points, placeholders, or surrounding quotes):
 
-\${userInput}`,
+\${userInput}\n`,
 	},
 	EXPLAIN: {
 		template: `Explain the following code from file path \${filePath}:\${startLine}-\${endLine}
@@ -61,7 +61,7 @@ const supportPromptConfigs: Record<SupportPromptType, SupportPromptConfig> = {
 Please provide a clear and concise explanation of what this code does, including:
 1. The purpose and functionality
 2. Key components and their interactions
-3. Important patterns or techniques used`,
+3. Important patterns or techniques used\n`,
 	},
 	FIX: {
 		template: `Fix any issues in the following code from file path \${filePath}:\${startLine}-\${endLine}
@@ -76,7 +76,7 @@ Please:
 1. Address all detected problems listed above (if any)
 2. Identify any other potential bugs or issues
 3. Provide corrected code
-4. Explain what was fixed and why`,
+4. Explain what was fixed and why\n`,
 	},
 	IMPROVE: {
 		template: `Improve the following code from file path \${filePath}:\${startLine}-\${endLine}
@@ -92,20 +92,20 @@ Please suggest improvements for:
 3. Best practices and patterns
 4. Error handling and edge cases
 
-Provide the improved code along with explanations for each enhancement.`,
+Provide the improved code along with explanations for each enhancement.\n`,
 	},
 	ADD_TO_CONTEXT: {
 		template: `\${filePath}:\${startLine}-\${endLine}
 \`\`\`
 \${selectedText}
-\`\`\``,
+\`\`\`\n`,
 	},
 	TERMINAL_ADD_TO_CONTEXT: {
 		template: `\${userInput}
 Terminal output:
 \`\`\`
 \${terminalContent}
-\`\`\``,
+\`\`\`\n`,
 	},
 	TERMINAL_FIX: {
 		template: `\${userInput}
@@ -117,7 +117,7 @@ Fix this terminal command:
 Please:
 1. Identify any issues in the command
 2. Provide the corrected command
-3. Explain what was fixed and why`,
+3. Explain what was fixed and why\n`,
 	},
 	TERMINAL_EXPLAIN: {
 		template: `\${userInput}
@@ -129,10 +129,10 @@ Explain this terminal command:
 Please provide:
 1. What the command does
 2. Explanation of each part/flag
-3. Expected output and behavior`,
+3. Expected output and behavior\n`,
 	},
 	NEW_TASK: {
-		template: `\${userInput}`,
+		template: `\${userInput}\n`,
 	},
 } as const
 
diff --git a/webview-ui/src/components/chat/ChatRow.tsx b/webview-ui/src/components/chat/ChatRow.tsx
index 4fd7977f..c0342645 100644
--- a/webview-ui/src/components/chat/ChatRow.tsx
+++ b/webview-ui/src/components/chat/ChatRow.tsx
@@ -40,6 +40,7 @@ import { CommandExecution } from "./CommandExecution"
 import { CommandExecutionError } from "./CommandExecutionError"
 import { AutoApprovedRequestLimitWarning } from "./AutoApprovedRequestLimitWarning"
 import { CondenseContextErrorRow, CondensingContextRow, ContextCondenseRow } from "./ContextCondenseRow"
+import { SaveMemoryErrorRow, SavingMemoryRow, SaveMemoryRow } from "./saveMemoryRow-rid"
 import CodebaseSearchResultsDisplay from "./CodebaseSearchResultsDisplay"
 
 interface ChatRowProps {
@@ -229,6 +230,20 @@ export const ChatRowContent = ({
 						style={{ color: successColor, marginBottom: "-1.5px" }}></span>,
 					<span style={{ color: successColor, fontWeight: "bold" }}>{t("chat:taskCompleted")}</span>,
 				]
+			case "user_feedback":
+				return [
+					<span
+						className="codicon codicon-account"
+						style={{ color: "var(--vscode-charts-blue)", marginBottom: "-1.5px" }}></span>,
+					<span style={{ color: "var(--vscode-charts-blue)", fontWeight: "bold" }}>{"用户反馈"}</span>,
+				]
+			case "save_memory_tag":
+				return [
+					<span
+						className="codicon codicon-save"
+						style={{ color: "var(--vscode-charts-yellow)", marginBottom: "-1.5px" }}></span>,
+					<span style={{ color: "var(--vscode-charts-yellow)", fontWeight: "bold" }}>{"记忆说明"}</span>,
+				]
 			case "api_req_retry_delayed":
 				return []
 			case "api_req_started":
@@ -1029,57 +1044,30 @@ export const ChatRowContent = ({
 					)
 				case "user_feedback":
 					return (
-						<div className="bg-vscode-editor-background border rounded-xs p-1 overflow-hidden whitespace-pre-wrap">
-							{isEditing ? (
-								<div className="flex flex-col gap-2 p-2">
-									<textarea
-										className="w-full p-2 bg-vscode-input-background text-vscode-input-foreground border border-vscode-input-border rounded-xs"
-										value={editedContent}
-										onChange={(e) => setEditedContent(e.target.value)}
-										rows={5}
-										autoFocus
-									/>
-									<div className="flex justify-end gap-2">
-										<Button variant="secondary" size="sm" onClick={handleCancelEdit}>
-											{t("chat:cancel.title")}
-										</Button>
-										<Button variant="default" size="sm" onClick={handleSaveEdit}>
-											{t("chat:save.title")}
-										</Button>
-									</div>
-								</div>
-							) : (
-								<div className="flex justify-between">
-									<div className="flex-grow px-2 py-1 wrap-anywhere">
-										<Mention text={message.text} withShadow />
-									</div>
-									<div className="flex">
-										<Button
-											variant="ghost"
-											size="icon"
-											className="shrink-0 hidden"
-											disabled={isStreaming}
-											onClick={(e) => {
-												e.stopPropagation()
-												handleEditClick()
-											}}>
-											<span className="codicon codicon-edit" />
-										</Button>
-										<Button
-											variant="ghost"
-											size="icon"
-											className="shrink-0"
-											disabled={isStreaming}
-											onClick={(e) => {
-												e.stopPropagation()
-												vscode.postMessage({ type: "deleteMessage", value: message.ts })
-											}}>
-											<span className="codicon codicon-trash" />
-										</Button>
-									</div>
+						// <div className="bg-vscode-editor-background border rounded-xs p-1 overflow-hidden whitespace-pre-wrap">
+						<div>
+							<div style={headerStyle}>
+								{icon}
+								{title}
+							</div>
+							<div className="flex justify-between">
+								<div className="flex-grow px-2 py-1 wrap-anywhere" style={{ color: "var(--vscode-charts-blue)" , paddingTop: 10 }}>
+									{/* <Mention text={message.text} withShadow /> */}
+									<Markdown markdown={message.text} partial={message.partial} />
 								</div>
-							)}
-							{!isEditing && message.images && message.images.length > 0 && (
+								<Button
+									variant="ghost"
+									size="icon"
+									className="shrink-0"
+									disabled={isStreaming}
+									onClick={(e) => {
+										e.stopPropagation()
+										vscode.postMessage({ type: "deleteMessage", value: message.ts })
+									}}>
+									<span className="codicon codicon-trash" />
+								</Button>
+							</div>
+							{message.images && message.images.length > 0 && (
 								<Thumbnails images={message.images} style={{ marginTop: "8px" }} />
 							)}
 						</div>
@@ -1132,6 +1120,25 @@ export const ChatRowContent = ({
 							checkpoint={message.checkpoint}
 						/>
 					)
+				case "save_memory":
+					if (message.partial) {
+						return <SavingMemoryRow />
+					}
+					return message.contextCondense ? <SaveMemoryRow {...message.contextCondense} /> : null
+				case "save_memory_error":
+					return <SaveMemoryErrorRow errorText={message.text} />
+				case "save_memory_tag":
+					return (<div>
+						<div style={headerStyle}>
+							{icon}
+							{title}
+						</div>
+						<div className="flex justify-between">
+							<div className="flex-grow px-2 py-1 wrap-anywhere" style={{ color: "var(--vscode-charts-yellow)" , paddingTop: 10 }}>
+								<Markdown markdown={message.text} partial={message.partial} />
+							</div>
+						</div>
+					</div>)
 				case "condense_context":
 					if (message.partial) {
 						return <CondensingContextRow />
diff --git a/webview-ui/src/components/chat/ChatTextArea.tsx b/webview-ui/src/components/chat/ChatTextArea.tsx
index ee622239..3c109157 100644
--- a/webview-ui/src/components/chat/ChatTextArea.tsx
+++ b/webview-ui/src/components/chat/ChatTextArea.tsx
@@ -45,6 +45,8 @@ interface ChatTextAreaProps {
 	mode: Mode
 	setMode: (value: Mode) => void
 	modeShortcutText: string
+	isSavingMemory: boolean
+	setIsSavingMemory: (value: boolean) => void
 }
 
 const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
@@ -64,6 +66,8 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 			mode,
 			setMode,
 			modeShortcutText,
+			isSavingMemory,
+			setIsSavingMemory,
 		},
 		ref,
 	) => {
@@ -178,6 +182,23 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 			}
 		}, [selectedType, searchQuery])
 
+		const handleSavingMemory = useCallback(() => {
+			if (sendingDisabled) {
+				return
+			}
+
+			const trimmedInput = inputValue.trim()
+
+			if (trimmedInput) {
+				setIsSavingMemory(true)
+				vscode.postMessage({ type: "saveMemory" as const, text: trimmedInput })
+			} else {
+				setIsSavingMemory(true)
+				vscode.postMessage({ type: "saveMemory" as const, text: "" })
+			}
+			setInputValue("")
+		}, [inputValue, sendingDisabled, setInputValue])
+
 		const handleEnhancePrompt = useCallback(() => {
 			if (sendingDisabled) {
 				return
@@ -199,6 +220,9 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 			return [
 				{ type: ContextMenuOptionType.Problems, value: "problems" },
 				{ type: ContextMenuOptionType.Terminal, value: "terminal" },
+				{ type: ContextMenuOptionType.Codebase, value: "codebase" },
+				{ type: ContextMenuOptionType.Summary, value: "summary" },
+				{ type: ContextMenuOptionType.Memory, value: "memory" },
 				...gitCommits,
 				...openedTabs
 					.filter((tab) => tab.path)
@@ -279,6 +303,12 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 						insertValue = "terminal"
 					} else if (type === ContextMenuOptionType.Git) {
 						insertValue = value || ""
+					} else if (type === ContextMenuOptionType.Codebase) {
+						insertValue = "codebase"
+					} else if (type === ContextMenuOptionType.Summary) {
+						insertValue = "summary"
+					} else if (type === ContextMenuOptionType.Memory) {
+					   insertValue = "memory"
 					}
 
 					const { newValue, mentionIndex } = insertMention(
@@ -381,7 +411,7 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 					return
 				}
 
-				if (event.key === "Enter" && !event.shiftKey && !isComposing) {
+				if (event.key === "Enter" && !event.shiftKey && !event.ctrlKey && !isComposing) {
 					event.preventDefault()
 
 					if (!sendingDisabled) {
@@ -1174,6 +1204,36 @@ const ChatTextArea = forwardRef<HTMLTextAreaElement, ChatTextAreaProps>(
 
 					<div className={cn("flex", "items-center", "gap-0.5", "shrink-0")}>
 						<IndexingStatusBadge />
+						<StandardTooltip content={t("保留永久记忆")}>
+							<button
+								aria-label={t("保留永久记忆")}
+								disabled={sendingDisabled}
+								onClick={handleSavingMemory}
+								className={cn(
+									"relative inline-flex items-center justify-center",
+									"bg-transparent border-none p-1.5",
+									"rounded-md min-w-[28px] min-h-[28px]",
+									"text-vscode-foreground opacity-85",
+									"transition-all duration-150",
+									"hover:opacity-100 hover:bg-[rgba(255,255,255,0.03)] hover:border-[rgba(255,255,255,0.15)]",
+									"focus:outline-none focus-visible:ring-1 focus-visible:ring-vscode-focusBorder",
+									"active:bg-[rgba(255,255,255,0.1)]",
+									!sendingDisabled && "cursor-pointer",
+									sendingDisabled &&
+										"opacity-40 cursor-not-allowed grayscale-[30%] hover:bg-transparent hover:border-[rgba(255,255,255,0.08)] active:bg-transparent",
+									"ml-1",
+								)}>
+								{isSavingMemory ? (
+									<div className="w-4 h-4 animate-spin rounded-full border-2 border-current border-t-transparent" />
+								) : (
+									<div className="w-4 h-4 flex items-center justify-center">
+										<svg className="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
+											<path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M4 7v10c0 2.21 1.79 4 4 4h8c2.21 0 4-1.79 4-4V7M4 7c0-2.21 1.79-4 4-4h8c2.21 0 4 1.79 4 4M4 7h16m-4 4h.01M7 11h.01" />
+										</svg>
+									</div>
+								)}
+							</button>
+						</StandardTooltip>
 						<StandardTooltip content={t("chat:addImages")}>
 							<button
 								aria-label={t("chat:addImages")}
diff --git a/webview-ui/src/components/chat/ChatView.tsx b/webview-ui/src/components/chat/ChatView.tsx
index 38b8997f..1f045bcd 100644
--- a/webview-ui/src/components/chat/ChatView.tsx
+++ b/webview-ui/src/components/chat/ChatView.tsx
@@ -161,6 +161,7 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 	const [wasStreaming, setWasStreaming] = useState<boolean>(false)
 	const [showCheckpointWarning, setShowCheckpointWarning] = useState<boolean>(false)
 	const [isCondensing, setIsCondensing] = useState<boolean>(false)
+	const [isSavingMemory, setIsSavingMemory] = useState<boolean>(false)
 	const [showAnnouncementModal, setShowAnnouncementModal] = useState(false)
 	const everVisibleMessagesTsRef = useRef<LRUCache<number, boolean>>(
 		new LRUCache({
@@ -746,6 +747,14 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 						setIsCondensing(false)
 					}
 					break
+				case "savedMemory":
+					if (message.success !== undefined) {
+						setIsSavingMemory(false)
+						if (sendingDisabled) {
+							setSendingDisabled(false)
+						}
+					}
+					break
 			}
 			// textAreaRef.current is not explicitly required here since React
 			// guarantees that ref will be stable across re-renders, and we're
@@ -1137,8 +1146,19 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 			})
 		}
 
+		if (isSavingMemory) {
+			// Show indicator after clicking save memory button
+			result.push({
+				type: "say",
+				say: "save_memory",
+				ts: Date.now(),
+				partial: true,
+			})
+		}
+
+
 		return result
-	}, [isCondensing, visibleMessages])
+	}, [isCondensing, isSavingMemory, visibleMessages])
 
 	// scrolling
 
@@ -1760,6 +1780,8 @@ const ChatViewComponent: React.ForwardRefRenderFunction<ChatViewRef, ChatViewPro
 				mode={mode}
 				setMode={setMode}
 				modeShortcutText={modeShortcutText}
+				isSavingMemory={isSavingMemory}
+				setIsSavingMemory={setIsSavingMemory}
 			/>
 
 			{isProfileDisabled && (
diff --git a/webview-ui/src/components/chat/CodeIndexPopover.tsx b/webview-ui/src/components/chat/CodeIndexPopover.tsx
index 1243b7ef..758bc4c5 100644
--- a/webview-ui/src/components/chat/CodeIndexPopover.tsx
+++ b/webview-ui/src/components/chat/CodeIndexPopover.tsx
@@ -4,6 +4,7 @@ import { z } from "zod"
 import {
 	VSCodeButton,
 	VSCodeTextField,
+	VSCodeCheckbox,
 	VSCodeDropdown,
 	VSCodeOption,
 	VSCodeLink,
@@ -67,6 +68,16 @@ interface LocalCodeIndexSettings {
 	codebaseIndexOpenAiCompatibleBaseUrl?: string
 	codebaseIndexOpenAiCompatibleApiKey?: string
 	codebaseIndexGeminiApiKey?: string
+
+	embeddingBaseUrl?: string
+	embeddingModelID?: string
+	embeddingApiKey?: string
+	enhancementBaseUrl?: string
+	enhancementModelID?: string
+	enhancementApiKey?: string
+	ragPath?: string
+	llmFilter?: boolean
+	codeBaseLogging?: boolean
 }
 
 // Validation schema for codebase index settings
@@ -99,17 +110,17 @@ const createValidationSchema = (provider: EmbedderProvider, t: any) => {
 
 		case "openai-compatible":
 			return baseSchema.extend({
-				codebaseIndexOpenAiCompatibleBaseUrl: z
+				embeddingBaseUrl: z
 					.string()
 					.min(1, t("settings:codeIndex.validation.baseUrlRequired"))
 					.url(t("settings:codeIndex.validation.invalidBaseUrl")),
-				codebaseIndexOpenAiCompatibleApiKey: z
+				embeddingApiKey: z
 					.string()
 					.min(1, t("settings:codeIndex.validation.apiKeyRequired")),
-				codebaseIndexEmbedderModelId: z.string().min(1, t("settings:codeIndex.validation.modelIdRequired")),
-				codebaseIndexEmbedderModelDimension: z
-					.number()
-					.min(1, t("settings:codeIndex.validation.modelDimensionRequired")),
+				embeddingModelID: z.string().min(1, t("settings:codeIndex.validation.modelIdRequired")),
+				// codebaseIndexEmbedderModelDimension: z
+				// 	.number()
+				// 	.min(1, t("settings:codeIndex.validation.modelDimensionRequired")),
 			})
 
 		case "gemini":
@@ -129,7 +140,7 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 	children,
 	indexingStatus: externalIndexingStatus,
 }) => {
-	const SECRET_PLACEHOLDER = "••••••••••••••••"
+	const SECRET_PLACEHOLDER = "••••••••••••••••••••••••••••••••••••••••••••••••"
 	const { t } = useAppTranslation()
 	const { codebaseIndexConfig, codebaseIndexModels } = useExtensionState()
 	const [open, setOpen] = useState(false)
@@ -152,7 +163,7 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 	const getDefaultSettings = (): LocalCodeIndexSettings => ({
 		codebaseIndexEnabled: true,
 		codebaseIndexQdrantUrl: "",
-		codebaseIndexEmbedderProvider: "openai",
+		codebaseIndexEmbedderProvider: "openai-compatible",
 		codebaseIndexEmbedderBaseUrl: "",
 		codebaseIndexEmbedderModelId: "",
 		codebaseIndexEmbedderModelDimension: undefined,
@@ -163,6 +174,16 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 		codebaseIndexOpenAiCompatibleBaseUrl: "",
 		codebaseIndexOpenAiCompatibleApiKey: "",
 		codebaseIndexGeminiApiKey: "",
+
+		embeddingBaseUrl: "",
+		embeddingModelID: "",
+		embeddingApiKey: "",
+		enhancementBaseUrl: "",
+		enhancementModelID: "",
+		enhancementApiKey: "",
+		ragPath: "",
+		llmFilter: false,
+		codeBaseLogging: false,
 	})
 
 	// Initial settings state - stores the settings when popover opens
@@ -196,6 +217,18 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 				codebaseIndexOpenAiCompatibleBaseUrl: codebaseIndexConfig.codebaseIndexOpenAiCompatibleBaseUrl || "",
 				codebaseIndexOpenAiCompatibleApiKey: "",
 				codebaseIndexGeminiApiKey: "",
+
+				embeddingBaseUrl: codebaseIndexConfig.embeddingBaseUrl || "",
+				embeddingModelID: codebaseIndexConfig.embeddingModelID || "",
+				enhancementBaseUrl: codebaseIndexConfig.enhancementBaseUrl || "",
+				enhancementModelID: codebaseIndexConfig.enhancementModelID || "",
+
+				embeddingApiKey: "",
+				enhancementApiKey: "",
+				
+				ragPath: codebaseIndexConfig.ragPath || "",
+				llmFilter: codebaseIndexConfig.llmFilter || false,
+				codeBaseLogging: codebaseIndexConfig.codeBaseLogging || false,
 			}
 			setInitialSettings(settings)
 			setCurrentSettings(settings)
@@ -277,6 +310,22 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 							? SECRET_PLACEHOLDER
 							: ""
 					}
+					if (
+						!prev.embeddingApiKey ||
+						prev.embeddingApiKey === SECRET_PLACEHOLDER
+					) {
+						updated.embeddingApiKey = secretStatus.hasEmbeddingApiKey
+							? SECRET_PLACEHOLDER
+							: ""
+					}
+					if (
+						!prev.enhancementApiKey ||
+						prev.enhancementApiKey === SECRET_PLACEHOLDER
+					) {
+						updated.enhancementApiKey = secretStatus.hasEnhancementApiKey
+							? SECRET_PLACEHOLDER
+							: ""
+					}
 					if (!prev.codebaseIndexGeminiApiKey || prev.codebaseIndexGeminiApiKey === SECRET_PLACEHOLDER) {
 						updated.codebaseIndexGeminiApiKey = secretStatus.hasGeminiApiKey ? SECRET_PLACEHOLDER : ""
 					}
@@ -347,7 +396,9 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 				if (
 					key === "codeIndexOpenAiKey" ||
 					key === "codebaseIndexOpenAiCompatibleApiKey" ||
-					key === "codebaseIndexGeminiApiKey"
+					key === "codebaseIndexGeminiApiKey" ||
+					key === "embeddingApiKey" ||
+					key === "enhancementApiKey"
 				) {
 					dataToValidate[key] = "placeholder-valid"
 				}
@@ -542,10 +593,10 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 							{isSetupSettingsOpen && (
 								<div className="mt-4 space-y-4">
 									{/* Embedder Provider Section */}
-									<div className="space-y-2">
-										<label className="text-sm font-medium">
-											{t("settings:codeIndex.embedderProviderLabel")}
-										</label>
+									<div className="flex flex-col gap-3">
+										<div className="flex items-center gap-4 font-bold">
+											<div>{t("settings:codeIndex.embedderProviderLabel")}</div>
+										</div>
 										<Select
 											value={currentSettings.codebaseIndexEmbedderProvider}
 											onValueChange={(value: EmbedderProvider) => {
@@ -557,18 +608,18 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 												<SelectValue />
 											</SelectTrigger>
 											<SelectContent>
-												<SelectItem value="openai">
+												{/* <SelectItem value="openai">
 													{t("settings:codeIndex.openaiProvider")}
 												</SelectItem>
 												<SelectItem value="ollama">
 													{t("settings:codeIndex.ollamaProvider")}
-												</SelectItem>
+												</SelectItem> */}
 												<SelectItem value="openai-compatible">
 													{t("settings:codeIndex.openaiCompatibleProvider")}
 												</SelectItem>
-												<SelectItem value="gemini">
+												{/* <SelectItem value="gemini">
 													{t("settings:codeIndex.geminiProvider")}
-												</SelectItem>
+												</SelectItem> */}
 											</SelectContent>
 										</Select>
 									</div>
@@ -673,7 +724,7 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 											</div>
 
 											<div className="space-y-2">
-												<label className="text-sm font-medium">
+												<label className="font-medium">
 													{t("settings:codeIndex.modelLabel")}
 												</label>
 												<VSCodeDropdown
@@ -715,7 +766,7 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 
 									{currentSettings.codebaseIndexEmbedderProvider === "openai-compatible" && (
 										<>
-											<div className="space-y-2">
+											{/* <div className="space-y-2">
 												<label className="text-sm font-medium">
 													{t("settings:codeIndex.openAiCompatibleBaseUrlLabel")}
 												</label>
@@ -817,6 +868,95 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 														{formErrors.codebaseIndexEmbedderModelDimension}
 													</p>
 												)}
+											</div> */}
+
+											<div className="flex flex-col gap-3">
+												<div className="flex items-center gap-2 font-bold">
+													<div>{"基础 URL"}</div>
+													<StandardTooltip
+														content={"使用嵌入模型对源文件片段进行向量化"}>
+														<span className="codicon codicon-info text-xs text-vscode-descriptionForeground cursor-help" />
+													</StandardTooltip>
+												</div>
+												<div>
+													<VSCodeTextField
+														value={currentSettings.embeddingBaseUrl || ""}
+														onInput={(e: any) =>
+															updateSetting("embeddingBaseUrl", e.target.value)
+														}
+														style={{ width: "100%" }}>
+													</VSCodeTextField>
+												</div>
+												<div className="flex items-center gap-2 font-bold">
+													<div>{"API 密钥"}</div>
+												</div>
+												<div>
+													<VSCodeTextField
+														type="password"
+														value={currentSettings.embeddingApiKey || ""}
+														onInput={(e: any) =>
+															updateSetting("embeddingApiKey", e.target.value)
+														}
+														style={{ width: "100%" }}></VSCodeTextField>
+												</div>
+												<div className="flex items-center gap-2 font-bold">
+													<div>{"模型 ID"}</div>
+												</div>
+												<div>
+													<VSCodeTextField
+														value={currentSettings.embeddingModelID || ""}
+														onInput={(e: any) =>
+															updateSetting("embeddingModelID", e.target.value)
+														}
+														style={{ width: "100%" }}></VSCodeTextField>
+												</div>
+											</div>
+
+											<div className="mt-8">
+												<div className="flex flex-col gap-3">
+													<div className="flex items-center gap-0 font-bold">
+														<div>{"注解基础 URL"}</div>
+														<p className="text-vscode-descriptionForeground m-0">{"（选填）"}</p>
+														<StandardTooltip
+															content={"使用对话模型对源文件片段进行注解，帮助 Codebase Search 搜索代码上下文，也使用于 LLM 重排序"}>
+															<span className="codicon codicon-info text-xs text-vscode-descriptionForeground cursor-help" />
+														</StandardTooltip>
+													</div>
+													<div>
+														<VSCodeTextField
+															value={currentSettings.enhancementBaseUrl || ""}
+															onInput={(e: any) =>
+																updateSetting("enhancementBaseUrl", e.target.value)
+															}
+															style={{ width: "100%" }}>
+														</VSCodeTextField>
+													</div>
+													<div className="flex items-center gap-0 font-bold">
+														<div>{"注解 API 密钥"}</div>
+														<p className="text-vscode-descriptionForeground m-0">{"（选填）"}</p>
+													</div>
+													<div>
+														<VSCodeTextField
+															type="password"
+															value={currentSettings.enhancementApiKey || ""}
+															onInput={(e: any) =>
+																updateSetting("enhancementApiKey", e.target.value)
+															}
+															style={{ width: "100%" }}></VSCodeTextField>
+													</div>
+													<div className="flex items-center gap-0 font-bold">
+														<div>{"注解模型 ID"}</div>
+														<p className="text-vscode-descriptionForeground m-0">{"（选填）"}</p>
+													</div>
+													<div>
+														<VSCodeTextField
+															value={currentSettings.enhancementModelID || ""}
+															onInput={(e: any) =>
+																updateSetting("enhancementModelID", e.target.value)
+															}
+															style={{ width: "100%" }}></VSCodeTextField>
+													</div>
+												</div>
 											</div>
 										</>
 									)}
@@ -887,7 +1027,7 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 									)}
 
 									{/* Qdrant Settings */}
-									<div className="space-y-2">
+									{/*<div className="space-y-2">
 										<label className="text-sm font-medium">
 											{t("settings:codeIndex.qdrantUrlLabel")}
 										</label>
@@ -933,7 +1073,7 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 												{formErrors.codeIndexQdrantApiKey}
 											</p>
 										)}
-									</div>
+									</div>*/}
 								</div>
 							)}
 						</div>
@@ -956,7 +1096,7 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 									{/* Search Score Threshold Slider */}
 									<div className="space-y-2">
 										<div className="flex items-center gap-2">
-											<label className="text-sm font-medium">
+											<label className="font-medium">
 												{t("settings:codeIndex.searchMinScoreLabel")}
 											</label>
 											<StandardTooltip
@@ -1002,7 +1142,7 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 									{/* Maximum Search Results Slider */}
 									<div className="space-y-2">
 										<div className="flex items-center gap-2">
-											<label className="text-sm font-medium">
+											<label className="font-medium">
 												{t("settings:codeIndex.searchMaxResultsLabel")}
 											</label>
 											<StandardTooltip
@@ -1041,6 +1181,54 @@ export const CodeIndexPopover: React.FC<CodeIndexPopoverProps> = ({
 												<span className="codicon codicon-discard" />
 											</VSCodeButton>
 										</div>
+									
+										<div className="flex flex-col gap-2 mt-6">
+											<div className="flex items-center gap-0 font-bold">
+												<div>{"存放位置"}</div>
+												<p className="text-vscode-descriptionForeground m-0">{"（选填）"}</p>
+												<StandardTooltip
+													content={"向量数据库仅支持存放在本地硬盘，不支持存放在网络硬盘"}>
+													<span className="codicon codicon-info text-xs text-vscode-descriptionForeground cursor-help" />
+												</StandardTooltip>
+											</div>
+											<div>
+												<VSCodeTextField
+													value={currentSettings.ragPath || ""}
+													onInput={(e: any) =>
+														updateSetting("ragPath", e.target.value)
+													}
+													style={{ width: "100%" }}>
+												</VSCodeTextField>
+											</div>
+										</div>
+										<div className="flex flex-col gap-2">
+											<div className="flex items-center gap-2">
+												<VSCodeCheckbox
+													checked={currentSettings.llmFilter || false}
+													onChange={(e: any) =>
+														updateSetting("llmFilter", e.target.checked)
+													}>
+													<span style={{ display: 'flex', alignItems: 'center' }}>
+														<span className="font-medium">{"启用 LLM 重排序"}</span>
+														<StandardTooltip
+															content={"使用对话模型对搜索结果进行筛选，提高检索准确性"}>
+															<span className="codicon codicon-info text-xs text-vscode-descriptionForeground cursor-help" style={{ display: 'inline-flex', alignItems: 'center', height: '1em', marginLeft: '8px' }} />
+														</StandardTooltip>
+													</span>
+												</VSCodeCheckbox>
+											</div>
+										</div>
+										<div className="flex flex-col gap-2">
+											<div className="flex items-center gap-2">
+												<VSCodeCheckbox
+													checked={currentSettings.codeBaseLogging || false}
+													onChange={(e: any) =>
+														updateSetting("codeBaseLogging", e.target.checked)
+													}>
+													<span className="font-medium">{"启用代码库日志记录"}</span>
+												</VSCodeCheckbox>
+											</div>
+										</div>
 									</div>
 								</div>
 							)}
diff --git a/webview-ui/src/components/chat/ContextMenu.tsx b/webview-ui/src/components/chat/ContextMenu.tsx
index 1672c35e..bf90d7ee 100644
--- a/webview-ui/src/components/chat/ContextMenu.tsx
+++ b/webview-ui/src/components/chat/ContextMenu.tsx
@@ -95,6 +95,12 @@ const ContextMenu: React.FC<ContextMenuProps> = ({
 				return <span>Paste URL to fetch contents</span>
 			case ContextMenuOptionType.NoResults:
 				return <span>No results found</span>
+			case ContextMenuOptionType.Codebase:
+				return <span>Codebase</span>
+			case ContextMenuOptionType.Summary:
+				return <span>Summary</span>
+			case ContextMenuOptionType.Memory:
+				return <span>Memory</span>
 			case ContextMenuOptionType.Git:
 				if (option.value) {
 					return (
@@ -171,6 +177,12 @@ const ContextMenu: React.FC<ContextMenuProps> = ({
 				return "folder"
 			case ContextMenuOptionType.Problems:
 				return "warning"
+			case ContextMenuOptionType.Codebase:
+				return "library"
+			case ContextMenuOptionType.Summary:
+				return "notebook"
+			case ContextMenuOptionType.Memory:
+				return "database"
 			case ContextMenuOptionType.Terminal:
 				return "terminal"
 			case ContextMenuOptionType.URL:
@@ -217,7 +229,7 @@ const ContextMenu: React.FC<ContextMenuProps> = ({
 					zIndex: 1000,
 					display: "flex",
 					flexDirection: "column",
-					maxHeight: "200px",
+					maxHeight: "405px", // 增加可以显示更多选项
 					overflowY: "auto",
 				}}>
 				{filteredOptions && filteredOptions.length > 0 ? (
diff --git a/webview-ui/src/components/chat/__tests__/ChatTextArea.spec.tsx b/webview-ui/src/components/chat/__tests__/ChatTextArea.spec.tsx
index 75324c97..e69de29b 100644
--- a/webview-ui/src/components/chat/__tests__/ChatTextArea.spec.tsx
+++ b/webview-ui/src/components/chat/__tests__/ChatTextArea.spec.tsx
@@ -1,860 +0,0 @@
-import { render, fireEvent, screen } from "@/utils/test-utils"
-
-import { defaultModeSlug } from "@roo/modes"
-
-import { useExtensionState } from "@src/context/ExtensionStateContext"
-import { vscode } from "@src/utils/vscode"
-import * as pathMentions from "@src/utils/path-mentions"
-
-import ChatTextArea from "../ChatTextArea"
-
-vi.mock("@src/utils/vscode", () => ({
-	vscode: {
-		postMessage: vi.fn(),
-	},
-}))
-
-vi.mock("@src/components/common/CodeBlock")
-vi.mock("@src/components/common/MarkdownBlock")
-vi.mock("@src/utils/path-mentions", () => ({
-	convertToMentionPath: vi.fn((path, cwd) => {
-		// Simple mock implementation that mimics the real function's behavior
-		if (cwd && path.toLowerCase().startsWith(cwd.toLowerCase())) {
-			const relativePath = path.substring(cwd.length)
-			return "@" + (relativePath.startsWith("/") ? relativePath : "/" + relativePath)
-		}
-		return path
-	}),
-}))
-
-// Get the mocked postMessage function
-const mockPostMessage = vscode.postMessage as ReturnType<typeof vi.fn>
-const mockConvertToMentionPath = pathMentions.convertToMentionPath as ReturnType<typeof vi.fn>
-
-// Mock ExtensionStateContext
-vi.mock("@src/context/ExtensionStateContext")
-
-// Custom query function to get the enhance prompt button
-const getEnhancePromptButton = () => {
-	return screen.getByRole("button", {
-		name: (_, element) => {
-			// Find the button with the wand sparkles icon (Lucide React)
-			return element.querySelector(".lucide-wand-sparkles") !== null
-		},
-	})
-}
-
-describe("ChatTextArea", () => {
-	const defaultProps = {
-		inputValue: "",
-		setInputValue: vi.fn(),
-		onSend: vi.fn(),
-		sendingDisabled: false,
-		selectApiConfigDisabled: false,
-		onSelectImages: vi.fn(),
-		shouldDisableImages: false,
-		placeholderText: "Type a message...",
-		selectedImages: [],
-		setSelectedImages: vi.fn(),
-		onHeightChange: vi.fn(),
-		mode: defaultModeSlug,
-		setMode: vi.fn(),
-		modeShortcutText: "(⌘. for next mode)",
-	}
-
-	beforeEach(() => {
-		vi.clearAllMocks()
-		// Default mock implementation for useExtensionState
-		;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-			filePaths: [],
-			openedTabs: [],
-			apiConfiguration: {
-				apiProvider: "anthropic",
-			},
-			taskHistory: [],
-			cwd: "/test/workspace",
-		})
-	})
-
-	describe("enhance prompt button", () => {
-		it("should be disabled when sendingDisabled is true", () => {
-			;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-				filePaths: [],
-				openedTabs: [],
-				taskHistory: [],
-				cwd: "/test/workspace",
-			})
-			render(<ChatTextArea {...defaultProps} sendingDisabled={true} />)
-			const enhanceButton = getEnhancePromptButton()
-			expect(enhanceButton).toHaveClass("cursor-not-allowed")
-		})
-	})
-
-	describe("handleEnhancePrompt", () => {
-		it("should send message with correct configuration when clicked", () => {
-			const apiConfiguration = {
-				apiProvider: "openrouter",
-				apiKey: "test-key",
-			}
-
-			;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-				filePaths: [],
-				openedTabs: [],
-				apiConfiguration,
-				taskHistory: [],
-				cwd: "/test/workspace",
-			})
-
-			render(<ChatTextArea {...defaultProps} inputValue="Test prompt" />)
-
-			const enhanceButton = getEnhancePromptButton()
-			fireEvent.click(enhanceButton)
-
-			expect(mockPostMessage).toHaveBeenCalledWith({
-				type: "enhancePrompt",
-				text: "Test prompt",
-			})
-		})
-
-		it("should not send message when input is empty", () => {
-			;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-				filePaths: [],
-				openedTabs: [],
-				apiConfiguration: {
-					apiProvider: "openrouter",
-				},
-				taskHistory: [],
-				cwd: "/test/workspace",
-			})
-
-			render(<ChatTextArea {...defaultProps} inputValue="" />)
-
-			// Clear any calls from component initialization (e.g., IndexingStatusBadge)
-			mockPostMessage.mockClear()
-
-			const enhanceButton = getEnhancePromptButton()
-			fireEvent.click(enhanceButton)
-
-			expect(mockPostMessage).not.toHaveBeenCalled()
-		})
-
-		it("should show loading state while enhancing", () => {
-			;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-				filePaths: [],
-				openedTabs: [],
-				apiConfiguration: {
-					apiProvider: "openrouter",
-				},
-				taskHistory: [],
-				cwd: "/test/workspace",
-			})
-
-			render(<ChatTextArea {...defaultProps} inputValue="Test prompt" />)
-
-			const enhanceButton = getEnhancePromptButton()
-			fireEvent.click(enhanceButton)
-
-			// Check if the WandSparkles icon has the animate-spin class
-			const animatingIcon = enhanceButton.querySelector(".animate-spin")
-			expect(animatingIcon).toBeInTheDocument()
-		})
-	})
-
-	describe("effect dependencies", () => {
-		it("should update when apiConfiguration changes", () => {
-			const { rerender } = render(<ChatTextArea {...defaultProps} />)
-
-			// Update apiConfiguration
-			;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-				filePaths: [],
-				openedTabs: [],
-				apiConfiguration: {
-					apiProvider: "openrouter",
-					newSetting: "test",
-				},
-				taskHistory: [],
-				cwd: "/test/workspace",
-			})
-
-			rerender(<ChatTextArea {...defaultProps} />)
-
-			// Verify the enhance button appears after apiConfiguration changes
-			expect(getEnhancePromptButton()).toBeInTheDocument()
-		})
-	})
-
-	describe("enhanced prompt response", () => {
-		it("should update input value when receiving enhanced prompt", () => {
-			const setInputValue = vi.fn()
-
-			render(<ChatTextArea {...defaultProps} setInputValue={setInputValue} />)
-
-			// Simulate receiving enhanced prompt message
-			window.dispatchEvent(
-				new MessageEvent("message", {
-					data: {
-						type: "enhancedPrompt",
-						text: "Enhanced test prompt",
-					},
-				}),
-			)
-
-			expect(setInputValue).toHaveBeenCalledWith("Enhanced test prompt")
-		})
-	})
-
-	describe("multi-file drag and drop", () => {
-		const mockCwd = "/Users/test/project"
-
-		beforeEach(() => {
-			vi.clearAllMocks()
-			;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-				filePaths: [],
-				openedTabs: [],
-				cwd: mockCwd,
-			})
-			mockConvertToMentionPath.mockClear()
-		})
-
-		it("should process multiple file paths separated by newlines", () => {
-			const setInputValue = vi.fn()
-
-			const { container } = render(
-				<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Initial text" />,
-			)
-
-			// Create a mock dataTransfer object with text data containing multiple file paths
-			const dataTransfer = {
-				getData: vi.fn().mockReturnValue("/Users/test/project/file1.js\n/Users/test/project/file2.js"),
-				files: [],
-			}
-
-			// Simulate drop event
-			fireEvent.drop(container.querySelector(".chat-text-area")!, {
-				dataTransfer,
-				preventDefault: vi.fn(),
-			})
-
-			// Verify convertToMentionPath was called for each file path
-			expect(mockConvertToMentionPath).toHaveBeenCalledTimes(2)
-			expect(mockConvertToMentionPath).toHaveBeenCalledWith("/Users/test/project/file1.js", mockCwd)
-			expect(mockConvertToMentionPath).toHaveBeenCalledWith("/Users/test/project/file2.js", mockCwd)
-
-			// Verify setInputValue was called with the correct value
-			// The mock implementation of convertToMentionPath will convert the paths to @/file1.js and @/file2.js
-			expect(setInputValue).toHaveBeenCalledWith("@/file1.js @/file2.js Initial text")
-		})
-
-		it("should filter out empty lines in the dragged text", () => {
-			const setInputValue = vi.fn()
-
-			const { container } = render(
-				<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Initial text" />,
-			)
-
-			// Create a mock dataTransfer object with text data containing empty lines
-			const dataTransfer = {
-				getData: vi.fn().mockReturnValue("/Users/test/project/file1.js\n\n/Users/test/project/file2.js\n\n"),
-				files: [],
-			}
-
-			// Simulate drop event
-			fireEvent.drop(container.querySelector(".chat-text-area")!, {
-				dataTransfer,
-				preventDefault: vi.fn(),
-			})
-
-			// Verify convertToMentionPath was called only for non-empty lines
-			expect(mockConvertToMentionPath).toHaveBeenCalledTimes(2)
-
-			// Verify setInputValue was called with the correct value
-			expect(setInputValue).toHaveBeenCalledWith("@/file1.js @/file2.js Initial text")
-		})
-
-		it("should correctly update cursor position after adding multiple mentions", () => {
-			const setInputValue = vi.fn()
-			const initialCursorPosition = 5
-
-			const { container } = render(
-				<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Hello world" />,
-			)
-
-			// Set the cursor position manually
-			const textArea = container.querySelector("textarea")
-			if (textArea) {
-				textArea.selectionStart = initialCursorPosition
-				textArea.selectionEnd = initialCursorPosition
-			}
-
-			// Create a mock dataTransfer object with text data
-			const dataTransfer = {
-				getData: vi.fn().mockReturnValue("/Users/test/project/file1.js\n/Users/test/project/file2.js"),
-				files: [],
-			}
-
-			// Simulate drop event
-			fireEvent.drop(container.querySelector(".chat-text-area")!, {
-				dataTransfer,
-				preventDefault: vi.fn(),
-			})
-
-			// The cursor position should be updated based on the implementation in the component
-			expect(setInputValue).toHaveBeenCalledWith("@/file1.js @/file2.js Hello world")
-		})
-
-		it("should handle very long file paths correctly", () => {
-			const setInputValue = vi.fn()
-
-			const { container } = render(<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />)
-
-			// Create a very long file path
-			const longPath =
-				"/Users/test/project/very/long/path/with/many/nested/directories/and/a/very/long/filename/with/extension.typescript"
-
-			// Create a mock dataTransfer object with the long path
-			const dataTransfer = {
-				getData: vi.fn().mockReturnValue(longPath),
-				files: [],
-			}
-
-			// Simulate drop event
-			fireEvent.drop(container.querySelector(".chat-text-area")!, {
-				dataTransfer,
-				preventDefault: vi.fn(),
-			})
-
-			// Verify convertToMentionPath was called with the long path
-			expect(mockConvertToMentionPath).toHaveBeenCalledWith(longPath, mockCwd)
-
-			// The mock implementation will convert it to @/very/long/path/...
-			expect(setInputValue).toHaveBeenCalledWith(
-				"@/very/long/path/with/many/nested/directories/and/a/very/long/filename/with/extension.typescript ",
-			)
-		})
-
-		it("should handle paths with special characters correctly", () => {
-			const setInputValue = vi.fn()
-
-			const { container } = render(<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />)
-
-			// Create paths with special characters
-			const specialPath1 = "/Users/test/project/file with spaces.js"
-			const specialPath2 = "/Users/test/project/file-with-dashes.js"
-			const specialPath3 = "/Users/test/project/file_with_underscores.js"
-			const specialPath4 = "/Users/test/project/file.with.dots.js"
-
-			// Create a mock dataTransfer object with the special paths
-			const dataTransfer = {
-				getData: vi.fn().mockReturnValue(`${specialPath1}\n${specialPath2}\n${specialPath3}\n${specialPath4}`),
-				files: [],
-			}
-
-			// Simulate drop event
-			fireEvent.drop(container.querySelector(".chat-text-area")!, {
-				dataTransfer,
-				preventDefault: vi.fn(),
-			})
-
-			// Verify convertToMentionPath was called for each path
-			expect(mockConvertToMentionPath).toHaveBeenCalledTimes(4)
-			expect(mockConvertToMentionPath).toHaveBeenCalledWith(specialPath1, mockCwd)
-			expect(mockConvertToMentionPath).toHaveBeenCalledWith(specialPath2, mockCwd)
-			expect(mockConvertToMentionPath).toHaveBeenCalledWith(specialPath3, mockCwd)
-			expect(mockConvertToMentionPath).toHaveBeenCalledWith(specialPath4, mockCwd)
-
-			// Verify setInputValue was called with the correct value
-			expect(setInputValue).toHaveBeenCalledWith(
-				"@/file with spaces.js @/file-with-dashes.js @/file_with_underscores.js @/file.with.dots.js ",
-			)
-		})
-
-		it("should handle paths outside the current working directory", () => {
-			const setInputValue = vi.fn()
-
-			const { container } = render(<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />)
-
-			// Create paths outside the current working directory
-			const outsidePath = "/Users/other/project/file.js"
-
-			// Mock the convertToMentionPath function to return the original path for paths outside cwd
-			mockConvertToMentionPath.mockImplementationOnce((path, _cwd) => {
-				return path // Return original path for this test
-			})
-
-			// Create a mock dataTransfer object with the outside path
-			const dataTransfer = {
-				getData: vi.fn().mockReturnValue(outsidePath),
-				files: [],
-			}
-
-			// Simulate drop event
-			fireEvent.drop(container.querySelector(".chat-text-area")!, {
-				dataTransfer,
-				preventDefault: vi.fn(),
-			})
-
-			// Verify convertToMentionPath was called with the outside path
-			expect(mockConvertToMentionPath).toHaveBeenCalledWith(outsidePath, mockCwd)
-
-			// Verify setInputValue was called with the original path
-			expect(setInputValue).toHaveBeenCalledWith("/Users/other/project/file.js ")
-		})
-
-		it("should do nothing when dropped text is empty", () => {
-			const setInputValue = vi.fn()
-
-			const { container } = render(
-				<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Initial text" />,
-			)
-
-			// Create a mock dataTransfer object with empty text
-			const dataTransfer = {
-				getData: vi.fn().mockReturnValue(""),
-				files: [],
-			}
-
-			// Simulate drop event
-			fireEvent.drop(container.querySelector(".chat-text-area")!, {
-				dataTransfer,
-				preventDefault: vi.fn(),
-			})
-
-			// Verify convertToMentionPath was not called
-			expect(mockConvertToMentionPath).not.toHaveBeenCalled()
-
-			// Verify setInputValue was not called
-			expect(setInputValue).not.toHaveBeenCalled()
-		})
-
-		describe("prompt history navigation", () => {
-			const mockClineMessages = [
-				{ type: "say", say: "user_feedback", text: "First prompt", ts: 1000 },
-				{ type: "say", say: "user_feedback", text: "Second prompt", ts: 2000 },
-				{ type: "say", say: "user_feedback", text: "Third prompt", ts: 3000 },
-			]
-
-			beforeEach(() => {
-				;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-					filePaths: [],
-					openedTabs: [],
-					apiConfiguration: {
-						apiProvider: "anthropic",
-					},
-					taskHistory: [],
-					clineMessages: mockClineMessages,
-					cwd: "/test/workspace",
-				})
-			})
-
-			it("should navigate to previous prompt on arrow up when cursor is at beginning", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-				// Ensure cursor is at the beginning
-				textarea.setSelectionRange(0, 0)
-
-				// Simulate arrow up key press
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-
-				// Should set the newest conversation message (first in reversed array)
-				expect(setInputValue).toHaveBeenCalledWith("Third prompt")
-			})
-
-			it("should navigate through history with multiple arrow up presses", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// First arrow up - newest conversation message
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Third prompt")
-
-				// Update input value to simulate the state change
-				setInputValue.mockClear()
-
-				// Second arrow up - previous conversation message
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Second prompt")
-			})
-
-			it("should navigate forward with arrow down", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Go back in history first (index 0 -> "Third prompt", then index 1 -> "Second prompt")
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				setInputValue.mockClear()
-
-				// Navigate forward (from index 1 back to index 0)
-				fireEvent.keyDown(textarea, { key: "ArrowDown" })
-				expect(setInputValue).toHaveBeenCalledWith("Third prompt")
-			})
-
-			it("should preserve current input when starting navigation", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Current input" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Navigate to history
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Third prompt")
-
-				setInputValue.mockClear()
-
-				// Navigate back to current input
-				fireEvent.keyDown(textarea, { key: "ArrowDown" })
-				expect(setInputValue).toHaveBeenCalledWith("Current input")
-			})
-
-			it("should reset history navigation when user types", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Navigate to history
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				setInputValue.mockClear()
-
-				// Type something
-				fireEvent.change(textarea, { target: { value: "New input", selectionStart: 9 } })
-
-				// Should reset history navigation
-				expect(setInputValue).toHaveBeenCalledWith("New input")
-			})
-
-			it("should reset history navigation when sending message", () => {
-				const onSend = vi.fn()
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea
-						{...defaultProps}
-						onSend={onSend}
-						setInputValue={setInputValue}
-						inputValue="Test message"
-					/>,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Navigate to history first
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				setInputValue.mockClear()
-
-				// Send message
-				fireEvent.keyDown(textarea, { key: "Enter" })
-
-				expect(onSend).toHaveBeenCalled()
-			})
-
-			it("should navigate history when cursor is at first line", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Clear any calls from initial render
-				setInputValue.mockClear()
-
-				// With empty input, cursor is at first line by default
-				// Arrow up should navigate history
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Third prompt")
-			})
-
-			it("should filter history by current workspace", () => {
-				const mixedClineMessages = [
-					{ type: "say", say: "user_feedback", text: "Workspace 1 prompt", ts: 1000 },
-					{ type: "say", say: "user_feedback", text: "Other workspace prompt", ts: 2000 },
-					{ type: "say", say: "user_feedback", text: "Workspace 1 prompt 2", ts: 3000 },
-				]
-
-				;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-					filePaths: [],
-					openedTabs: [],
-					apiConfiguration: {
-						apiProvider: "anthropic",
-					},
-					taskHistory: [],
-					clineMessages: mixedClineMessages,
-					cwd: "/test/workspace",
-				})
-
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Should show conversation messages newest first (after reverse)
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Workspace 1 prompt 2")
-
-				setInputValue.mockClear()
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Other workspace prompt")
-			})
-
-			it("should handle empty conversation history gracefully", () => {
-				;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-					filePaths: [],
-					openedTabs: [],
-					apiConfiguration: {
-						apiProvider: "anthropic",
-					},
-					taskHistory: [],
-					clineMessages: [],
-					cwd: "/test/workspace",
-				})
-
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Should not crash or call setInputValue
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).not.toHaveBeenCalled()
-			})
-
-			it("should ignore empty or whitespace-only messages", () => {
-				const clineMessagesWithEmpty = [
-					{ type: "say", say: "user_feedback", text: "Valid prompt", ts: 1000 },
-					{ type: "say", say: "user_feedback", text: "", ts: 2000 },
-					{ type: "say", say: "user_feedback", text: "   ", ts: 3000 },
-					{ type: "say", say: "user_feedback", text: "Another valid prompt", ts: 4000 },
-				]
-
-				;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-					filePaths: [],
-					openedTabs: [],
-					apiConfiguration: {
-						apiProvider: "anthropic",
-					},
-					taskHistory: [],
-					clineMessages: clineMessagesWithEmpty,
-					cwd: "/test/workspace",
-				})
-
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Should skip empty messages, newest first for conversation
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Another valid prompt")
-
-				setInputValue.mockClear()
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Valid prompt")
-			})
-
-			it("should use task history (oldest first) when no conversation messages exist", () => {
-				const mockTaskHistory = [
-					{ task: "First task", workspace: "/test/workspace" },
-					{ task: "Second task", workspace: "/test/workspace" },
-					{ task: "Third task", workspace: "/test/workspace" },
-				]
-
-				;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-					filePaths: [],
-					openedTabs: [],
-					apiConfiguration: {
-						apiProvider: "anthropic",
-					},
-					taskHistory: mockTaskHistory,
-					clineMessages: [], // No conversation messages
-					cwd: "/test/workspace",
-				})
-
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-
-				// Should show task history oldest first (chronological order)
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("First task")
-
-				setInputValue.mockClear()
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Second task")
-			})
-
-			it("should reset navigation position when switching between history sources", () => {
-				const setInputValue = vi.fn()
-				const { rerender } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />,
-				)
-
-				// Start with task history
-				;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-					filePaths: [],
-					openedTabs: [],
-					apiConfiguration: {
-						apiProvider: "anthropic",
-					},
-					taskHistory: [
-						{ task: "Task 1", workspace: "/test/workspace" },
-						{ task: "Task 2", workspace: "/test/workspace" },
-					],
-					clineMessages: [],
-					cwd: "/test/workspace",
-				})
-
-				rerender(<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />)
-
-				const textarea = document.querySelector("textarea")!
-
-				// Navigate in task history
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Task 1")
-
-				// Switch to conversation messages
-				;(useExtensionState as ReturnType<typeof vi.fn>).mockReturnValue({
-					filePaths: [],
-					openedTabs: [],
-					apiConfiguration: {
-						apiProvider: "anthropic",
-					},
-					taskHistory: [],
-					clineMessages: [
-						{ type: "say", say: "user_feedback", text: "Message 1", ts: 1000 },
-						{ type: "say", say: "user_feedback", text: "Message 2", ts: 2000 },
-					],
-					cwd: "/test/workspace",
-				})
-
-				setInputValue.mockClear()
-				rerender(<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="" />)
-
-				// Should start from beginning of conversation history (newest first)
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-				expect(setInputValue).toHaveBeenCalledWith("Message 2")
-			})
-
-			it("should not navigate history with arrow up when cursor is not at beginning", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Some text here" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-				// Set cursor to middle of text (not at beginning)
-				textarea.setSelectionRange(5, 5)
-
-				// Clear any calls from initial render
-				setInputValue.mockClear()
-
-				// Simulate arrow up key press
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-
-				// Should not navigate history, allowing default behavior (move cursor to start)
-				expect(setInputValue).not.toHaveBeenCalled()
-			})
-
-			it("should navigate history with arrow up when cursor is at beginning", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Some text here" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-				// Set cursor to beginning of text
-				textarea.setSelectionRange(0, 0)
-
-				// Clear any calls from initial render
-				setInputValue.mockClear()
-
-				// Simulate arrow up key press
-				fireEvent.keyDown(textarea, { key: "ArrowUp" })
-
-				// Should navigate to history since cursor is at beginning
-				expect(setInputValue).toHaveBeenCalledWith("Third prompt")
-			})
-
-			it("should navigate history with Command+Up when cursor is at beginning", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Some text here" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-				// Set cursor to beginning of text
-				textarea.setSelectionRange(0, 0)
-
-				// Clear any calls from initial render
-				setInputValue.mockClear()
-
-				// Simulate Command+Up key press
-				fireEvent.keyDown(textarea, { key: "ArrowUp", metaKey: true })
-
-				// Should navigate to history since cursor is at beginning (same as regular Up)
-				expect(setInputValue).toHaveBeenCalledWith("Third prompt")
-			})
-
-			it("should not navigate history with Command+Up when cursor is not at beginning", () => {
-				const setInputValue = vi.fn()
-				const { container } = render(
-					<ChatTextArea {...defaultProps} setInputValue={setInputValue} inputValue="Some text here" />,
-				)
-
-				const textarea = container.querySelector("textarea")!
-				// Set cursor to middle of text (not at beginning)
-				textarea.setSelectionRange(5, 5)
-
-				// Clear any calls from initial render
-				setInputValue.mockClear()
-
-				// Simulate Command+Up key press
-				fireEvent.keyDown(textarea, { key: "ArrowUp", metaKey: true })
-
-				// Should not navigate history, allowing default behavior (same as regular Up)
-				expect(setInputValue).not.toHaveBeenCalled()
-			})
-		})
-	})
-
-	describe("selectApiConfig", () => {
-		// Helper function to get the API config dropdown
-		const getApiConfigDropdown = () => {
-			return screen.getByTestId("dropdown-trigger")
-		}
-		it("should be enabled independently of sendingDisabled", () => {
-			render(<ChatTextArea {...defaultProps} sendingDisabled={true} selectApiConfigDisabled={false} />)
-			const apiConfigDropdown = getApiConfigDropdown()
-			expect(apiConfigDropdown).not.toHaveAttribute("disabled")
-		})
-		it("should be disabled when selectApiConfigDisabled is true", () => {
-			render(<ChatTextArea {...defaultProps} sendingDisabled={true} selectApiConfigDisabled={true} />)
-			const apiConfigDropdown = getApiConfigDropdown()
-			expect(apiConfigDropdown).toHaveAttribute("disabled")
-		})
-	})
-})
diff --git a/webview-ui/src/components/common/CodeAccordian.tsx b/webview-ui/src/components/common/CodeAccordian.tsx
index b07461c7..ef896d42 100644
--- a/webview-ui/src/components/common/CodeAccordian.tsx
+++ b/webview-ui/src/components/common/CodeAccordian.tsx
@@ -33,6 +33,7 @@ const CodeAccordian = ({
 	const inferredLanguage = useMemo(() => language ?? (path ? getLanguageFromPath(path) : "txt"), [path, language])
 	const source = useMemo(() => code.trim(), [code])
 	const hasHeader = Boolean(path || isFeedback || header)
+	const isAgentEdits = Boolean(source.startsWith('# agentEdits'))
 
 	return (
 		<ToolUseBlock>
@@ -46,9 +47,9 @@ const CodeAccordian = ({
 						</div>
 					) : isFeedback ? (
 						<div className="flex items-center">
-							<span className={`codicon codicon-${isFeedback ? "feedback" : "codicon-output"} mr-1.5`} />
+							<span className={`codicon codicon-${isAgentEdits ? "hubot" : isFeedback ? "feedback" : "output"} mr-1.5`} />
 							<span className="whitespace-nowrap overflow-hidden text-ellipsis mr-2 rtl">
-								{isFeedback ? "User Edits" : "Console Logs"}
+								{isAgentEdits ? "Roo Edits" : isFeedback ? "User Edits" : "Console Logs"}
 							</span>
 						</div>
 					) : (
diff --git a/webview-ui/src/components/common/CodeBlock.tsx b/webview-ui/src/components/common/CodeBlock.tsx
index 28492acd..458fa3c2 100644
--- a/webview-ui/src/components/common/CodeBlock.tsx
+++ b/webview-ui/src/components/common/CodeBlock.tsx
@@ -145,7 +145,16 @@ export const StyledPre = styled.div<{
 		word-break: ${({ wordwrap }) => (wordwrap === "false" ? "normal" : "normal")};
 		overflow-wrap: ${({ wordwrap }) => (wordwrap === "false" ? "normal" : "break-word")};
 		font-size: var(--vscode-editor-font-size, var(--vscode-font-size, 12px));
-		font-family: var(--vscode-editor-font-family);
+		font-family: 'JetBrains Mono', 'Fira Code', Consolas, 'Courier New', monospace;
+    
+		@font-face {
+			font-family: 'code-chinese';
+			src: local('Microsoft YaHei'), local('PingFang SC'), local('SimHei');
+			unicode-range: U+4E00-9FFF, U+3400-4DBF, U+20000-2A6DF, U+2A700-2B73F, U+2B740-2B81F, U+2B820-2CEAF;
+			size-adjust: 90%;
+		}
+		
+		font-family: 'JetBrains Mono', 'Fira Code', Consolas, 'code-chinese', 'Courier New', monospace, var(--vscode-font-family);
 	}
 
 	pre > code {
diff --git a/webview-ui/src/components/common/MarkdownBlock.tsx b/webview-ui/src/components/common/MarkdownBlock.tsx
index fe033efe..4375acb4 100644
--- a/webview-ui/src/components/common/MarkdownBlock.tsx
+++ b/webview-ui/src/components/common/MarkdownBlock.tsx
@@ -79,7 +79,16 @@ const remarkUrlToLink = () => {
 
 const StyledMarkdown = styled.div`
 	code:not(pre > code) {
-		font-family: var(--vscode-editor-font-family, monospace);
+		font-family: 'JetBrains Mono', 'Fira Code', Consolas, 'Courier New', monospace;
+    
+		@font-face {
+			font-family: 'code-chinese';
+			src: local('Microsoft YaHei'), local('PingFang SC'), local('SimHei');
+			unicode-range: U+4E00-9FFF, U+3400-4DBF, U+20000-2A6DF, U+2A700-2B73F, U+2B740-2B81F, U+2B820-2CEAF;
+			size-adjust: 90%;
+		}
+		
+		font-family: 'JetBrains Mono', 'Fira Code', Consolas, 'code-chinese', 'Courier New', monospace, var(--vscode-font-family);
 		filter: saturation(110%) brightness(95%);
 		color: var(--vscode-textPreformat-foreground) !important;
 		background-color: var(--vscode-textPreformat-background) !important;
diff --git a/webview-ui/src/components/settings/ApiOptions.tsx b/webview-ui/src/components/settings/ApiOptions.tsx
index 5f869290..8a8fbe06 100644
--- a/webview-ui/src/components/settings/ApiOptions.tsx
+++ b/webview-ui/src/components/settings/ApiOptions.tsx
@@ -16,6 +16,7 @@ import {
 	claudeCodeDefaultModelId,
 	geminiDefaultModelId,
 	deepSeekDefaultModelId,
+	modelScopeDefaultModelId,
 	mistralDefaultModelId,
 	xaiDefaultModelId,
 	groqDefaultModelId,
@@ -39,6 +40,7 @@ import {
 	Chutes,
 	ClaudeCode,
 	DeepSeek,
+	ModelScope,
 	Gemini,
 	Glama,
 	Groq,
@@ -260,6 +262,7 @@ const ApiOptions = ({
 				"openai-native": { field: "apiModelId", default: openAiNativeDefaultModelId },
 				gemini: { field: "apiModelId", default: geminiDefaultModelId },
 				deepseek: { field: "apiModelId", default: deepSeekDefaultModelId },
+				modelscope: { field: "apiModelId", default: modelScopeDefaultModelId },
 				mistral: { field: "apiModelId", default: mistralDefaultModelId },
 				xai: { field: "apiModelId", default: xaiDefaultModelId },
 				groq: { field: "apiModelId", default: groqDefaultModelId },
@@ -433,6 +436,10 @@ const ApiOptions = ({
 				<LMStudio apiConfiguration={apiConfiguration} setApiConfigurationField={setApiConfigurationField} />
 			)}
 
+			{selectedProvider === "modelscope" && (
+				<ModelScope apiConfiguration={apiConfiguration} setApiConfigurationField={setApiConfigurationField} />
+			)}
+
 			{selectedProvider === "deepseek" && (
 				<DeepSeek apiConfiguration={apiConfiguration} setApiConfigurationField={setApiConfigurationField} />
 			)}
diff --git a/webview-ui/src/components/settings/constants.ts b/webview-ui/src/components/settings/constants.ts
index bbee8f99..417e6e5f 100644
--- a/webview-ui/src/components/settings/constants.ts
+++ b/webview-ui/src/components/settings/constants.ts
@@ -12,6 +12,7 @@ import {
 	xaiModels,
 	groqModels,
 	chutesModels,
+	modelScopeModels
 } from "@roo-code/types"
 
 export const MODELS_BY_PROVIDER: Partial<Record<ProviderName, Record<string, ModelInfo>>> = {
@@ -26,28 +27,29 @@ export const MODELS_BY_PROVIDER: Partial<Record<ProviderName, Record<string, Mod
 	xai: xaiModels,
 	groq: groqModels,
 	chutes: chutesModels,
+	modelscope: modelScopeModels,
 }
 
 export const PROVIDERS = [
 	{ value: "openrouter", label: "OpenRouter" },
-	{ value: "anthropic", label: "Anthropic" },
-	{ value: "claude-code", label: "Claude Code" },
+	// { value: "anthropic", label: "Anthropic" },
 	{ value: "gemini", label: "Google Gemini" },
 	{ value: "deepseek", label: "DeepSeek" },
 	{ value: "openai-native", label: "OpenAI" },
 	{ value: "openai", label: "OpenAI Compatible" },
-	{ value: "vertex", label: "GCP Vertex AI" },
-	{ value: "bedrock", label: "Amazon Bedrock" },
-	{ value: "glama", label: "Glama" },
-	{ value: "vscode-lm", label: "VS Code LM API" },
-	{ value: "mistral", label: "Mistral" },
-	{ value: "lmstudio", label: "LM Studio" },
-	{ value: "ollama", label: "Ollama" },
-	{ value: "unbound", label: "Unbound" },
-	{ value: "requesty", label: "Requesty" },
-	{ value: "human-relay", label: "Human Relay" },
-	{ value: "xai", label: "xAI (Grok)" },
-	{ value: "groq", label: "Groq" },
-	{ value: "chutes", label: "Chutes AI" },
-	{ value: "litellm", label: "LiteLLM" },
+	{ value: "modelscope", label: "ModelScope" },
+	// { value: "vertex", label: "GCP Vertex AI" },
+	// { value: "bedrock", label: "Amazon Bedrock" },
+	// { value: "glama", label: "Glama" },
+	// { value: "vscode-lm", label: "VS Code LM API" },
+	// { value: "mistral", label: "Mistral" },
+	// { value: "lmstudio", label: "LM Studio" },
+	// { value: "ollama", label: "Ollama" },
+	// { value: "unbound", label: "Unbound" },
+	// { value: "requesty", label: "Requesty" },
+	// { value: "human-relay", label: "Human Relay" },
+	// { value: "xai", label: "xAI (Grok)" },
+	// { value: "groq", label: "Groq" },
+	// { value: "chutes", label: "Chutes AI" },
+	// { value: "litellm", label: "LiteLLM" },
 ].sort((a, b) => a.label.localeCompare(b.label))
diff --git a/webview-ui/src/components/settings/providers/index.ts b/webview-ui/src/components/settings/providers/index.ts
index b1956074..4e1fa6a3 100644
--- a/webview-ui/src/components/settings/providers/index.ts
+++ b/webview-ui/src/components/settings/providers/index.ts
@@ -7,6 +7,7 @@ export { Gemini } from "./Gemini"
 export { Glama } from "./Glama"
 export { Groq } from "./Groq"
 export { LMStudio } from "./LMStudio"
+export { ModelScope } from "./ModelScope"
 export { Mistral } from "./Mistral"
 export { Ollama } from "./Ollama"
 export { OpenAI } from "./OpenAI"
diff --git a/webview-ui/src/components/ui/hooks/useOpenRouterModelProviders.ts b/webview-ui/src/components/ui/hooks/useOpenRouterModelProviders.ts
index dc50c0f6..439b328c 100644
--- a/webview-ui/src/components/ui/hooks/useOpenRouterModelProviders.ts
+++ b/webview-ui/src/components/ui/hooks/useOpenRouterModelProviders.ts
@@ -43,7 +43,7 @@ async function getOpenRouterProvidersForModel(modelId: string) {
 	const models: Record<string, OpenRouterModelProvider> = {}
 
 	try {
-		const response = await axios.get(`https://openrouter.ai/api/v1/models/${modelId}/endpoints`)
+		const response = await axios.get(`https://riddler.mynatapp.cc/api/openrouter/v1/models/${modelId}/endpoints`)
 		const result = openRouterEndpointsSchema.safeParse(response.data)
 
 		if (!result.success) {
diff --git a/webview-ui/src/components/ui/hooks/useSelectedModel.ts b/webview-ui/src/components/ui/hooks/useSelectedModel.ts
index 40c1ff24..859bc025 100644
--- a/webview-ui/src/components/ui/hooks/useSelectedModel.ts
+++ b/webview-ui/src/components/ui/hooks/useSelectedModel.ts
@@ -8,6 +8,8 @@ import {
 	bedrockModels,
 	deepSeekDefaultModelId,
 	deepSeekModels,
+	modelScopeDefaultModelId,
+	modelScopeModels,
 	geminiDefaultModelId,
 	geminiModels,
 	mistralDefaultModelId,
@@ -162,6 +164,11 @@ function getSelectedModel({
 			const info = deepSeekModels[id as keyof typeof deepSeekModels]
 			return { id, info }
 		}
+		case "modelscope": {
+			const id = apiConfiguration.apiModelId ?? modelScopeDefaultModelId
+			const info = modelScopeModels[id as keyof typeof modelScopeModels]
+			return { id, info }
+		}
 		case "openai-native": {
 			const id = apiConfiguration.apiModelId ?? openAiNativeDefaultModelId
 			const info = openAiNativeModels[id as keyof typeof openAiNativeModels]
diff --git a/webview-ui/src/context/ExtensionStateContext.tsx b/webview-ui/src/context/ExtensionStateContext.tsx
index bf927211..fc59c908 100644
--- a/webview-ui/src/context/ExtensionStateContext.tsx
+++ b/webview-ui/src/context/ExtensionStateContext.tsx
@@ -221,6 +221,14 @@ export const ExtensionStateContextProvider: React.FC<{ children: React.ReactNode
 			codebaseIndexEmbedderModelId: "",
 			codebaseIndexSearchMaxResults: undefined,
 			codebaseIndexSearchMinScore: undefined,
+
+			embeddingBaseUrl: "",
+			embeddingModelID: "",
+			enhancementBaseUrl: "",
+			enhancementModelID: "",
+			ragPath: "",
+			llmFilter: false,
+			codeBaseLogging: false,
 		},
 		codebaseIndexModels: { ollama: {}, openai: {} },
 		alwaysAllowUpdateTodoList: true,
diff --git a/webview-ui/src/i18n/locales/zh-CN/chat.json b/webview-ui/src/i18n/locales/zh-CN/chat.json
index e8dbbb97..ef46fadb 100644
--- a/webview-ui/src/i18n/locales/zh-CN/chat.json
+++ b/webview-ui/src/i18n/locales/zh-CN/chat.json
@@ -107,7 +107,7 @@
 	"sendMessage": "发送消息",
 	"stopTts": "停止文本转语音",
 	"typeMessage": "输入消息...",
-	"typeTask": "在此处输入您的任务...",
+	"typeTask": "在此处输入您的任务... (按 Enter 键发送，Ctrl+Enter 或 Shift+Enter 换行)",
 	"addContext": "@添加上下文，/切换模式",
 	"dragFiles": "Shift+拖拽文件",
 	"dragFilesImages": "Shift+拖拽文件/图片",
diff --git a/webview-ui/src/utils/context-mentions.ts b/webview-ui/src/utils/context-mentions.ts
index 889dca9d..d5340b23 100644
--- a/webview-ui/src/utils/context-mentions.ts
+++ b/webview-ui/src/utils/context-mentions.ts
@@ -105,6 +105,9 @@ export enum ContextMenuOptionType {
 	Git = "git",
 	NoResults = "noResults",
 	Mode = "mode", // Add mode type
+	Codebase = "codebase", // Add codebase type
+	Summary = "summary", // Add summary type
+	Memory = "memory", // Add memory type
 }
 
 export interface ContextMenuQueryItem {
@@ -170,6 +173,7 @@ export function getContextMenuOptions(
 			const files = queryItems
 				.filter(
 					(item) =>
+						// item.type === ContextMenuOptionType.OpenedFile,
 						item.type === ContextMenuOptionType.File || item.type === ContextMenuOptionType.OpenedFile,
 				)
 				.map((item) => ({
@@ -192,12 +196,15 @@ export function getContextMenuOptions(
 		}
 
 		return [
+			{ type: ContextMenuOptionType.Git },
 			{ type: ContextMenuOptionType.Problems },
-			{ type: ContextMenuOptionType.Terminal },
-			{ type: ContextMenuOptionType.URL },
-			{ type: ContextMenuOptionType.Folder },
+			// { type: ContextMenuOptionType.Terminal },
+			// { type: ContextMenuOptionType.URL },
+			// { type: ContextMenuOptionType.Folder },
 			{ type: ContextMenuOptionType.File },
-			{ type: ContextMenuOptionType.Git },
+			{ type: ContextMenuOptionType.Codebase },
+			{ type: ContextMenuOptionType.Summary },
+			{ type: ContextMenuOptionType.Memory },
 		]
 	}
 
@@ -218,6 +225,15 @@ export function getContextMenuOptions(
 	if ("problems".startsWith(lowerQuery)) {
 		suggestions.push({ type: ContextMenuOptionType.Problems })
 	}
+	if ("codebase".startsWith(lowerQuery)) {
+		suggestions.push({ type: ContextMenuOptionType.Codebase })
+	}
+	if ("summary".startsWith(lowerQuery)) {
+		suggestions.push({ type: ContextMenuOptionType.Summary })
+	}
+	if ("memory".startsWith(lowerQuery)) {
+		suggestions.push({ type: ContextMenuOptionType.Memory })
+	}
 	if ("terminal".startsWith(lowerQuery)) {
 		suggestions.push({ type: ContextMenuOptionType.Terminal })
 	}
